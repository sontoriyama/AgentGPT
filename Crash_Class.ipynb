{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sontoriyama/AgentGPT/blob/main/Crash_Class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JckP5_EZhgYH",
        "outputId": "ae0da5ac-52c6-42b7-8a64-feb734bbb713"
      },
      "id": "JckP5_EZhgYH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4==4.12.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: huggingface-hub==0.17.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.17.2)\n",
            "Requirement already satisfied: langchain==0.0.297 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.0.297)\n",
            "Requirement already satisfied: langsmith==0.0.39 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.0.39)\n",
            "Requirement already satisfied: notebook==7.0.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (7.0.4)\n",
            "Requirement already satisfied: numexpr==2.8.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.8.6)\n",
            "Requirement already satisfied: numpy==1.26.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.26.0)\n",
            "Requirement already satisfied: openai==0.28.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.28.0)\n",
            "Requirement already satisfied: pandas==2.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.1.1)\n",
            "Requirement already satisfied: safetensors==0.3.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.3.3)\n",
            "Requirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (0.9.0)\n",
            "Requirement already satisfied: transformers==4.33.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (4.33.2)\n",
            "Requirement already satisfied: typing-inspect==0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (0.9.0)\n",
            "Requirement already satisfied: typing_extensions==4.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (4.8.0)\n",
            "Requirement already satisfied: youtube-transcript-api==0.6.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (0.6.1)\n",
            "Requirement already satisfied: pytube==15.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (15.0.0)\n",
            "Collecting python-dotenv==1.0.0 (from -r requirements.txt (line 17))\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4==4.12.2->-r requirements.txt (line 1)) (2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.17.2->-r requirements.txt (line 2)) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.17.2->-r requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.17.2->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.17.2->-r requirements.txt (line 2)) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.17.2->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.17.2->-r requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.297->-r requirements.txt (line 3)) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.297->-r requirements.txt (line 3)) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.297->-r requirements.txt (line 3)) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.297->-r requirements.txt (line 3)) (0.6.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.297->-r requirements.txt (line 3)) (1.10.12)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.297->-r requirements.txt (line 3)) (8.2.3)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from notebook==7.0.4->-r requirements.txt (line 5)) (2.7.3)\n",
            "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /usr/local/lib/python3.10/dist-packages (from notebook==7.0.4->-r requirements.txt (line 5)) (2.25.0)\n",
            "Requirement already satisfied: jupyterlab<5,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from notebook==7.0.4->-r requirements.txt (line 5)) (4.0.6)\n",
            "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /usr/local/lib/python3.10/dist-packages (from notebook==7.0.4->-r requirements.txt (line 5)) (0.2.3)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from notebook==7.0.4->-r requirements.txt (line 5)) (6.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.1.1->-r requirements.txt (line 9)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.1.1->-r requirements.txt (line 9)) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.1.1->-r requirements.txt (line 9)) (2023.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2->-r requirements.txt (line 12)) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2->-r requirements.txt (line 12)) (0.13.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect==0.9.0->-r requirements.txt (line 13)) (1.0.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.297->-r requirements.txt (line 3)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.297->-r requirements.txt (line 3)) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.297->-r requirements.txt (line 3)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.297->-r requirements.txt (line 3)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.297->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.297->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.297->-r requirements.txt (line 3)) (3.20.1)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (3.7.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (23.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (3.1.2)\n",
            "Requirement already satisfied: jupyter-client>=7.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (8.3.1)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (5.3.1)\n",
            "Requirement already satisfied: jupyter-events>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: jupyter-server-terminals in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (0.4.4)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (6.5.4)\n",
            "Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (5.9.2)\n",
            "Requirement already satisfied: overrides in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (7.4.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (0.17.1)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (25.1.1)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (0.17.1)\n",
            "Requirement already satisfied: traitlets>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (5.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (1.6.2)\n",
            "Requirement already satisfied: async-lru>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab<5,>=4.0.2->notebook==7.0.4->-r requirements.txt (line 5)) (2.0.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyterlab<5,>=4.0.2->notebook==7.0.4->-r requirements.txt (line 5)) (5.5.6)\n",
            "Requirement already satisfied: jupyter-lsp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab<5,>=4.0.2->notebook==7.0.4->-r requirements.txt (line 5)) (2.2.0)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from jupyterlab<5,>=4.0.2->notebook==7.0.4->-r requirements.txt (line 5)) (2.0.1)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.22.1->notebook==7.0.4->-r requirements.txt (line 5)) (2.12.1)\n",
            "Requirement already satisfied: json5>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.22.1->notebook==7.0.4->-r requirements.txt (line 5)) (0.9.14)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.22.1->notebook==7.0.4->-r requirements.txt (line 5)) (4.19.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.1.1->-r requirements.txt (line 9)) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.17.2->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.17.2->-r requirements.txt (line 2)) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.17.2->-r requirements.txt (line 2)) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.297->-r requirements.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (1.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (2.1.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook==7.0.4->-r requirements.txt (line 5)) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook==7.0.4->-r requirements.txt (line 5)) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook==7.0.4->-r requirements.txt (line 5)) (0.10.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (3.10.0)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (2.0.7)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (0.1.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (4.9.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (0.2.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (0.8.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (1.5.0)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (2.16.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (2.18.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (21.2.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook==7.0.4->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook==7.0.4->-r requirements.txt (line 5)) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook==7.0.4->-r requirements.txt (line 5)) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook==7.0.4->-r requirements.txt (line 5)) (0.19.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook==7.0.4->-r requirements.txt (line 5)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook==7.0.4->-r requirements.txt (line 5)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook==7.0.4->-r requirements.txt (line 5)) (3.0.39)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook==7.0.4->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook==7.0.4->-r requirements.txt (line 5)) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook==7.0.4->-r requirements.txt (line 5)) (4.8.0)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook==7.0.4->-r requirements.txt (line 5)) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook==7.0.4->-r requirements.txt (line 5)) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook==7.0.4->-r requirements.txt (line 5)) (2.4)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook==7.0.4->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook==7.0.4->-r requirements.txt (line 5)) (1.13)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (1.15.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook==7.0.4->-r requirements.txt (line 5)) (2.21)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook==7.0.4->-r requirements.txt (line 5)) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook==7.0.4->-r requirements.txt (line 5)) (0.2.6)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook==7.0.4->-r requirements.txt (line 5)) (1.2.3)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b187bd4-ecc6-49c2-9e9f-0c9093e68b91",
      "metadata": {
        "id": "7b187bd4-ecc6-49c2-9e9f-0c9093e68b91"
      },
      "source": [
        "# Langchain Crash Class\n",
        "\n",
        "1. Prompts & LLMs\n",
        "2. Chains\n",
        "3. Memory and tools\n",
        "4. Agents\n",
        "5. Document Loaders & Transformers\n",
        "6. FastAPI Streaming\n",
        "\n",
        "<center><img src=\"https://diagnosemlpdf.s3.us-east-2.amazonaws.com/langchain_crash_class/langchain_chain.png\" alt=\"chains\" width=\"800\"/></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c425cde-6172-4641-b286-81b5f0a6f4b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c425cde-6172-4641-b286-81b5f0a6f4b7",
        "outputId": "c96be22d-d0be-4bea-9734-755fd5cd1301"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Cargar tu API KEY de OpenAI y otros recursos necesarios\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05e9fa8a-7e54-40f2-a9cc-ce4417e3454e",
      "metadata": {
        "id": "05e9fa8a-7e54-40f2-a9cc-ce4417e3454e"
      },
      "source": [
        "## 1. Prompts & LLMs\n",
        "\n",
        "- Solo unas líneas de código\n",
        "- Utilizamos default o prompt específico\n",
        "- Podemos utilizar variables\n",
        "- Chat vs LLM\n",
        "\n",
        "<center><img src=\"https://diagnosemlpdf.s3.us-east-2.amazonaws.com/langchain_crash_class/prompt_vs_penginering.png\" alt=\"chains\" width=\"800\"/></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecb5276e-335f-4463-acde-cff0e153b8c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ecb5276e-335f-4463-acde-cff0e153b8c7",
        "outputId": "44c27674-5e8e-4000-fbac-0b7182710f98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Redacta un poema como Pablo Neurda que trate de modelos de lenguaje.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Redacta un poema como Pablo Neurda que trate de {topic}.\"\n",
        ")\n",
        "prompt_template.format(topic=\"modelos de lenguaje\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc1ddf2b-89b2-4ea4-91e1-1557ad6e4cab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc1ddf2b-89b2-4ea4-91e1-1557ad6e4cab",
        "outputId": "83e9f96b-a3a6-4bd6-908d-7c8abd82fe56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['topic'], output_parser=None, partial_variables={}, template='Redacta un poema como Pablo Neurda que trate de {topic}.', template_format='f-string', validate_template=True)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "prompt_template"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7af2b0d2-151b-4c50-b944-d1e213eb78ae",
      "metadata": {
        "id": "7af2b0d2-151b-4c50-b944-d1e213eb78ae"
      },
      "source": [
        "### Chat models son distintos a LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38275f2e-b959-4c8f-b0e8-020de65c7a4f",
      "metadata": {
        "id": "38275f2e-b959-4c8f-b0e8-020de65c7a4f"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un experto en planificación de proyectos, tu nombres es {name}, te mantienes conciso al dar una respuesta y solo respondes al estar seguro que se trate de la temática, si no, simplemente di 'No lo sé' y tu nombre.\"),\n",
        "    (\"human\", \"Que día es hoy?\"),\n",
        "    (\"ai\", \"Es el 21 de Septienbre de 2023.\"),\n",
        "    (\"human\", \"Qué roles debe tener un proyecto tecnológico\"),\n",
        "    (\"ai\", \"Los roles principales en un proyecto tecnológico incluyen Gerente de Proyecto, Desarrollador de Software, Científico de Datos, Diseñador de UX/UI, Ingeniero de Pruebas, Arquitecto de Software, Analista de Negocios, Especialista en Seguridad, Operador de Sistemas, Especialista en Infraestructura, Gerente de Calidad, Documentador Técnico y Soporte Técnico.\"),\n",
        "    (\"human\", \"{user_input}\"),\n",
        "])\n",
        "\n",
        "messages = template.format_messages(\n",
        "    name=\"Myfuture\",\n",
        "    user_input=\"Cómo se cocina un plato de fídeos?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaf887b4-5a1c-49c5-a998-524e964b97c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaf887b4-5a1c-49c5-a998-524e964b97c1",
        "outputId": "587f1bdb-6567-494e-e36f-d948c4c3f4fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"Eres un experto en planificación de proyectos, tu nombres es Myfuture, te mantienes conciso al dar una respuesta y solo respondes al estar seguro que se trate de la temática, si no, simplemente di 'No lo sé' y tu nombre.\", additional_kwargs={}),\n",
              " HumanMessage(content='Que día es hoy?', additional_kwargs={}, example=False),\n",
              " AIMessage(content='Es el 21 de Septienbre de 2023.', additional_kwargs={}, example=False),\n",
              " HumanMessage(content='Qué roles debe tener un proyecto tecnológico', additional_kwargs={}, example=False),\n",
              " AIMessage(content='Los roles principales en un proyecto tecnológico incluyen Gerente de Proyecto, Desarrollador de Software, Científico de Datos, Diseñador de UX/UI, Ingeniero de Pruebas, Arquitecto de Software, Analista de Negocios, Especialista en Seguridad, Operador de Sistemas, Especialista en Infraestructura, Gerente de Calidad, Documentador Técnico y Soporte Técnico.', additional_kwargs={}, example=False),\n",
              " HumanMessage(content='Cómo se cocina un plato de fídeos?', additional_kwargs={}, example=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86d3aa02-3eaf-4f49-9cf7-45eed0f88333",
      "metadata": {
        "id": "86d3aa02-3eaf-4f49-9cf7-45eed0f88333"
      },
      "source": [
        "## 2. Chains\n",
        "\n",
        "- Solo unas líneas de código\n",
        "- Utilizamos default o prompt específico\n",
        "- Utilizamos un LLM en específico\n",
        "\n",
        "<center><img src=\"https://diagnosemlpdf.s3.us-east-2.amazonaws.com/langchain_crash_class/langchain_chains.png\" alt=\"chains\" width=\"800\"/></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dca0ab36-3702-42cd-aefb-82b40d5d3414",
      "metadata": {
        "id": "dca0ab36-3702-42cd-aefb-82b40d5d3414"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dad31f3-60fd-4005-a336-ad06eb84ebe7",
      "metadata": {
        "id": "8dad31f3-60fd-4005-a336-ad06eb84ebe7"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0)\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt_template\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d68c0de5-0d4a-4e75-96ef-afe4d7d6061b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "d68c0de5-0d4a-4e75-96ef-afe4d7d6061b",
        "outputId": "39747cf6-63ff-4ba6-e26e-b600704b4794"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Redacta un poema como Pablo Neurda que trate de $TOPICO.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "prompt_template.format(topic='$TOPICO')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e7ae355-f365-4b9f-8533-89768d706f60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e7ae355-f365-4b9f-8533-89768d706f60",
        "outputId": "e376f613-7836-42c1-faa6-795bb915b815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Modelos de lenguaje,\n",
            "Un código de hablar,\n",
            "Un lenguaje que nos une,\n",
            "Y nos hace comprender.\n",
            "\n",
            "Un lenguaje que nos permite\n",
            "Expresar lo que sentimos,\n",
            "Y nos da la libertad\n",
            "De comunicar nuestros sueños.\n",
            "\n",
            "Un lenguaje que nos conecta\n",
            "Y nos ayuda a compartir,\n",
            "Un lenguaje que nos une\n",
            "Y nos hace más fuertes.\n",
            "\n",
            "Un lenguaje que nos permite\n",
            "Ver el mundo de otra forma,\n",
            "Y nos da la oportunidad\n",
            "De crear un mejor mañana.\n",
            "CPU times: user 181 ms, sys: 22.3 ms, total: 203 ms\n",
            "Wall time: 3.58 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "response = llm_chain(\"modelos de lenguaje\")\n",
        "print(response['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d56b6739-87ac-4d68-9e4c-7ea13ffa8057",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "d56b6739-87ac-4d68-9e4c-7ea13ffa8057",
        "outputId": "c5bd3a3e-e660-437b-f857-33a0c33b0745"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             outputs = (\n\u001b[0;32m--> 286\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     ) -> Dict[str, str]:\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;34m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    491\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m             ]\n\u001b[0;32m--> 627\u001b[0;31m             output = self._generate_helper(\n\u001b[0m\u001b[1;32m    628\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0mflattened_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m             output = (\n\u001b[0;32m--> 516\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    517\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m                 )\n\u001b[1;32m    386\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                 response = completion_with_retry(\n\u001b[0m\u001b[1;32m    388\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_prompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py\u001b[0m in \u001b[0;36mcompletion_with_retry\u001b[0;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mis_explicit_retry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTryAgain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_explicit_retry\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py\u001b[0m in \u001b[0;36m_completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mretry_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Incorrect API key provided: sk-QvcX5***************************************sDyi. You can find your API key at https://platform.openai.com/account/api-keys."
          ]
        }
      ],
      "source": [
        "%%time\n",
        "response = llm_chain(\"mascotas\")\n",
        "print(response['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdf525f3-f040-48aa-8f8a-f43ba0f02d24",
      "metadata": {
        "id": "cdf525f3-f040-48aa-8f8a-f43ba0f02d24"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "499358b3-ffb7-4ded-9e91-09d95f3bd5ec",
      "metadata": {
        "id": "499358b3-ffb7-4ded-9e91-09d95f3bd5ec"
      },
      "outputs": [],
      "source": [
        "llm_chat = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e94e804-dbe4-40f0-afd2-f8fc72f629d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e94e804-dbe4-40f0-afd2-f8fc72f629d9",
        "outputId": "9524be71-fd70-418f-9d00-09ab37dcc294"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"Eres un experto en planificación de proyectos, tu nombres es Myfuture, te mantienes conciso al dar una respuesta y solo respondes al estar seguro que se trate de la temática, si no, simplemente di 'No lo sé' y tu nombre.\", additional_kwargs={}),\n",
              " HumanMessage(content='Que día es hoy?', additional_kwargs={}, example=False),\n",
              " AIMessage(content='Es el 21 de Septienbre de 2023.', additional_kwargs={}, example=False),\n",
              " HumanMessage(content='Qué roles debe tener un proyecto tecnológico', additional_kwargs={}, example=False),\n",
              " AIMessage(content='Los roles principales en un proyecto tecnológico incluyen Gerente de Proyecto, Desarrollador de Software, Científico de Datos, Diseñador de UX/UI, Ingeniero de Pruebas, Arquitecto de Software, Analista de Negocios, Especialista en Seguridad, Operador de Sistemas, Especialista en Infraestructura, Gerente de Calidad, Documentador Técnico y Soporte Técnico.', additional_kwargs={}, example=False),\n",
              " HumanMessage(content='Cómo se cocina un plato de fídeos?', additional_kwargs={}, example=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1de97ddb-b44d-48d7-869f-4179de329165",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1de97ddb-b44d-48d7-869f-4179de329165",
        "outputId": "706dbe1b-d814-437d-88ac-eb651a9022b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='No lo sé, soy un experto en planificación de proyectos. Mi nombre es Myfuture.' additional_kwargs={} example=False\n"
          ]
        }
      ],
      "source": [
        "result = llm_chat(messages)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31c05e89-97d7-44d9-a03b-70a47d110f85",
      "metadata": {
        "id": "31c05e89-97d7-44d9-a03b-70a47d110f85"
      },
      "source": [
        "### ¿Qué pasa si no tengo API KEY de OpenAI?\n",
        "- Podemos usar Huggingface ys su modelos OpenSource para experimentar de igual manera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21d4b1c1-5a50-4c17-b515-01673b6464ca",
      "metadata": {
        "id": "21d4b1c1-5a50-4c17-b515-01673b6464ca"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFacePipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ce5b59a-9fc6-4348-9289-93b28d1e4273",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232,
          "referenced_widgets": [
            "44cb8c58dc6f430683384636ace1ca03",
            "2a71e6d29010441a8e2955d1fed6a238",
            "3b68c7a6d38848cb85b685498841f15e",
            "b13e186868144215bae144814a45d9ae",
            "11f28707077c433a8c116564edb48b29",
            "d5b0a13b49ab4a81820cb9bd77ad2fb3",
            "bbe4b678c97c4fc398b2543ab82cbd0f",
            "4cf895fc7bc8480a99b8fea9861a5173",
            "5a2dd4d65b9d4f908b393e8fd07975a1",
            "7b4ba8913e324acf85413e805b92b17c",
            "12164f6f4d804c54a1637664fa777635",
            "f7a982d0e05a42feaceed3bab3a43ca1",
            "64b0087d9e6b4530a39e141817cec450",
            "2d952fb267a74e989933a847183b10e9",
            "592592b920ea4a73b66171d712de10ae",
            "048c1b4785924ca2baf0058d9c1f588b",
            "c1fba6ee5b184e28af81fb04fe1501b4",
            "057e0ce4fb914c2284fd6133c558c424",
            "2753ddb437ab457a8869b08fe79c326c",
            "1784be0601d94fe5917c8165f1c41bf4",
            "0921d36d99d04a0186911eafa52136cd",
            "02e625c05b2b46e7b107fb438676731a",
            "136e536874fc472192a9f738a6878f58",
            "d7562aff84ad45ba844fbb99a5271f00",
            "94dd5170bbf34bb6a0e1f7bfb23286bd",
            "b0ce6b8d39da4f329d0fa06716c0c82f",
            "3d8b6176300a407da4024254a95150ad",
            "7e60778062f14b5bb99ee0a94bdbcab6",
            "86927bf0ce1141d383bc70c57b9916f4",
            "4a8fda2caf374ff4a4631936540b5a83",
            "8b3ac37fc27848aaaac266244298315c",
            "ffa0ae6d623b4d839f708fa9d379bc15",
            "502bb716ec9940858a496e4d9bce54bc",
            "1271dffeb97d4641a2dd73fb529a24d9",
            "69a304532bb44aeb903720bb40fda2de",
            "bcffbccf55de43e2a781005173532865",
            "c773133ab5a042329b9bdd55a62384aa",
            "10d984d7befc48ebb517ace485182ac1",
            "2791bf92dd5d4909a6c6d15a46f9b3d5",
            "07b5a67a651447a185c70323155418d3",
            "be523aa578214881baec56d7d902ae44",
            "d9aa6b898177495bad5ed9ad2733d48b",
            "c6031a55930d4776a50494269c198915",
            "8a4046c8cd954e85944072c313680205",
            "f5e8e715f13c41e8808719bbabdf9ad5",
            "b61ed740038e4fda8e422683e4f3d7cc",
            "2c3309d4ab014da583ee58787b89de9c",
            "dbb5f7c63d5e4f71bdea431848523f1b",
            "40b32581127e48058430e48fa58f16c0",
            "de9eedb48fb4442ebd7d89447003bdd3",
            "22eb48a5737847a8bc60e6c282dcbe3f",
            "e2d042099afb439c853cef681bb633aa",
            "66e90256d7cf4ce393745cc7b0fa9404",
            "289c223e455f463e8981d5cad60039ba",
            "6f03e4b9b32d40f3b510dbb1bf4c1b4a"
          ]
        },
        "id": "9ce5b59a-9fc6-4348-9289-93b28d1e4273",
        "outputId": "33fddc41-8ae6-4390-b90b-a456f99de5a5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44cb8c58dc6f430683384636ace1ca03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7a982d0e05a42feaceed3bab3a43ca1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "136e536874fc472192a9f738a6878f58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1271dffeb97d4641a2dd73fb529a24d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5e8e715f13c41e8808719bbabdf9ad5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "llm_open = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"bigscience/bloom-1b7\",\n",
        "    task=\"text-generation\",\n",
        "    model_kwargs={\"temperature\": 0, \"max_length\": 64},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f207539-6b82-4e6b-8fe9-dad5e6990e54",
      "metadata": {
        "id": "9f207539-6b82-4e6b-8fe9-dad5e6990e54"
      },
      "outputs": [],
      "source": [
        "chain = prompt_template | llm_open"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f5b3103-09e9-49b4-a5e7-3d6f30011eee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f5b3103-09e9-49b4-a5e7-3d6f30011eee",
        "outputId": "1991d0e4-0984-461f-b328-a862b2c40a2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " El poema debe ser de una sola línea y debe tener un título. El poema debe ser inédito y no haber sido publicado en ningún otro medio. El poema debe ser inédito y no haber sido publicado en ningún otro medio. El poema debe ser inédito y no\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke({\"topic\": 'comida'}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df05af03-317b-4a15-baa9-c9b2d6b44d90",
      "metadata": {
        "id": "df05af03-317b-4a15-baa9-c9b2d6b44d90"
      },
      "outputs": [],
      "source": [
        "# Import things that are needed generically\n",
        "from langchain.chains import LLMMathChain\n",
        "from langchain.utilities import SerpAPIWrapper\n",
        "from langchain.agents import AgentType, initialize_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.tools import BaseTool, StructuredTool, Tool, tool"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f274eef-5225-4420-b43b-15b1e9a202e0",
      "metadata": {
        "id": "1f274eef-5225-4420-b43b-15b1e9a202e0"
      },
      "source": [
        "## 3. Agents\n",
        "\n",
        "- Razonamiento autonomo\n",
        "- BabyAGI / AutoGPT\n",
        "- Múltiples iteraciones\n",
        "- Utilizamos un LLM en específico\n",
        "- Ejemplo base: ReAct\n",
        "\n",
        "<center><img src=\"https://diagnosemlpdf.s3.us-east-2.amazonaws.com/langchain_crash_class/agent.png\" alt=\"chains\" width=\"800\"/></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84efe0b3-af08-4466-9a79-fa17487e7467",
      "metadata": {
        "id": "84efe0b3-af08-4466-9a79-fa17487e7467"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "781d1ef4-c19a-4a6a-89b7-64ce2d9f3285",
      "metadata": {
        "id": "781d1ef4-c19a-4a6a-89b7-64ce2d9f3285"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea228713-4ecb-4a4b-b1cc-ad115a2bb5d1",
      "metadata": {
        "id": "ea228713-4ecb-4a4b-b1cc-ad115a2bb5d1"
      },
      "outputs": [],
      "source": [
        "tools = load_tools([\"llm-math\"], llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13443751-44b6-437f-9600-295e1f71d38f",
      "metadata": {
        "id": "13443751-44b6-437f-9600-295e1f71d38f"
      },
      "outputs": [],
      "source": [
        "agent_executor = initialize_agent(tools=tools, llm=llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3150d10-fc5d-457d-9835-ba5e989ad614",
      "metadata": {
        "id": "a3150d10-fc5d-457d-9835-ba5e989ad614",
        "outputId": "9e3a955d-7cbe-456a-dae2-c2897a81d55c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to subtract 3 from 9\n",
            "Action: Calculator\n",
            "Action Input: 9 - 3\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mAnswer: 6\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Perdí 6 perritos.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Si tenía 9 perritos, no me quedan más que 3, calcula cuántos perdí',\n",
              " 'output': 'Perdí 6 perritos.'}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke({\"input\": \"Si tenía 9 perritos, no me quedan más que 3, calcula cuántos perdí\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4753d7db-1591-43e5-9a2e-0198c6766f83",
      "metadata": {
        "id": "4753d7db-1591-43e5-9a2e-0198c6766f83",
        "outputId": "e5aa3eea-b7a6-4f09-ef39-8e6aa3052d0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to figure out what yesterday's date was.\n",
            "Action: Calculator\n",
            "Action Input: Today's date minus 1 day\u001b[0m"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "LLMMathChain._evaluate(\"\ndate.today() - timedelta(days=1)\n\") raised error: Expression date.today() - timedelta(days=1) has forbidden control characters.. Please try again with a valid numerical expression",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[0;32m~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/llm_math/base.py:81\u001b[0m, in \u001b[0;36mLLMMathChain._evaluate_expression\u001b[0;34m(self, expression)\u001b[0m\n\u001b[1;32m     79\u001b[0m     local_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpi\u001b[39m\u001b[38;5;124m\"\u001b[39m: math\u001b[38;5;241m.\u001b[39mpi, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m\"\u001b[39m: math\u001b[38;5;241m.\u001b[39me}\n\u001b[1;32m     80\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[0;32m---> 81\u001b[0m         \u001b[43mnumexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpression\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m            \u001b[49m\u001b[43mglobal_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# restrict access to globals\u001b[39;49;00m\n\u001b[1;32m     84\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# add common mathematical functions\u001b[39;49;00m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     )\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/numexpr/necompiler.py:975\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(ex, local_dict, global_dict, out, order, casting, sanitize, _frame_depth, **kwargs)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "File \u001b[0;32m~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/numexpr/necompiler.py:872\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(ex, local_dict, global_dict, out, order, casting, _frame_depth, sanitize, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expr_key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _names_cache:\n\u001b[0;32m--> 872\u001b[0m     _names_cache[expr_key] \u001b[38;5;241m=\u001b[39m \u001b[43mgetExprNames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msanitize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    873\u001b[0m names, ex_uses_vml \u001b[38;5;241m=\u001b[39m _names_cache[expr_key]\n",
            "File \u001b[0;32m~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/numexpr/necompiler.py:721\u001b[0m, in \u001b[0;36mgetExprNames\u001b[0;34m(text, context, sanitize)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetExprNames\u001b[39m(text, context, sanitize: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 721\u001b[0m     ex \u001b[38;5;241m=\u001b[39m \u001b[43mstringToExpression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m     ast \u001b[38;5;241m=\u001b[39m expressionToAST(ex)\n",
            "File \u001b[0;32m~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/numexpr/necompiler.py:281\u001b[0m, in \u001b[0;36mstringToExpression\u001b[0;34m(s, types, context, sanitize)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _blacklist_re\u001b[38;5;241m.\u001b[39msearch(no_whitespace) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpression \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has forbidden control characters.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    283\u001b[0m old_ctx \u001b[38;5;241m=\u001b[39m expressions\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mget_current_context()\n",
            "\u001b[0;31mValueError\u001b[0m: Expression date.today() - timedelta(days=1) has forbidden control characters.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQué fecha fue ayer?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/base.py:66\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m     62\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     64\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m     65\u001b[0m     config \u001b[38;5;241m=\u001b[39m config \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/base.py:292\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    293\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    294\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    295\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    296\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/base.py:286\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    279\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    280\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    281\u001b[0m     inputs,\n\u001b[1;32m    282\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    289\u001b[0m     )\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
            "File \u001b[0;32m~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/agents/agent.py:1122\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1122\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1131\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1132\u001b[0m         )\n",
            "File \u001b[0;32m~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/agents/agent.py:977\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    975\u001b[0m         tool_run_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;66;03m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[0;32m--> 977\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mtool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_action\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_run_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    985\u001b[0m     tool_run_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mtool_run_logging_kwargs()\n",
            "File \u001b[0;32m~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/tools/base.py:356\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    355\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(e)\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;28mstr\u001b[39m(observation), color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    360\u001b[0m     )\n",
            "File \u001b[0;32m~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/tools/base.py:328\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[1;32m    327\u001b[0m     observation \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 328\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[1;32m    331\u001b[0m     )\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ToolException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_tool_error:\n",
            "File \u001b[0;32m~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/tools/base.py:499\u001b[0m, in \u001b[0;36mTool._run\u001b[0;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc:\n\u001b[1;32m    497\u001b[0m     new_argument_supported \u001b[38;5;241m=\u001b[39m signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 499\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_argument_supported\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    506\u001b[0m     )\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool does not support sync\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/base.py:487\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    488\u001b[0m         _output_key\n\u001b[1;32m    489\u001b[0m     ]\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    493\u001b[0m         _output_key\n\u001b[1;32m    494\u001b[0m     ]\n",
            "File \u001b[0;32m~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/base.py:292\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    293\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    294\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    295\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    296\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/base.py:286\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    279\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    280\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    281\u001b[0m     inputs,\n\u001b[1;32m    282\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    289\u001b[0m     )\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
            "File \u001b[0;32m~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/llm_math/base.py:150\u001b[0m, in \u001b[0;36mLLMMathChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    144\u001b[0m _run_manager\u001b[38;5;241m.\u001b[39mon_text(inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key])\n\u001b[1;32m    145\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m    146\u001b[0m     question\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key],\n\u001b[1;32m    147\u001b[0m     stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```output\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    148\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(),\n\u001b[1;32m    149\u001b[0m )\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_llm_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_run_manager\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/llm_math/base.py:104\u001b[0m, in \u001b[0;36mLLMMathChain._process_llm_result\u001b[0;34m(self, llm_output, run_manager)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_match:\n\u001b[1;32m    103\u001b[0m     expression \u001b[38;5;241m=\u001b[39m text_match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_expression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpression\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_text(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    106\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_text(output, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myellow\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
            "File \u001b[0;32m~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/llm_math/base.py:88\u001b[0m, in \u001b[0;36mLLMMathChain._evaluate_expression\u001b[0;34m(self, expression)\u001b[0m\n\u001b[1;32m     80\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m     81\u001b[0m         numexpr\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[1;32m     82\u001b[0m             expression\u001b[38;5;241m.\u001b[39mstrip(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m         )\n\u001b[1;32m     86\u001b[0m     )\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLLMMathChain._evaluate(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpression\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) raised error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please try again with a valid numerical expression\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m     )\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Remove any leading and trailing brackets from the output\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m[|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m]$\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, output)\n",
            "\u001b[0;31mValueError\u001b[0m: LLMMathChain._evaluate(\"\ndate.today() - timedelta(days=1)\n\") raised error: Expression date.today() - timedelta(days=1) has forbidden control characters.. Please try again with a valid numerical expression"
          ]
        }
      ],
      "source": [
        "agent_executor.invoke({\"input\": \"Qué fecha fue ayer?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c3de053-c630-4d4d-945f-fc4c9bfab07f",
      "metadata": {
        "id": "7c3de053-c630-4d4d-945f-fc4c9bfab07f"
      },
      "outputs": [],
      "source": [
        "agent_executor_chat = initialize_agent(tools=tools, llm=llm_chat, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9232890c-01e8-44bf-a371-4135f5c21c75",
      "metadata": {
        "id": "9232890c-01e8-44bf-a371-4135f5c21c75",
        "outputId": "6d19bcbb-1e41-48cd-8590-95ae29be9da7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to subtract the number of remaining dogs from the original number of dogs.\n",
            "Action: Calculator\n",
            "Action Input: 9 - 3\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mAnswer: 6\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know that I lost 6 dogs.\n",
            "Final Answer: I lost 6 dogs.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Si tenía 9 perritos, no me quedan más que 3, calcula cuántos perdí',\n",
              " 'output': 'I lost 6 dogs.'}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor_chat.invoke({\"input\": \"Si tenía 9 perritos, no me quedan más que 3, calcula cuántos perdí\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad3c3f3a-7e32-4108-9a8c-5e773bb4fde1",
      "metadata": {
        "id": "ad3c3f3a-7e32-4108-9a8c-5e773bb4fde1"
      },
      "outputs": [],
      "source": [
        "from langchain.tools.python.tool import PythonREPLTool\n",
        "from langchain.agents.agent_toolkits import create_python_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae40ed3e-776f-4aa6-90ee-75af5b354f12",
      "metadata": {
        "id": "ae40ed3e-776f-4aa6-90ee-75af5b354f12"
      },
      "outputs": [],
      "source": [
        "agent_executor = create_python_agent(\n",
        "    llm=OpenAI(temperature=0, max_tokens=1000),\n",
        "    tool=PythonREPLTool(),\n",
        "    verbose=True,\n",
        "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c24fd28-7b48-4b67-829e-6264db405888",
      "metadata": {
        "id": "0c24fd28-7b48-4b67-829e-6264db405888",
        "outputId": "774583ef-350e-4d71-c889-781cb07eb74c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Python REPL can execute arbitrary code. Use with caution.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m I need to get the current date and subtract one day\n",
            "Action: Python_REPL\n",
            "Action Input: from datetime import date; print(date.today() - timedelta(days=1))\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mNameError(\"name 'timedelta' is not defined\")\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to import the timedelta module\n",
            "Action: Python_REPL\n",
            "Action Input: from datetime import date, timedelta; print(date.today() - timedelta(days=1))\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m2023-09-20\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Ayer fue el 20 de septiembre de 2023.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Qué fecha fue ayer?',\n",
              " 'output': 'Ayer fue el 20 de septiembre de 2023.'}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke({\"input\": \"Qué fecha fue ayer?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ecc9144-b18c-444e-8129-c8059390a99f",
      "metadata": {
        "id": "7ecc9144-b18c-444e-8129-c8059390a99f"
      },
      "source": [
        "## 3. Memory and Tools\n",
        "\n",
        "- Extender capacidades\n",
        "- Disminuir errores\n",
        "- Crear contexto y continuidad\n",
        "\n",
        "<center><img src=\"https://diagnosemlpdf.s3.us-east-2.amazonaws.com/langchain_crash_class/tools_memory.png\" alt=\"chains\" width=\"800\"/></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bef38fdb-6a6d-4994-9b30-e55edf2388b9",
      "metadata": {
        "id": "bef38fdb-6a6d-4994-9b30-e55edf2388b9"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1af46e6-24ee-44ee-9611-7fd0a56dbd7e",
      "metadata": {
        "id": "f1af46e6-24ee-44ee-9611-7fd0a56dbd7e",
        "outputId": "8bbeeaa1-7371-4b8e-abd3-66d74b110725"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bgg/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/llm_math/base.py:51: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LLMMathChain(memory=None, callbacks=None, callback_manager=None, verbose=True, tags=None, metadata=None, llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n', template_format='f-string', validate_template=True), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=StrOutputParser(), return_final_only=True, llm_kwargs={}), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n', template_format='f-string', validate_template=True), input_key='question', output_key='answer')"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import LLMMathChain\n",
        "\n",
        "llm_math_chain = LLMMathChain(llm=llm, verbose=True)\n",
        "llm_math_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6001be2-d8d0-48b3-b311-7b44bb136c87",
      "metadata": {
        "id": "e6001be2-d8d0-48b3-b311-7b44bb136c87",
        "outputId": "fc65b816-965f-4342-ee6a-61417a01c790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
            "\n",
            "Question: ${{Question with math problem.}}\n",
            "```text\n",
            "${{single line mathematical expression that solves the problem}}\n",
            "```\n",
            "...numexpr.evaluate(text)...\n",
            "```output\n",
            "${{Output of running the code}}\n",
            "```\n",
            "Answer: ${{Answer}}\n",
            "\n",
            "Begin.\n",
            "\n",
            "Question: What is 37593 * 67?\n",
            "```text\n",
            "37593 * 67\n",
            "```\n",
            "...numexpr.evaluate(\"37593 * 67\")...\n",
            "```output\n",
            "2518731\n",
            "```\n",
            "Answer: 2518731\n",
            "\n",
            "Question: 37593^(1/5)\n",
            "```text\n",
            "37593**(1/5)\n",
            "```\n",
            "...numexpr.evaluate(\"37593**(1/5)\")...\n",
            "```output\n",
            "8.222831614237718\n",
            "```\n",
            "Answer: 8.222831614237718\n",
            "\n",
            "Question: {question}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(llm_math_chain.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2693fc0-e13b-4b24-acbd-ba23d2dc4bac",
      "metadata": {
        "id": "c2693fc0-e13b-4b24-acbd-ba23d2dc4bac"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ea94213-9729-4786-924f-f50be9ddab3d",
      "metadata": {
        "id": "4ea94213-9729-4786-924f-f50be9ddab3d",
        "outputId": "ff5ca591-8d89-469f-c328-d7c55e604b9e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DÃ­a</th>\n",
              "      <th>Enero</th>\n",
              "      <th>Febrero</th>\n",
              "      <th>Marzo</th>\n",
              "      <th>Abril</th>\n",
              "      <th>Mayo</th>\n",
              "      <th>Junio</th>\n",
              "      <th>Julio</th>\n",
              "      <th>Agosto</th>\n",
              "      <th>Septiembre</th>\n",
              "      <th>Octubre</th>\n",
              "      <th>Noviembre</th>\n",
              "      <th>Diciembre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>35.122,26</td>\n",
              "      <td>35.290,91</td>\n",
              "      <td>35.519,79</td>\n",
              "      <td>35.574,33</td>\n",
              "      <td>35.851,62</td>\n",
              "      <td>36.036,37</td>\n",
              "      <td>36.090,68</td>\n",
              "      <td>36.046,72</td>\n",
              "      <td>36.134,97</td>\n",
              "      <td>36.198,73</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>35.133,53</td>\n",
              "      <td>35.294,32</td>\n",
              "      <td>35.529,90</td>\n",
              "      <td>35.573,19</td>\n",
              "      <td>35.864,70</td>\n",
              "      <td>36.039,85</td>\n",
              "      <td>36.091,89</td>\n",
              "      <td>36.044,39</td>\n",
              "      <td>36.139,62</td>\n",
              "      <td>36.199,94</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>35.144,81</td>\n",
              "      <td>35.297,73</td>\n",
              "      <td>35.540,01</td>\n",
              "      <td>35.572,04</td>\n",
              "      <td>35.877,78</td>\n",
              "      <td>36.043,34</td>\n",
              "      <td>36.093,09</td>\n",
              "      <td>36.042,06</td>\n",
              "      <td>36.144,27</td>\n",
              "      <td>36.201,14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>35.156,09</td>\n",
              "      <td>35.301,14</td>\n",
              "      <td>35.550,13</td>\n",
              "      <td>35.570,89</td>\n",
              "      <td>35.890,87</td>\n",
              "      <td>36.046,82</td>\n",
              "      <td>36.094,29</td>\n",
              "      <td>36.039,73</td>\n",
              "      <td>36.148,93</td>\n",
              "      <td>36.202,35</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>35.167,38</td>\n",
              "      <td>35.304,55</td>\n",
              "      <td>35.560,24</td>\n",
              "      <td>35.569,74</td>\n",
              "      <td>35.903,96</td>\n",
              "      <td>36.050,30</td>\n",
              "      <td>36.095,49</td>\n",
              "      <td>36.037,41</td>\n",
              "      <td>36.153,58</td>\n",
              "      <td>36.203,56</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>35.178,67</td>\n",
              "      <td>35.307,96</td>\n",
              "      <td>35.570,37</td>\n",
              "      <td>35.568,59</td>\n",
              "      <td>35.917,05</td>\n",
              "      <td>36.053,79</td>\n",
              "      <td>36.096,70</td>\n",
              "      <td>36.035,08</td>\n",
              "      <td>36.158,24</td>\n",
              "      <td>36.204,76</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>35.189,96</td>\n",
              "      <td>35.311,37</td>\n",
              "      <td>35.580,49</td>\n",
              "      <td>35.567,44</td>\n",
              "      <td>35.930,15</td>\n",
              "      <td>36.057,27</td>\n",
              "      <td>36.097,90</td>\n",
              "      <td>36.032,75</td>\n",
              "      <td>36.162,90</td>\n",
              "      <td>36.205,97</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>35.201,26</td>\n",
              "      <td>35.314,79</td>\n",
              "      <td>35.590,62</td>\n",
              "      <td>35.566,30</td>\n",
              "      <td>35.943,26</td>\n",
              "      <td>36.060,75</td>\n",
              "      <td>36.099,10</td>\n",
              "      <td>36.030,43</td>\n",
              "      <td>36.167,55</td>\n",
              "      <td>36.207,18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>35.212,56</td>\n",
              "      <td>35.318,20</td>\n",
              "      <td>35.600,75</td>\n",
              "      <td>35.565,15</td>\n",
              "      <td>35.956,37</td>\n",
              "      <td>36.064,24</td>\n",
              "      <td>36.100,30</td>\n",
              "      <td>36.028,10</td>\n",
              "      <td>36.172,21</td>\n",
              "      <td>36.208,38</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>35.215,96</td>\n",
              "      <td>35.328,25</td>\n",
              "      <td>35.599,60</td>\n",
              "      <td>35.578,12</td>\n",
              "      <td>35.959,84</td>\n",
              "      <td>36.065,44</td>\n",
              "      <td>36.097,97</td>\n",
              "      <td>36.032,74</td>\n",
              "      <td>36.173,42</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>35.219,37</td>\n",
              "      <td>35.338,31</td>\n",
              "      <td>35.598,45</td>\n",
              "      <td>35.591,10</td>\n",
              "      <td>35.963,32</td>\n",
              "      <td>36.066,64</td>\n",
              "      <td>36.095,64</td>\n",
              "      <td>36.037,38</td>\n",
              "      <td>36.174,62</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>35.222,77</td>\n",
              "      <td>35.348,37</td>\n",
              "      <td>35.597,30</td>\n",
              "      <td>35.604,08</td>\n",
              "      <td>35.966,79</td>\n",
              "      <td>36.067,84</td>\n",
              "      <td>36.093,31</td>\n",
              "      <td>36.042,02</td>\n",
              "      <td>36.175,83</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>35.226,17</td>\n",
              "      <td>35.358,43</td>\n",
              "      <td>35.596,15</td>\n",
              "      <td>35.617,07</td>\n",
              "      <td>35.970,27</td>\n",
              "      <td>36.069,05</td>\n",
              "      <td>36.090,98</td>\n",
              "      <td>36.046,66</td>\n",
              "      <td>36.177,03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>35.229,58</td>\n",
              "      <td>35.368,49</td>\n",
              "      <td>35.595,01</td>\n",
              "      <td>35.630,06</td>\n",
              "      <td>35.973,75</td>\n",
              "      <td>36.070,25</td>\n",
              "      <td>36.088,64</td>\n",
              "      <td>36.051,31</td>\n",
              "      <td>36.178,24</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>35.232,98</td>\n",
              "      <td>35.378,56</td>\n",
              "      <td>35.593,86</td>\n",
              "      <td>35.643,05</td>\n",
              "      <td>35.977,22</td>\n",
              "      <td>36.071,45</td>\n",
              "      <td>36.086,31</td>\n",
              "      <td>36.055,95</td>\n",
              "      <td>36.179,44</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16</td>\n",
              "      <td>35.236,39</td>\n",
              "      <td>35.388,63</td>\n",
              "      <td>35.592,71</td>\n",
              "      <td>35.656,05</td>\n",
              "      <td>35.980,70</td>\n",
              "      <td>36.072,65</td>\n",
              "      <td>36.083,98</td>\n",
              "      <td>36.060,59</td>\n",
              "      <td>36.180,65</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17</td>\n",
              "      <td>35.239,79</td>\n",
              "      <td>35.398,70</td>\n",
              "      <td>35.591,56</td>\n",
              "      <td>35.669,06</td>\n",
              "      <td>35.984,18</td>\n",
              "      <td>36.073,85</td>\n",
              "      <td>36.081,65</td>\n",
              "      <td>36.065,24</td>\n",
              "      <td>36.181,85</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18</td>\n",
              "      <td>35.243,20</td>\n",
              "      <td>35.408,77</td>\n",
              "      <td>35.590,41</td>\n",
              "      <td>35.682,07</td>\n",
              "      <td>35.987,65</td>\n",
              "      <td>36.075,06</td>\n",
              "      <td>36.079,32</td>\n",
              "      <td>36.069,88</td>\n",
              "      <td>36.183,06</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19</td>\n",
              "      <td>35.246,60</td>\n",
              "      <td>35.418,85</td>\n",
              "      <td>35.589,26</td>\n",
              "      <td>35.695,08</td>\n",
              "      <td>35.991,13</td>\n",
              "      <td>36.076,26</td>\n",
              "      <td>36.076,99</td>\n",
              "      <td>36.074,53</td>\n",
              "      <td>36.184,26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20</td>\n",
              "      <td>35.250,01</td>\n",
              "      <td>35.428,93</td>\n",
              "      <td>35.588,11</td>\n",
              "      <td>35.708,10</td>\n",
              "      <td>35.994,61</td>\n",
              "      <td>36.077,46</td>\n",
              "      <td>36.074,66</td>\n",
              "      <td>36.079,17</td>\n",
              "      <td>36.185,47</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>21</td>\n",
              "      <td>35.253,41</td>\n",
              "      <td>35.439,02</td>\n",
              "      <td>35.586,96</td>\n",
              "      <td>35.721,12</td>\n",
              "      <td>35.998,09</td>\n",
              "      <td>36.078,66</td>\n",
              "      <td>36.072,33</td>\n",
              "      <td>36.083,82</td>\n",
              "      <td>36.186,67</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>22</td>\n",
              "      <td>35.256,82</td>\n",
              "      <td>35.449,10</td>\n",
              "      <td>35.585,82</td>\n",
              "      <td>35.734,15</td>\n",
              "      <td>36.001,57</td>\n",
              "      <td>36.079,86</td>\n",
              "      <td>36.070,00</td>\n",
              "      <td>36.088,46</td>\n",
              "      <td>36.187,88</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>23</td>\n",
              "      <td>35.260,23</td>\n",
              "      <td>35.459,19</td>\n",
              "      <td>35.584,67</td>\n",
              "      <td>35.747,19</td>\n",
              "      <td>36.005,05</td>\n",
              "      <td>36.081,07</td>\n",
              "      <td>36.067,68</td>\n",
              "      <td>36.093,11</td>\n",
              "      <td>36.189,09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>24</td>\n",
              "      <td>35.263,64</td>\n",
              "      <td>35.469,28</td>\n",
              "      <td>35.583,52</td>\n",
              "      <td>35.760,22</td>\n",
              "      <td>36.008,52</td>\n",
              "      <td>36.082,27</td>\n",
              "      <td>36.065,35</td>\n",
              "      <td>36.097,76</td>\n",
              "      <td>36.190,29</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>25</td>\n",
              "      <td>35.267,04</td>\n",
              "      <td>35.479,38</td>\n",
              "      <td>35.582,37</td>\n",
              "      <td>35.773,27</td>\n",
              "      <td>36.012,00</td>\n",
              "      <td>36.083,47</td>\n",
              "      <td>36.063,02</td>\n",
              "      <td>36.102,41</td>\n",
              "      <td>36.191,50</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>26</td>\n",
              "      <td>35.270,45</td>\n",
              "      <td>35.489,48</td>\n",
              "      <td>35.581,22</td>\n",
              "      <td>35.786,31</td>\n",
              "      <td>36.015,48</td>\n",
              "      <td>36.084,67</td>\n",
              "      <td>36.060,69</td>\n",
              "      <td>36.107,06</td>\n",
              "      <td>36.192,70</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>27</td>\n",
              "      <td>35.273,86</td>\n",
              "      <td>35.499,58</td>\n",
              "      <td>35.580,07</td>\n",
              "      <td>35.799,37</td>\n",
              "      <td>36.018,96</td>\n",
              "      <td>36.085,87</td>\n",
              "      <td>36.058,36</td>\n",
              "      <td>36.111,71</td>\n",
              "      <td>36.193,91</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>28</td>\n",
              "      <td>35.277,27</td>\n",
              "      <td>35.509,68</td>\n",
              "      <td>35.578,93</td>\n",
              "      <td>35.812,42</td>\n",
              "      <td>36.022,44</td>\n",
              "      <td>36.087,08</td>\n",
              "      <td>36.056,03</td>\n",
              "      <td>36.116,36</td>\n",
              "      <td>36.195,11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>29</td>\n",
              "      <td>35.280,68</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35.577,78</td>\n",
              "      <td>35.825,49</td>\n",
              "      <td>36.025,93</td>\n",
              "      <td>36.088,28</td>\n",
              "      <td>36.053,70</td>\n",
              "      <td>36.121,01</td>\n",
              "      <td>36.196,32</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>30</td>\n",
              "      <td>35.284,09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35.576,63</td>\n",
              "      <td>35.838,55</td>\n",
              "      <td>36.029,41</td>\n",
              "      <td>36.089,48</td>\n",
              "      <td>36.051,37</td>\n",
              "      <td>36.125,66</td>\n",
              "      <td>36.197,53</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>31</td>\n",
              "      <td>35.287,50</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35.575,48</td>\n",
              "      <td>NaN</td>\n",
              "      <td>36.032,89</td>\n",
              "      <td>NaN</td>\n",
              "      <td>36.049,05</td>\n",
              "      <td>36.130,31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    DÃ­a      Enero    Febrero      Marzo      Abril       Mayo      Junio  \\\n",
              "0      1  35.122,26  35.290,91  35.519,79  35.574,33  35.851,62  36.036,37   \n",
              "1      2  35.133,53  35.294,32  35.529,90  35.573,19  35.864,70  36.039,85   \n",
              "2      3  35.144,81  35.297,73  35.540,01  35.572,04  35.877,78  36.043,34   \n",
              "3      4  35.156,09  35.301,14  35.550,13  35.570,89  35.890,87  36.046,82   \n",
              "4      5  35.167,38  35.304,55  35.560,24  35.569,74  35.903,96  36.050,30   \n",
              "5      6  35.178,67  35.307,96  35.570,37  35.568,59  35.917,05  36.053,79   \n",
              "6      7  35.189,96  35.311,37  35.580,49  35.567,44  35.930,15  36.057,27   \n",
              "7      8  35.201,26  35.314,79  35.590,62  35.566,30  35.943,26  36.060,75   \n",
              "8      9  35.212,56  35.318,20  35.600,75  35.565,15  35.956,37  36.064,24   \n",
              "9     10  35.215,96  35.328,25  35.599,60  35.578,12  35.959,84  36.065,44   \n",
              "10    11  35.219,37  35.338,31  35.598,45  35.591,10  35.963,32  36.066,64   \n",
              "11    12  35.222,77  35.348,37  35.597,30  35.604,08  35.966,79  36.067,84   \n",
              "12    13  35.226,17  35.358,43  35.596,15  35.617,07  35.970,27  36.069,05   \n",
              "13    14  35.229,58  35.368,49  35.595,01  35.630,06  35.973,75  36.070,25   \n",
              "14    15  35.232,98  35.378,56  35.593,86  35.643,05  35.977,22  36.071,45   \n",
              "15    16  35.236,39  35.388,63  35.592,71  35.656,05  35.980,70  36.072,65   \n",
              "16    17  35.239,79  35.398,70  35.591,56  35.669,06  35.984,18  36.073,85   \n",
              "17    18  35.243,20  35.408,77  35.590,41  35.682,07  35.987,65  36.075,06   \n",
              "18    19  35.246,60  35.418,85  35.589,26  35.695,08  35.991,13  36.076,26   \n",
              "19    20  35.250,01  35.428,93  35.588,11  35.708,10  35.994,61  36.077,46   \n",
              "20    21  35.253,41  35.439,02  35.586,96  35.721,12  35.998,09  36.078,66   \n",
              "21    22  35.256,82  35.449,10  35.585,82  35.734,15  36.001,57  36.079,86   \n",
              "22    23  35.260,23  35.459,19  35.584,67  35.747,19  36.005,05  36.081,07   \n",
              "23    24  35.263,64  35.469,28  35.583,52  35.760,22  36.008,52  36.082,27   \n",
              "24    25  35.267,04  35.479,38  35.582,37  35.773,27  36.012,00  36.083,47   \n",
              "25    26  35.270,45  35.489,48  35.581,22  35.786,31  36.015,48  36.084,67   \n",
              "26    27  35.273,86  35.499,58  35.580,07  35.799,37  36.018,96  36.085,87   \n",
              "27    28  35.277,27  35.509,68  35.578,93  35.812,42  36.022,44  36.087,08   \n",
              "28    29  35.280,68        NaN  35.577,78  35.825,49  36.025,93  36.088,28   \n",
              "29    30  35.284,09        NaN  35.576,63  35.838,55  36.029,41  36.089,48   \n",
              "30    31  35.287,50        NaN  35.575,48        NaN  36.032,89        NaN   \n",
              "\n",
              "        Julio     Agosto Septiembre    Octubre  Noviembre  Diciembre  \n",
              "0   36.090,68  36.046,72  36.134,97  36.198,73        NaN        NaN  \n",
              "1   36.091,89  36.044,39  36.139,62  36.199,94        NaN        NaN  \n",
              "2   36.093,09  36.042,06  36.144,27  36.201,14        NaN        NaN  \n",
              "3   36.094,29  36.039,73  36.148,93  36.202,35        NaN        NaN  \n",
              "4   36.095,49  36.037,41  36.153,58  36.203,56        NaN        NaN  \n",
              "5   36.096,70  36.035,08  36.158,24  36.204,76        NaN        NaN  \n",
              "6   36.097,90  36.032,75  36.162,90  36.205,97        NaN        NaN  \n",
              "7   36.099,10  36.030,43  36.167,55  36.207,18        NaN        NaN  \n",
              "8   36.100,30  36.028,10  36.172,21  36.208,38        NaN        NaN  \n",
              "9   36.097,97  36.032,74  36.173,42        NaN        NaN        NaN  \n",
              "10  36.095,64  36.037,38  36.174,62        NaN        NaN        NaN  \n",
              "11  36.093,31  36.042,02  36.175,83        NaN        NaN        NaN  \n",
              "12  36.090,98  36.046,66  36.177,03        NaN        NaN        NaN  \n",
              "13  36.088,64  36.051,31  36.178,24        NaN        NaN        NaN  \n",
              "14  36.086,31  36.055,95  36.179,44        NaN        NaN        NaN  \n",
              "15  36.083,98  36.060,59  36.180,65        NaN        NaN        NaN  \n",
              "16  36.081,65  36.065,24  36.181,85        NaN        NaN        NaN  \n",
              "17  36.079,32  36.069,88  36.183,06        NaN        NaN        NaN  \n",
              "18  36.076,99  36.074,53  36.184,26        NaN        NaN        NaN  \n",
              "19  36.074,66  36.079,17  36.185,47        NaN        NaN        NaN  \n",
              "20  36.072,33  36.083,82  36.186,67        NaN        NaN        NaN  \n",
              "21  36.070,00  36.088,46  36.187,88        NaN        NaN        NaN  \n",
              "22  36.067,68  36.093,11  36.189,09        NaN        NaN        NaN  \n",
              "23  36.065,35  36.097,76  36.190,29        NaN        NaN        NaN  \n",
              "24  36.063,02  36.102,41  36.191,50        NaN        NaN        NaN  \n",
              "25  36.060,69  36.107,06  36.192,70        NaN        NaN        NaN  \n",
              "26  36.058,36  36.111,71  36.193,91        NaN        NaN        NaN  \n",
              "27  36.056,03  36.116,36  36.195,11        NaN        NaN        NaN  \n",
              "28  36.053,70  36.121,01  36.196,32        NaN        NaN        NaN  \n",
              "29  36.051,37  36.125,66  36.197,53        NaN        NaN        NaN  \n",
              "30  36.049,05  36.130,31        NaN        NaN        NaN        NaN  "
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfs = pd.read_html('https://si3.bcentral.cl/indicadoressiete/secure/Serie.aspx?gcode=UF&param=RABmAFYAWQB3AGYAaQBuAEkALQAzADUAbgBNAGgAaAAkADUAVwBQAC4AbQBYADAARwBOAGUAYwBjACMAQQBaAHAARgBhAGcAUABTAGUAYwBsAEMAMQA0AE0AawBLAF8AdQBDACQASABzAG0AXwA2AHQAawBvAFcAZwBKAEwAegBzAF8AbgBMAHIAYgBDAC4ARQA3AFUAVwB4AFIAWQBhAEEAOABkAHkAZwAxAEEARAA=')\n",
        "dfs[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9e38ac3-3ee0-4564-a3ec-3c890acd932f",
      "metadata": {
        "id": "d9e38ac3-3ee0-4564-a3ec-3c890acd932f"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_pandas_dataframe_agent\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e87599d6-327f-4dbf-9dcc-893c02cf91c8",
      "metadata": {
        "id": "e87599d6-327f-4dbf-9dcc-893c02cf91c8"
      },
      "outputs": [],
      "source": [
        "df_1 = deepcopy(dfs[1])\n",
        "df_2 = deepcopy(dfs[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01566656-acbf-465a-950d-9f4b36ba5c2b",
      "metadata": {
        "id": "01566656-acbf-465a-950d-9f4b36ba5c2b"
      },
      "outputs": [],
      "source": [
        "agent = create_pandas_dataframe_agent(OpenAI(temperature=0), df_1, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "790eda17-1389-49b5-a185-b9fa39b01c16",
      "metadata": {
        "id": "790eda17-1389-49b5-a185-b9fa39b01c16",
        "outputId": "9292080e-3628-4ba1-e0ce-fe66402a8b73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: Necesito eliminar todas las columnas excepto las dos que necesito\n",
            "Action: python_repl_ast\n",
            "Action Input: df.drop(df.columns.difference(['Día', 'Enero']), 1, inplace=True)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mTypeError: drop() takes from 1 to 2 positional arguments but 3 positional arguments (and 1 keyword-only argument) were given\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m Necesito especificar los argumentos\n",
            "Action: python_repl_ast\n",
            "Action Input: df.drop(df.columns.difference(['Día', 'Enero']), axis=1, inplace=True)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m Ahora tengo la tabla con las dos columnas que necesito\n",
            "Final Answer: La tabla ahora tiene dos columnas, 'Día' y 'Enero'.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"La tabla ahora tiene dos columnas, 'Día' y 'Enero'.\""
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.run(\"Modificalo para que tenga solo 2 columnas, fecha y valor\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d86022c-5e5b-4f66-86f5-96d94242e292",
      "metadata": {
        "scrolled": true,
        "id": "8d86022c-5e5b-4f66-86f5-96d94242e292",
        "outputId": "972aebf5-1842-4dcf-924b-470e4b21f712"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Enero</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>35.122,26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>35.133,53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>35.144,81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>35.156,09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>35.167,38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>35.178,67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>35.189,96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>35.201,26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>35.212,56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>35.215,96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>35.219,37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>35.222,77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>35.226,17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>35.229,58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>35.232,98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>35.236,39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>35.239,79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>35.243,20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>35.246,60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>35.250,01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>35.253,41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>35.256,82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>35.260,23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>35.263,64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>35.267,04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>35.270,45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>35.273,86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>35.277,27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>35.280,68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>35.284,09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>35.287,50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Enero\n",
              "0   35.122,26\n",
              "1   35.133,53\n",
              "2   35.144,81\n",
              "3   35.156,09\n",
              "4   35.167,38\n",
              "5   35.178,67\n",
              "6   35.189,96\n",
              "7   35.201,26\n",
              "8   35.212,56\n",
              "9   35.215,96\n",
              "10  35.219,37\n",
              "11  35.222,77\n",
              "12  35.226,17\n",
              "13  35.229,58\n",
              "14  35.232,98\n",
              "15  35.236,39\n",
              "16  35.239,79\n",
              "17  35.243,20\n",
              "18  35.246,60\n",
              "19  35.250,01\n",
              "20  35.253,41\n",
              "21  35.256,82\n",
              "22  35.260,23\n",
              "23  35.263,64\n",
              "24  35.267,04\n",
              "25  35.270,45\n",
              "26  35.273,86\n",
              "27  35.277,27\n",
              "28  35.280,68\n",
              "29  35.284,09\n",
              "30  35.287,50"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ac8febd-f688-4d24-983c-5e0b0563eb45",
      "metadata": {
        "id": "5ac8febd-f688-4d24-983c-5e0b0563eb45"
      },
      "outputs": [],
      "source": [
        "agent_openai_fx = create_pandas_dataframe_agent(\n",
        "    ChatOpenAI(temperature=0, model=\"gpt-4\"),\n",
        "    df_1,\n",
        "    verbose=True,\n",
        "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b889a485-d614-4372-8530-f0f235acc721",
      "metadata": {
        "id": "b889a485-d614-4372-8530-f0f235acc721",
        "outputId": "a2414517-2787-45f8-ae32-1fbb0516dbb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `python_repl_ast` with `{'query': \"df['Enero'] = df['Enero'].str.replace('.', '').str.replace(',', '.').astype(float)\\ndf[df['Enero'] > 35280].count()\"}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mEnero    3\n",
            "dtype: int64\u001b[0m\u001b[32;1m\u001b[1;3mHay 3 valores en la columna 'Enero' que son mayores a 35280.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Hay 3 valores en la columna 'Enero' que son mayores a 35280.\""
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_openai_fx.run(\"Cuantos son mayores de 35280 de la columna enero?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0571d0b8-e8bc-408e-8d5d-2efabd640068",
      "metadata": {
        "id": "0571d0b8-e8bc-408e-8d5d-2efabd640068"
      },
      "source": [
        "## 5. Document Loaders\n",
        "\n",
        "- Decenas de integraciones listas\n",
        "- Conexiones rápidas low code\n",
        "\n",
        "<center><img src=\"https://diagnosemlpdf.s3.us-east-2.amazonaws.com/langchain_crash_class/langchain_chain.png\" alt=\"chains\" width=\"800\"/></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b9e2606-223d-48f0-9c44-4a16ec29cfc3",
      "metadata": {
        "id": "6b9e2606-223d-48f0-9c44-4a16ec29cfc3"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import YoutubeLoader\n",
        "from langchain.document_loaders import TextLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b6f2fb3-03d3-4a83-b04c-7273724c74ed",
      "metadata": {
        "id": "4b6f2fb3-03d3-4a83-b04c-7273724c74ed"
      },
      "outputs": [],
      "source": [
        "loader_u2 = YoutubeLoader.from_youtube_url(\n",
        "    \"https://www.youtube.com/watch?v=pfjWK5ojbRE\", add_video_info=True, language=\"es\",\n",
        ")\n",
        "\n",
        "loader_note = TextLoader(\"./Crash Class.md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "595e0ee5-7417-4f63-bf79-494d79672d1b",
      "metadata": {
        "id": "595e0ee5-7417-4f63-bf79-494d79672d1b",
        "outputId": "da3ad1d5-817d-4f12-a154-aa1b40084d09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='decir que no no podía caminar Ok es todo bueno fue un daño que se produjo a nivel de la médula espinal luego un accidente en bicicleta Ok entonces lo que hicieron una serie de médicos y de informáticos y biotecnólogos y hay una diferencia entre envío informativa y biotecnología que dentro de un ratito les voy a comentar bello medicinas también no como un montón de personas dedicadas al área que dijeron Hey vamos a investigar esto y Qué es la interfaz cerebro ordenador un puente digital entre el cerebro y la médula espinal si la médula espinal básicamente es materia que recorre adentro de nuestra vértebra de nuestra columna vertebral nuestros huesos Sí para explicarlo a groso modo y básicamente este puente entre lo digital y lo analógico que es el cuerpo humano le permitió a este paciente volver a caminar volver a tener control sobre sus movimientos en sus piernas bueno no solo sus piernas pero en este caso le ha servido porque era algo que le faltaba a esta persona así el poder controlar y mover sus extremidades inferiores y bueno y puedo ponerse de pie volver a caminar subir escaleras la verdad que esta noticia no tiene menos de una semana hoy 6 de junio de 2023 ok súper interesante la verdad y bueno lo que hace esta interfaz es transformar el pensamiento en acción Ok esto que les comentaba de las neuronas al principio de la charla bueno básicamente eso hay una diferencia entre bioinformática y biotecnología y biomedicina bioinformática es cuando nosotros hablamos de software en específico programas que ayudan a la investigación científica Okay cuando estamos hablando de biotecnología son tecnologías aplicadas a en este caso la esta persona que volvió a caminar si tiene un montón de cosas como una especie de casco o algo así que lo que hace es medir la un accesorio No es cierto que lo que hace es medir las señales nerviosas del cuerpo y eso se pasa un dispositivo ese dispositivo finalmente a un procesador que es el que procesa finalmente estos datos O sea que estamos hablando como de accesorios si se quiere Esa es la biotecnología como más allá del Software Okay no está centrado en programación código o herramientas no code de Inteligencia artificial sino que es algo más tangible OK Bueno y la biomedicina que son las aplicaciones al diagnóstico no en este caso el diagnóstico de la persona tetrapléjica vaya a saber cuál era el diagnóstico Pero bueno digamos parálisis de piernas medicina de precisión Sí esta es otra de la de las y ya vamos terminando si ahora les voy a comentar Data muy interesante sobre donde estudiar esto pero este es el último ejemplo que les traigo medicina de precisión básicamente incluso acá en Argentina se está empezando a implementar esto es el seguimiento personalizado de la historia clínica de cada persona Recuerden que los diagnósticos que se hacen hoy en día en medicina estamos hablando de muestreo de una cierta cierto grupo de personas y a la mayoría que tiene ciertos síntomas se le asigna un diagnóstico pero no siempre se cumple Ok entonces acá estamos hablando de no generalizar los diagnósticos sino medicina de precisión de precisar o personalizar el diagnóstico a cada paciente porque no todos reaccionan igual a los medicamentos no todos tienen los mismos síntomas frente a la misma enfermedad Ok entonces básicamente la medicina de precisión se trata de un enfoque emergente para el tratamiento de prevención de enfermedades que toma en cuenta la variabilidad individual de los genes el ambiente y el estilo de vida de cada persona Bien voy a tomar un poquito de agua y vamos a hablar rapidito sobre qué Necesito aprender bien estamos para los que vienen del mundo de la informática y les interesa este tema qué necesitan aprender bueno en pantalla vemos y para los que nos están viendo la pantalla les voy a leer Necesito aprender un poco sobre conceptos básicos Ok porque eso después a medida que ustedes van estudiando y capacitándose van a ir incorporándolos no pero necesitamos saber un poquito sobre biología general de psicoquímica estadística probabilidad que les comentaba hoy O sea no solo para las ciencias biológicas para el mayor learning y también no para el mundo de datos en general bases de genéticas biología celular biología molecular teorías de la evolución es un tema muy interesante hablar también de cómo evoluciona con el a medida que pasa el tiempo cada una de las especies no voy a entrar mucho en detalle porque me encanta hablar de estos temas química orgánica y bioquímica básica Estos son algunos de los temas que Estaría bueno como ir incorporando si vienen del mundo de la biología medicina nutrición etcétera y no tiene ni idea de informática qué necesitan aprender sistemas operativos de basados en unix sí básicamente todo lo que no sea de Microsoft Se podría decir ahora Microsoft se incorporó sistemas operativos para poder utilizar Mejor dicho el sistema operativo Linux en su en su en su en su sistema operativo en general pero se utiliza mucho Linux también Mac no pero se utiliza muchísimo Linux en este mundito así que está bueno aprender sobre sobre este sistema operativo por lo tanto líneas de comandos un lenguaje de programación puede ser python r que son los más utilizadas No necesariamente solo eso pero son las más utilizadas y donde hay más herramientas para trabajar en datos biológicos en especial r la luz estadísticos y a los biólogos le encanta r yo en particular me gusta más python pero bueno para gustos colores Júpiter notebooks y manejar notebooks en general puede ser de Júpiter puede ser de cualquier otro otro tipo de software pero notebooks git have o git lapse otras herramientas de control de agresiones expresiones regulares hasta yo los odio la verdad es muy muy complicado de estudiarlos pero está bueno saber expresiones regulares Okay archivos de texto que es donde se guarda la información genética ojo archivo fasta.bet.pdb son algunos de los archivos con los que se trabaja bueno obviamente Inteligencia artificial y está buenísimo es lo ideal saber un poquito de cálculo de álgebra lineal una de las herramientas favoritas a mí me encanta lo estuve estudiando hace No mucho tampoco es que soy súper grosa con estas herramientas pero yo que vengo del mundo de python sí la librería Bio python está muy buena estuve analizando el código genético del coronavirus Sí la verdad que está muy bueno tengo una charla en mi portafolio luego les comparto si quieren me pueden buscar como la talla punto deb ahí en mi página web está o en mis redes sociales Pero bueno estuve grabando una charla para nerdearla que es una conferencia de nerds de acá de Argentina donde estuve aplicando Bio python para decodificar un poquito el coronavirus Así que Les recomiendo que que averigüen qué tal esta herramienta Bio python.org que está muy buena lecturas recomendadas sí vida punto exe de bueno de entre varias de sus autores Y coautores tenemos a Germán González Nicolás andaburu y nicoláspoli Nicolás palos hace poco tuvo el placer de conocerlo En una conferencia de informática es un son personas que son miembros de la del RFC Argentina que es la comunidad que les comenté hace un rato donde soy miembro y otro libro también de dos personas que admiro mucho que son Sebastián bassi Virginia González de python para bioinformática que bueno acá ya es un poquito más técnico no el vida.exe es un libro tal vez más de de más teórico pero a la vez con un lenguaje muy de para personas que no vienen del mundo ni de la informática en las ciencias biológicas sino son conceptos explicados de una manera muy divertida y fácil Clara y python para había informática de vacío González que es un libro por ahí más técnico donde hablan de Bio payton justamente y la verdad que bueno son dos personas que admiro mucho también no tuve el placer de conocerlos en persona pero hablamos mucho en redes y siempre están disponibles para cualquier pregunta que tengan al respecto respecto Bueno hablemos rapidito sobre la situación bioinformática en hispanoamérica Lamentablemente en Argentina ha sido si necesitas mínimo un título de grado y ni hablar de posgrado para ejercer la bioinformática entiendo que es la situación de la mayoría de los países de hispanoamérica al menos bueno latinoamérica incluyendo España no es posible estudiar carreras de grado de posgrado de muchos países hispanoamericanos en todos de momento bueno sabemos que hay ofertas de pregrado sí es decir tecnicaturas eh en chile así que muchas personas de chile acá si les interesa averigüen yo no sé en qué universidad pero sé que las hay pocas carreras de grado incluyen un título intermedio como es en el caso de la Universidad Católica de Córdoba de Argentina o incluso la Universidad Nacional de quilmes que es donde yo estoy inscripta tenemos un título intermedio que es la la tecnicatura envió informática pero para eso también tenés que completar el grado de la licenciatura no para las materias son afines Ok existen ofertas públicas y privadas en todas hispanoamérica las licenciaturas suelen durar entre o de grado No necesariamente licenciaturas capaz algún país tenga alguna ingeniería pero más o menos suelen durar entre tres y cuatro años en algunos cinco como mucho la licenciatura de la Universidad de quilmes dura cinco también en Argentina que es una universidad privada tienen esa opción y también existen diversos cursos y mentorías impulsados por las comunidades sí tienen un montón de comunidades de las que vamos a hablar ahora que son bueno la asociación de Argentina de bioinformática y biología computacional [Música] que es una empresa Pero bueno también tiene una comunidad muy muy ocupada Woman envió informatics de latinoamérica atg genomics que ellos tienen también un podcast muy interesante y donde soy miembro yo el Ace Cívico que es la institución madre que abarca todos los rcg o regional student de de todo el mundo Sí donde estábamos el de Argentina el de chile el de España el de Perú entre otros países hace poco habrían bangladesh en India en muchos lados estaba presente acá si quieren saber un poquito más sobre sobre esta sobre esta sesión o institución Sí y scv b larga.org y ahí tienen toda la información También tienen eventos en Argentina tenemos un hospital que está súper adelantado sobre el tema de herramientas bioinformáticas y y bueno medicina personalizada no que es el hospital italiano Sí donde se hacen la mayoría de las investigaciones Además del conicet y hace un evento anual que es el j y ssumith en este caso tenemos el de 2023 que se va a hacer en noviembre son tres días si no me equivoco que bueno Les recomiendo también hospital italiano.org y ahí tienen toda la información al respecto Muchísimas gracias llegamos Justo a los 40 minutos si tienen alguna pregunta al respecto tengo un ratito para responderlas Muchas gracias Natalia Ay sí pueden hacer clic en la manito Y seguramente consultar o alguna duda que tengan algo que quieran volver a revisar o a ver está abierto el espacio inclusive si quieren dar algún comentario o algo que les llamó la atención también hacen clic en la manito que es justamente para para hablar y ahí estamos atentas tengan vergüenza chicos bueno y justo en el en los comentarios estaba seba consultando acerca de bueno los cursos sobre ya y su relación con la vía informática que ahí no estuviste nombrando principalmente en este caso no son cursos Pero serían como invitaciones no talleres no Natalia los últimos que nombraste y son eventos Sí el del hospital italiano elegí sumit 2023 es un evento que se hace bueno híbrido hoy en día se hace emitido antes hacía presencial en Buenos Aires Así que si entran al hospital italiano.org van a poder inscribirse entiendo que es gratuito al menos las veces que yo estuve gratuito y pueden ver todas las últimas investigaciones al respecto las comunidades que nombré como el isbc Grove o los rcg de cada país son grupos de estudiantes no solo de grado de pregrado como en el caso sino también de posgrado postdoterado sí gente de todos los niveles que se dedican a esta área de una manera dentro de la investigación no de una manera más y como un nivel es mucho más grandes de la parte de investigación que pueden sumarse y tienen muchas actividades muy interesantes para capacitarse como cursos talleres simposios todo el año pasado estuve participando en la provincia de corrientes en un simposio internacional que estuvo muy bueno para hacer un poquito a networking en Chile me imagino que si tienen carreras de pregrado es el único país que tiene deben de tener también este tipo de eventos consulten rcg chile o el rfg de cada país Así que les invito a sumarse ahí que tienen un montón de cursos muy interesantes pero recuerden que para ejercer por el momento necesitan mínimo una carrera de una carrera de pregrado de grado Ok esa es uno de los contras pero pueden ir estudiando de cursos Mientras tanto bien muchas gracias y ahí Fer consulta si hay alguna experiencia para ingresar a este mercado laboral y porque él por ejemplo viene del lado biológico ya tiene una licenciatura Y actualmente está avanzado en una diplomatura en ciencia de datos en la tiene todas las herramientas me encanta muy buena universidad esa la universidad de San Martín si no me equivoco de Argentina Sí yo tiene todas las herramientas para para entrar a la vía informática que le escriba a los chicos del rcg Argentina y o con estas personas que les comenté por redes Virginia González Sebastián y Nicolás paleópoli Germán González son personas que le pueden ayudar ya o que mandan algún correo al conicet todo el tiempo están compartiendo búsquedas dentro de lo privado dentro de Argentina también tiene tienen a stam que hace investigaciones muy interesantes al respecto Pero si ya ven con una base de nuevo de grado en lo posible de grado si bien también de pregrado como lo comenté si ya venís con una base Universitaria es cuestión de sea de informática de ciencias biológicas ya tenés todo lo necesario ahora si ya hiciste cursos o ya entras con algún curso base tenés obviamente un currículum mucho más interesante donde probablemente gane los concursos no porque todo esto se hace por concursos normalmente al menos la parte pública eligen los mejores currículums y ahí entran a proyectos de investigación no hay mercado laboral dentro de lo privado dentro de fuera de lo que sea investigación Okay Por el momento buenísimo y también otra pregunta para volver a escuchar la La charla va a estar grabada la vamos a compartir y otra que es si hay un emprendimientos relacionados a vía informática acá en lata sabes alguno Natalia llegaste a ver hay qué movimiento a nivel de emprendimiento hay dentro del campo no no hay muchos dentro bien formal les comentaba esta empresa que creo que se escribe con que tienen investigaciones Y algún que otro producto pero sé que son productos propios porque hacen investigaciones internas y no no están publicadas Ok yo estuve haciendo un proceso de entrevistas con ello lamentablemente no no vivo en Buenos Aires Así que no pude sumarme al Team pero me estuvieron comentando que sí tienen algún que otro producto pero se enfocan en la investigación y no hay emprendimientos Ok más que eso hoy en día a ver la carrera había informática es muy reciente y se enfoca solo en investigación por el momento sí Habría que ver ahí la verdad que no estoy al tanto si Benja ofenda quieren comentar algo si si llegaron a ver algún emprendimiento relacionado a vía informática están también abierto el escenario para que Comenten hacen la Data los que sepan que me interesa también los siguientes comentarios la verdad es que lograste Sí tal cual lograste responder justo también lo que tenían un formulario Respecto a los libros a Cómo empezar a formarse dentro del campo que eso también va a estar disponible para compartirlo y había una pregunta que hace el análisis de datos también les cuento un poquito rapidito todos los roles de datos sí ingeniero en datos Data scientist análisis de datos con todas áreas si ustedes vienen de esas áreas o les interesa está buenísimo que lo estudien fue el camino que yo elegí mientras me formo en la carrera porque te da muchas herramientas como les había comentado anteriormente el Linux un lenguaje de programación para visualización también se usa mucho si bien se usa más r también son herramientas que les puede llegar a servir eso más que nada dentro del análisis de datos para para empezar en este mundito desde el lado del avión informática porque Recuerden que tienen dos caminos de lo Bio o de la info pero son esos son herramientas que las pueden estudiar en cualquier curso de internet en YouTube o Inteligencia artificial por favor métanse al curso de My Future allá y que está muy bueno tienen otras maneras de estudiarlo Más allá de la facultad no genial ahí justo estaban respondiendo algo que había preguntado Nico que es Cómo empezar en el mundo de la bioinformático qué curso básico como ya es la alumna de pregrado de biología alumnos productos que justo esto de ir incorporando estas herramientas bien Además de los de los libros o de las comunidades que les había comentado antes en cursera tienen entiendo que no es gratuito con todas las personas tienen crucera yo al menos sé que podés cursarlo gratis pero para el certificado que en el mundo del avión informática sí es importante los certificados Obviamente que tenés que pagar para acceder No pero en crucera tenés un par A ver déjame un segundo que los Estoy buscando porque tengo en Twitter un montón de información al respecto en segundo precursores tienen algunos algunos cursos no hay muchos online no hay muchos así que son dos o tres que están en esa web creo con la desde la universidad Harvard también pero no más que eso por ahí comentando también nos comentan que por ejemplo en Guatemala hay un posgrado en bien informática por la universidad de San Carlos Estaría bueno saber si es totalmente remoto si es necesario estar o híbrido o virtual Cuál es la modalidad de esa del posgrado y en Ecuador y maestría en biología computacional muy interesante y es online muy muy buen dato se va muy bueno acá tengo el nombre del curso de cursomática de la Universidad de San Diego para que lo busquen yo lo yo lo hice por la mitad de ese está muy bueno muy interesante para para empezar de cero la verdad Bueno las competiciones de cada eso es otro otro comentario que Les recomiendo también genial Bueno ahí está ginet Esperamos que nos está comentando Ahí está más datos de dónde empezar a estudiar por ejemplo está la maestría informática en la Universidad Nacional en Colombia y también hay una maestría en biología computacional también en la universidad de los Andes también desde Colombia excelente montón de edad me encanta porque yo tengo alguna que otra Data de Argentina pero está bueno que aporten un poquito más sobre la realidad de su país Sí ahí está también Inclusive la oportunidad de estudiarlo online que está como comentaba seba de la Universidad Católica de Ecuador se me estaba olvidando también hace poco hizo un curso de la Universidad de valencia creo que era de valencia que era online era un mooc Ok era un curso corto que no lo abre todo el tiempo ahora está cerrado pero lo había hecho de marzo hasta mayo cada tanto suero en abrir también un curso online y gratuito creo que para el certificado hay que pagar pero para empezar en el mundo también está bueno no como un curso bastante introductorio tal cual y podemos cerrar con esta pregunta porque hace seba que es Cuál es la diferencia entre bioinformática y biología computacional que dice que nunca le quedó claro Cuál es la Cuáles son los puntos más comparando entre estas dos ramas de vida informática y biología computacional les voy a matar con esta respuesta para mí son lo mismo cuando hablamos de biología en sí obviamente estamos hablando de la aplicación de de herramientas a la biología computacional y la bioinformática ya bueno de software No pero en esencia son lo mismo La verdad tampoco es que la tengo Claro si es que hay una gran diferencia al respecto si las hay no les voy a mentir No las sé [Música] para seguir investigando porque es un muy buen tema y al principio por ahí está abierto para seguir comentando la verdad que muchas gracias Natalia esta este esta charla va a estar grabada y la vamos a compartir también va a estar disponible si querés compartirnos el material para que puedan seguir profundizando a las personas que puedan seguir buscando información formándose en esta área que también está muy solicitada y que por lo que vemos también es inclusive podemos desarrollar los emprendimientos o ir innovando dentro de las ciencias biológicas y muchas gracias muchas gracias a todos los que están estuvieron escuchando estuvieron en esta hora Natalia sé que ya tenés que dar una clase Así que muchas gracias Y seguramente dentro de dos semanas otra vez vamos a estar haciendo otro evento relacionado análisis de datos Así que estén muy atentos a la comunidad que lo vamos a estar compartiendo y Esto fue todo gracias estamos hablando Gracias a todos Ahora sí genial nos vemos', metadata={'source': 'pfjWK5ojbRE', 'title': 'Hablemos de IA en bioinformática', 'description': 'Unknown', 'view_count': 21, 'thumbnail_url': 'https://i.ytimg.com/vi/pfjWK5ojbRE/hq720.jpg?v=64bb257c', 'publish_date': '2023-07-21 00:00:00', 'length': 1479, 'author': 'Myfuture-AI'})]"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loader_u2.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51f8b5cf-222c-4e60-8a66-2f9b44d75ab0",
      "metadata": {
        "id": "51f8b5cf-222c-4e60-8a66-2f9b44d75ab0",
        "outputId": "027594d3-909c-4bb1-e81c-3632976ca8bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='# Langchain Crash Class\\n\\n1. Prompts & LLMs\\n2. Chains\\n3. Memory and tools\\n4. Agents\\n5. Document Loaders & Transformers\\n6. FastAPI Streaming\\n\\n<center><img src=\"https://diagnosemlpdf.s3.us-east-2.amazonaws.com/langchain_crash_class/langchain_chain.png\" alt=\"chains\" width=\"800\"/></center>\\n\\n\\n```python\\n# Cargar tu API KEY de OpenAI y otros recursos necesarios\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n```\\n\\n\\n\\n\\n    True\\n\\n\\n\\n## 1. Prompts & LLMs\\n\\n- Solo unas líneas de código\\n- Utilizamos default o prompt específico\\n- Podemos utilizar variables\\n- Chat vs LLM\\n\\n<center><img src=\"https://diagnosemlpdf.s3.us-east-2.amazonaws.com/langchain_crash_class/prompt_vs_penginering.png\" alt=\"chains\" width=\"800\"/></center>\\n\\n\\n```python\\nfrom langchain.prompts import PromptTemplate\\n\\nprompt_template = PromptTemplate.from_template(\\n    \"Redacta un poema como Pablo Neurda que trate de {topic}.\"\\n)\\nprompt_template.format(topic=\"modelos de lenguaje\")\\n```\\n\\n\\n\\n\\n    \\'Redacta un poema como Pablo Neurda que trate de modelos de lenguaje.\\'\\n\\n\\n\\n\\n```python\\nprompt_template\\n```\\n\\n\\n\\n\\n    PromptTemplate(input_variables=[\\'topic\\'], output_parser=None, partial_variables={}, template=\\'Redacta un poema como Pablo Neurda que trate de {topic}.\\', template_format=\\'f-string\\', validate_template=True)\\n\\n\\n\\n### Chat models son distintos a LLM\\n\\n\\n```python\\nfrom langchain.prompts import ChatPromptTemplate\\n\\ntemplate = ChatPromptTemplate.from_messages([\\n    (\"system\", \"Eres un experto en planificación de proyectos, tu nombres es {name}, te mantienes conciso al dar una respuesta y solo respondes al estar seguro que se trate de la temática, si no, simplemente di \\'No lo sé\\' y tu nombre.\"),\\n    (\"human\", \"Que día es hoy?\"),\\n    (\"ai\", \"Es el 21 de Septienbre de 2023.\"),\\n    (\"human\", \"Qué roles debe tener un proyecto tecnológico\"),\\n    (\"ai\", \"Los roles principales en un proyecto tecnológico incluyen Gerente de Proyecto, Desarrollador de Software, Científico de Datos, Diseñador de UX/UI, Ingeniero de Pruebas, Arquitecto de Software, Analista de Negocios, Especialista en Seguridad, Operador de Sistemas, Especialista en Infraestructura, Gerente de Calidad, Documentador Técnico y Soporte Técnico.\"),\\n    (\"human\", \"{user_input}\"),\\n])\\n\\nmessages = template.format_messages(\\n    name=\"Myfuture\",\\n    user_input=\"Cómo se cocina un plato de fídeos?\"\\n)\\n```\\n\\n\\n```python\\nmessages\\n```\\n\\n\\n\\n\\n    [SystemMessage(content=\"Eres un experto en planificación de proyectos, tu nombres es Myfuture, te mantienes conciso al dar una respuesta y solo respondes al estar seguro que se trate de la temática, si no, simplemente di \\'No lo sé\\' y tu nombre.\", additional_kwargs={}),\\n     HumanMessage(content=\\'Que día es hoy?\\', additional_kwargs={}, example=False),\\n     AIMessage(content=\\'Es el 21 de Septienbre de 2023.\\', additional_kwargs={}, example=False),\\n     HumanMessage(content=\\'Qué roles debe tener un proyecto tecnológico\\', additional_kwargs={}, example=False),\\n     AIMessage(content=\\'Los roles principales en un proyecto tecnológico incluyen Gerente de Proyecto, Desarrollador de Software, Científico de Datos, Diseñador de UX/UI, Ingeniero de Pruebas, Arquitecto de Software, Analista de Negocios, Especialista en Seguridad, Operador de Sistemas, Especialista en Infraestructura, Gerente de Calidad, Documentador Técnico y Soporte Técnico.\\', additional_kwargs={}, example=False),\\n     HumanMessage(content=\\'Cómo se cocina un plato de fídeos?\\', additional_kwargs={}, example=False)]\\n\\n\\n\\n## 2. Chains\\n\\n- Solo unas líneas de código\\n- Utilizamos default o prompt específico\\n- Utilizamos un LLM en específico\\n\\n<center><img src=\"https://diagnosemlpdf.s3.us-east-2.amazonaws.com/langchain_crash_class/langchain_chains.png\" alt=\"chains\" width=\"800\"/></center>\\n\\n\\n```python\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import LLMChain\\n```\\n\\n\\n```python\\nllm = OpenAI(temperature=0)\\nllm_chain = LLMChain(\\n    llm=llm,\\n    prompt=prompt_template\\n)\\n```\\n\\n\\n```python\\nprompt_template.format(topic=\\'$TOPICO\\')\\n```\\n\\n\\n\\n\\n    \\'Redacta un poema como Pablo Neurda que trate de $TOPICO.\\'\\n\\n\\n\\n\\n```python\\n%%time\\nresponse = llm_chain(\"modelos de lenguaje\")\\nprint(response[\\'text\\'])\\n```\\n\\n    \\n    \\n    Modelos de lenguaje,\\n    Un código de hablar,\\n    Un lenguaje que nos une,\\n    Y nos hace comprender.\\n    \\n    Un lenguaje que nos permite\\n    Expresar lo que sentimos,\\n    Y nos da la libertad\\n    De comunicar nuestros sueños.\\n    \\n    Un lenguaje que nos conecta\\n    Y nos ayuda a compartir,\\n    Un lenguaje que nos une\\n    Y nos hace más fuertes.\\n    \\n    Un lenguaje que nos permite\\n    Ver el mundo de otra forma,\\n    Y nos da la oportunidad\\n    De crear un mejor mañana.\\n    CPU times: user 35 ms, sys: 7.33 ms, total: 42.3 ms\\n    Wall time: 4.34 s\\n\\n\\n\\n```python\\n%%time\\nresponse = llm_chain(\"mascotas\")\\nprint(response[\\'text\\'])\\n```\\n\\n    \\n    \\n    Mascotas, compañeras de alegrías\\n    Que nos acompañan en los días\\n    Y nos llenan de felicidad\\n    Con su amor y su lealtad.\\n    \\n    Son una luz en la oscuridad\\n    Y nos dan una sonrisa de bondad\\n    Nos dan cariño y compañía\\n    Y nos hacen sentir alegría.\\n    \\n    Nos dan una razón para vivir\\n    Y nos hacen sentir queridos\\n    Nos dan una razón para sonreír\\n    Y nos hacen sentir bendecidos.\\n    \\n    Mascotas, compañeras de alegrías\\n    Que nos acompañan en los días\\n    Y nos llenan de felicidad\\n    Con su amor y su lealtad.\\n    CPU times: user 38 ms, sys: 7.68 ms, total: 45.7 ms\\n    Wall time: 5.93 s\\n\\n\\n\\n```python\\nfrom langchain.chat_models import ChatOpenAI\\n```\\n\\n\\n```python\\nllm_chat = ChatOpenAI(temperature=0)\\n```\\n\\n\\n```python\\nmessages\\n```\\n\\n\\n\\n\\n    [SystemMessage(content=\"Eres un experto en planificación de proyectos, tu nombres es Myfuture, te mantienes conciso al dar una respuesta y solo respondes al estar seguro que se trate de la temática, si no, simplemente di \\'No lo sé\\' y tu nombre.\", additional_kwargs={}),\\n     HumanMessage(content=\\'Que día es hoy?\\', additional_kwargs={}, example=False),\\n     AIMessage(content=\\'Es el 21 de Septienbre de 2023.\\', additional_kwargs={}, example=False),\\n     HumanMessage(content=\\'Qué roles debe tener un proyecto tecnológico\\', additional_kwargs={}, example=False),\\n     AIMessage(content=\\'Los roles principales en un proyecto tecnológico incluyen Gerente de Proyecto, Desarrollador de Software, Científico de Datos, Diseñador de UX/UI, Ingeniero de Pruebas, Arquitecto de Software, Analista de Negocios, Especialista en Seguridad, Operador de Sistemas, Especialista en Infraestructura, Gerente de Calidad, Documentador Técnico y Soporte Técnico.\\', additional_kwargs={}, example=False),\\n     HumanMessage(content=\\'Cómo se cocina un plato de fídeos?\\', additional_kwargs={}, example=False)]\\n\\n\\n\\n\\n```python\\nresult = llm_chat(messages)\\nprint(result)\\n```\\n\\n    content=\\'No lo sé, soy un experto en planificación de proyectos. Mi nombre es Myfuture.\\' additional_kwargs={} example=False\\n\\n\\n### ¿Qué pasa si no tengo API KEY de OpenAI?\\n- Podemos usar Huggingface ys su modelos OpenSource para experimentar de igual manera.\\n\\n\\n```python\\nfrom langchain.llms import HuggingFacePipeline\\n```\\n\\n\\n```python\\nllm_open = HuggingFacePipeline.from_model_id(\\n    model_id=\"bigscience/bloom-1b7\",\\n    task=\"text-generation\",\\n    model_kwargs={\"temperature\": 0, \"max_length\": 64},\\n)\\n```\\n\\n\\n```python\\nchain = prompt_template | llm_open\\n```\\n\\n\\n```python\\nprint(chain.invoke({\"topic\": \\'comida\\'}))\\n```\\n\\n\\n```python\\n# Import things that are needed generically\\nfrom langchain.chains import LLMMathChain\\nfrom langchain.utilities import SerpAPIWrapper\\nfrom langchain.agents import AgentType, initialize_agent\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.tools import BaseTool, StructuredTool, Tool, tool\\n```\\n\\n## 3. Agents\\n\\n- Razonamiento autonomo\\n- BabyAGI / AutoGPT\\n- Múltiples iteraciones\\n- Utilizamos un LLM en específico\\n- Ejemplo base: ReAct\\n\\n<center><img src=\"https://diagnosemlpdf.s3.us-east-2.amazonaws.com/langchain_crash_class/agent.png\" alt=\"chains\" width=\"800\"/></center>\\n\\n\\n```python\\nfrom langchain.agents import load_tools\\nfrom langchain.agents import initialize_agent\\nfrom langchain.agents import AgentType\\nfrom langchain.llms import OpenAI\\n```\\n\\n\\n```python\\nllm = OpenAI(temperature=0)\\n```\\n\\n\\n```python\\ntools = load_tools([\"llm-math\"], llm=llm)\\n```\\n\\n\\n```python\\nagent_executor = initialize_agent(tools=tools, llm=llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\\n```\\n\\n\\n```python\\nagent_executor.invoke({\"input\": \"Si tenía 9 perritos, no me quedan más que 3, calcula cuántos perdí\"})\\n```\\n\\n    \\n    \\n    \\x1b[1m> Entering new AgentExecutor chain...\\x1b[0m\\n    \\x1b[32;1m\\x1b[1;3m I need to subtract 3 from 9\\n    Action: Calculator\\n    Action Input: 9 - 3\\x1b[0m\\n    Observation: \\x1b[36;1m\\x1b[1;3mAnswer: 6\\x1b[0m\\n    Thought:\\x1b[32;1m\\x1b[1;3m I now know the final answer\\n    Final Answer: Perdí 6 perritos.\\x1b[0m\\n    \\n    \\x1b[1m> Finished chain.\\x1b[0m\\n\\n\\n\\n\\n\\n    {\\'input\\': \\'Si tenía 9 perritos, no me quedan más que 3, calcula cuántos perdí\\',\\n     \\'output\\': \\'Perdí 6 perritos.\\'}\\n\\n\\n\\n\\n```python\\nagent_executor.invoke({\"input\": \"Qué fecha fue ayer?\"})\\n```\\n\\n    \\n    \\n    \\x1b[1m> Entering new AgentExecutor chain...\\x1b[0m\\n    \\x1b[32;1m\\x1b[1;3m I need to figure out what yesterday\\'s date was.\\n    Action: Calculator\\n    Action Input: Today\\'s date minus 1 day\\x1b[0m\\n\\n\\n    ---------------------------------------------------------------------------\\n\\n    ValueError                                Traceback (most recent call last)\\n\\n    File ~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/llm_math/base.py:81, in LLMMathChain._evaluate_expression(self, expression)\\n         79     local_dict = {\"pi\": math.pi, \"e\": math.e}\\n         80     output = str(\\n    ---> 81         numexpr.evaluate(\\n         82             expression.strip(),\\n         83             global_dict={},  # restrict access to globals\\n         84             local_dict=local_dict,  # add common mathematical functions\\n         85         )\\n         86     )\\n         87 except Exception as e:\\n\\n\\n    File ~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/numexpr/necompiler.py:975, in evaluate(ex, local_dict, global_dict, out, order, casting, sanitize, _frame_depth, **kwargs)\\n        974 else:\\n    --> 975     raise e\\n\\n\\n    File ~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/numexpr/necompiler.py:872, in validate(ex, local_dict, global_dict, out, order, casting, _frame_depth, sanitize, **kwargs)\\n        871 if expr_key not in _names_cache:\\n    --> 872     _names_cache[expr_key] = getExprNames(ex, context, sanitize=sanitize)\\n        873 names, ex_uses_vml = _names_cache[expr_key]\\n\\n\\n    File ~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/numexpr/necompiler.py:721, in getExprNames(text, context, sanitize)\\n        720 def getExprNames(text, context, sanitize: bool=True):\\n    --> 721     ex = stringToExpression(text, {}, context, sanitize)\\n        722     ast = expressionToAST(ex)\\n\\n\\n    File ~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/numexpr/necompiler.py:281, in stringToExpression(s, types, context, sanitize)\\n        280     if _blacklist_re.search(no_whitespace) is not None:\\n    --> 281         raise ValueError(f\\'Expression {s} has forbidden control characters.\\')\\n        283 old_ctx = expressions._context.get_current_context()\\n\\n\\n    ValueError: Expression date.today() - timedelta(days=1) has forbidden control characters.\\n\\n    \\n    During handling of the above exception, another exception occurred:\\n\\n\\n    ValueError                                Traceback (most recent call last)\\n\\n    Cell In[20], line 1\\n    ----> 1 agent_executor.invoke({\"input\": \"Qué fecha fue ayer?\"})\\n\\n\\n    File ~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/base.py:66, in Chain.invoke(self, input, config, **kwargs)\\n         59 def invoke(\\n         60     self,\\n         61     input: Dict[str, Any],\\n         62     config: Optional[RunnableConfig] = None,\\n         63     **kwargs: Any,\\n         64 ) -> Dict[str, Any]:\\n         65     config = config or {}\\n    ---> 66     return self(\\n         67         input,\\n         68         callbacks=config.get(\"callbacks\"),\\n         69         tags=config.get(\"tags\"),\\n         70         metadata=config.get(\"metadata\"),\\n         71         run_name=config.get(\"run_name\"),\\n         72         **kwargs,\\n         73     )\\n\\n\\n    File ~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/base.py:292, in Chain.__call__(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\\n        290 except BaseException as e:\\n        291     run_manager.on_chain_error(e)\\n    --> 292     raise e\\n        293 run_manager.on_chain_end(outputs)\\n        294 final_outputs: Dict[str, Any] = self.prep_outputs(\\n        295     inputs, outputs, return_only_outputs\\n        296 )\\n\\n\\n    File ~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/base.py:286, in Chain.__call__(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\\n        279 run_manager = callback_manager.on_chain_start(\\n        280     dumpd(self),\\n        281     inputs,\\n        282     name=run_name,\\n        283 )\\n        284 try:\\n        285     outputs = (\\n    --> 286         self._call(inputs, run_manager=run_manager)\\n        287         if new_arg_supported\\n        288         else self._call(inputs)\\n        289     )\\n        290 except BaseException as e:\\n        291     run_manager.on_chain_error(e)\\n\\n\\n    File ~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/agents/agent.py:1122, in AgentExecutor._call(self, inputs, run_manager)\\n       1120 # We now enter the agent loop (until it returns something).\\n       1121 while self._should_continue(iterations, time_elapsed):\\n    -> 1122     next_step_output = self._take_next_step(\\n       1123         name_to_tool_map,\\n       1124         color_mapping,\\n       1125         inputs,\\n       1126         intermediate_steps,\\n       1127         run_manager=run_manager,\\n       1128     )\\n       1129     if isinstance(next_step_output, AgentFinish):\\n       1130         return self._return(\\n       1131             next_step_output, intermediate_steps, run_manager=run_manager\\n       1132         )\\n\\n\\n    File ~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/agents/agent.py:977, in AgentExecutor._take_next_step(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\\n        975         tool_run_kwargs[\"llm_prefix\"] = \"\"\\n        976     # We then call the tool on the tool input to get an observation\\n    --> 977     observation = tool.run(\\n        978         agent_action.tool_input,\\n        979         verbose=self.verbose,\\n        980         color=color,\\n        981         callbacks=run_manager.get_child() if run_manager else None,\\n        982         **tool_run_kwargs,\\n        983     )\\n        984 else:\\n        985     tool_run_kwargs = self.agent.tool_run_logging_kwargs()\\n\\n\\n    File ~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/tools/base.py:356, in BaseTool.run(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, **kwargs)\\n        354 except (Exception, KeyboardInterrupt) as e:\\n        355     run_manager.on_tool_error(e)\\n    --> 356     raise e\\n        357 else:\\n        358     run_manager.on_tool_end(\\n        359         str(observation), color=color, name=self.name, **kwargs\\n        360     )\\n\\n\\n    File ~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/tools/base.py:328, in BaseTool.run(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, **kwargs)\\n        325 try:\\n        326     tool_args, tool_kwargs = self._to_args_and_kwargs(parsed_input)\\n        327     observation = (\\n    --> 328         self._run(*tool_args, run_manager=run_manager, **tool_kwargs)\\n        329         if new_arg_supported\\n        330         else self._run(*tool_args, **tool_kwargs)\\n        331     )\\n        332 except ToolException as e:\\n        333     if not self.handle_tool_error:\\n\\n\\n    File ~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/tools/base.py:499, in Tool._run(self, run_manager, *args, **kwargs)\\n        496 if self.func:\\n        497     new_argument_supported = signature(self.func).parameters.get(\"callbacks\")\\n        498     return (\\n    --> 499         self.func(\\n        500             *args,\\n        501             callbacks=run_manager.get_child() if run_manager else None,\\n        502             **kwargs,\\n        503         )\\n        504         if new_argument_supported\\n        505         else self.func(*args, **kwargs)\\n        506     )\\n        507 raise NotImplementedError(\"Tool does not support sync\")\\n\\n\\n    File ~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/base.py:487, in Chain.run(self, callbacks, tags, metadata, *args, **kwargs)\\n        485     if len(args) != 1:\\n        486         raise ValueError(\"`run` supports only one positional argument.\")\\n    --> 487     return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\\n        488         _output_key\\n        489     ]\\n        491 if kwargs and not args:\\n        492     return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\\n        493         _output_key\\n        494     ]\\n\\n\\n    File ~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/base.py:292, in Chain.__call__(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\\n        290 except BaseException as e:\\n        291     run_manager.on_chain_error(e)\\n    --> 292     raise e\\n        293 run_manager.on_chain_end(outputs)\\n        294 final_outputs: Dict[str, Any] = self.prep_outputs(\\n        295     inputs, outputs, return_only_outputs\\n        296 )\\n\\n\\n    File ~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/base.py:286, in Chain.__call__(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\\n        279 run_manager = callback_manager.on_chain_start(\\n        280     dumpd(self),\\n        281     inputs,\\n        282     name=run_name,\\n        283 )\\n        284 try:\\n        285     outputs = (\\n    --> 286         self._call(inputs, run_manager=run_manager)\\n        287         if new_arg_supported\\n        288         else self._call(inputs)\\n        289     )\\n        290 except BaseException as e:\\n        291     run_manager.on_chain_error(e)\\n\\n\\n    File ~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/llm_math/base.py:150, in LLMMathChain._call(self, inputs, run_manager)\\n        144 _run_manager.on_text(inputs[self.input_key])\\n        145 llm_output = self.llm_chain.predict(\\n        146     question=inputs[self.input_key],\\n        147     stop=[\"```output\"],\\n        148     callbacks=_run_manager.get_child(),\\n        149 )\\n    --> 150 return self._process_llm_result(llm_output, _run_manager)\\n\\n\\n    File ~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/llm_math/base.py:104, in LLMMathChain._process_llm_result(self, llm_output, run_manager)\\n        102 if text_match:\\n        103     expression = text_match.group(1)\\n    --> 104     output = self._evaluate_expression(expression)\\n        105     run_manager.on_text(\"\\\\nAnswer: \", verbose=self.verbose)\\n        106     run_manager.on_text(output, color=\"yellow\", verbose=self.verbose)\\n\\n\\n    File ~/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/llm_math/base.py:88, in LLMMathChain._evaluate_expression(self, expression)\\n         80     output = str(\\n         81         numexpr.evaluate(\\n         82             expression.strip(),\\n       (...)\\n         85         )\\n         86     )\\n         87 except Exception as e:\\n    ---> 88     raise ValueError(\\n         89         f\\'LLMMathChain._evaluate(\"{expression}\") raised error: {e}.\\'\\n         90         \" Please try again with a valid numerical expression\"\\n         91     )\\n         93 # Remove any leading and trailing brackets from the output\\n         94 return re.sub(r\"^\\\\[|\\\\]$\", \"\", output)\\n\\n\\n    ValueError: LLMMathChain._evaluate(\"\\n    date.today() - timedelta(days=1)\\n    \") raised error: Expression date.today() - timedelta(days=1) has forbidden control characters.. Please try again with a valid numerical expression\\n\\n\\n\\n```python\\nagent_executor_chat = initialize_agent(tools=tools, llm=llm_chat, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\\n```\\n\\n\\n```python\\nagent_executor_chat.invoke({\"input\": \"Si tenía 9 perritos, no me quedan más que 3, calcula cuántos perdí\"})\\n```\\n\\n    \\n    \\n    \\x1b[1m> Entering new AgentExecutor chain...\\x1b[0m\\n    \\x1b[32;1m\\x1b[1;3mI need to subtract the number of remaining dogs from the original number of dogs.\\n    Action: Calculator\\n    Action Input: 9 - 3\\x1b[0m\\n    Observation: \\x1b[36;1m\\x1b[1;3mAnswer: 6\\x1b[0m\\n    Thought:\\x1b[32;1m\\x1b[1;3mI now know that I lost 6 dogs.\\n    Final Answer: I lost 6 dogs.\\x1b[0m\\n    \\n    \\x1b[1m> Finished chain.\\x1b[0m\\n\\n\\n\\n\\n\\n    {\\'input\\': \\'Si tenía 9 perritos, no me quedan más que 3, calcula cuántos perdí\\',\\n     \\'output\\': \\'I lost 6 dogs.\\'}\\n\\n\\n\\n\\n```python\\nfrom langchain.tools.python.tool import PythonREPLTool\\nfrom langchain.agents.agent_toolkits import create_python_agent\\n```\\n\\n\\n```python\\nagent_executor = create_python_agent(\\n    llm=OpenAI(temperature=0, max_tokens=1000),\\n    tool=PythonREPLTool(),\\n    verbose=True,\\n    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\\n)\\n```\\n\\n\\n```python\\nagent_executor.invoke({\"input\": \"Qué fecha fue ayer?\"})\\n```\\n\\n    \\n    \\n    \\x1b[1m> Entering new AgentExecutor chain...\\x1b[0m\\n\\n\\n    Python REPL can execute arbitrary code. Use with caution.\\n\\n\\n    \\x1b[32;1m\\x1b[1;3m I need to get the current date and subtract one day\\n    Action: Python_REPL\\n    Action Input: from datetime import date; print(date.today() - timedelta(days=1))\\x1b[0m\\n    Observation: \\x1b[36;1m\\x1b[1;3mNameError(\"name \\'timedelta\\' is not defined\")\\x1b[0m\\n    Thought:\\x1b[32;1m\\x1b[1;3m I need to import the timedelta module\\n    Action: Python_REPL\\n    Action Input: from datetime import date, timedelta; print(date.today() - timedelta(days=1))\\x1b[0m\\n    Observation: \\x1b[36;1m\\x1b[1;3m2023-09-20\\n    \\x1b[0m\\n    Thought:\\x1b[32;1m\\x1b[1;3m I now know the final answer\\n    Final Answer: Ayer fue el 20 de septiembre de 2023.\\x1b[0m\\n    \\n    \\x1b[1m> Finished chain.\\x1b[0m\\n\\n\\n\\n\\n\\n    {\\'input\\': \\'Qué fecha fue ayer?\\',\\n     \\'output\\': \\'Ayer fue el 20 de septiembre de 2023.\\'}\\n\\n\\n\\n## 3. Memory and Tools\\n\\n- Extender capacidades\\n- Disminuir errores\\n- Crear contexto y continuidad\\n\\n<center><img src=\"https://diagnosemlpdf.s3.us-east-2.amazonaws.com/langchain_crash_class/tools_memory.png\" alt=\"chains\" width=\"800\"/></center>\\n\\n\\n```python\\nllm = ChatOpenAI(temperature=0)\\n```\\n\\n\\n```python\\nfrom langchain.chains import LLMMathChain\\n\\nllm_math_chain = LLMMathChain(llm=llm, verbose=True)\\nllm_math_chain\\n```\\n\\n    /Users/bgg/Documents/repos/langchain_crash_class/venv/lib/python3.9/site-packages/langchain/chains/llm_math/base.py:51: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\\n      warnings.warn(\\n\\n\\n\\n\\n\\n    LLMMathChain(memory=None, callbacks=None, callback_manager=None, verbose=True, tags=None, metadata=None, llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=[\\'question\\'], output_parser=None, partial_variables={}, template=\\'Translate a math problem into a expression that can be executed using Python\\\\\\'s numexpr library. Use the output of running this code to answer the question.\\\\n\\\\nQuestion: ${{Question with math problem.}}\\\\n```text\\\\n${{single line mathematical expression that solves the problem}}\\\\n```\\\\n...numexpr.evaluate(text)...\\\\n```output\\\\n${{Output of running the code}}\\\\n```\\\\nAnswer: ${{Answer}}\\\\n\\\\nBegin.\\\\n\\\\nQuestion: What is 37593 * 67?\\\\n```text\\\\n37593 * 67\\\\n```\\\\n...numexpr.evaluate(\"37593 * 67\")...\\\\n```output\\\\n2518731\\\\n```\\\\nAnswer: 2518731\\\\n\\\\nQuestion: 37593^(1/5)\\\\n```text\\\\n37593**(1/5)\\\\n```\\\\n...numexpr.evaluate(\"37593**(1/5)\")...\\\\n```output\\\\n8.222831614237718\\\\n```\\\\nAnswer: 8.222831614237718\\\\n\\\\nQuestion: {question}\\\\n\\', template_format=\\'f-string\\', validate_template=True), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class \\'openai.api_resources.chat_completion.ChatCompletion\\'>, model_name=\\'gpt-3.5-turbo\\', temperature=0.0, model_kwargs={}, openai_api_key=\\'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\\', openai_api_base=\\'\\', openai_organization=\\'\\', openai_proxy=\\'\\', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key=\\'text\\', output_parser=StrOutputParser(), return_final_only=True, llm_kwargs={}), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class \\'openai.api_resources.chat_completion.ChatCompletion\\'>, model_name=\\'gpt-3.5-turbo\\', temperature=0.0, model_kwargs={}, openai_api_key=\\'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\\', openai_api_base=\\'\\', openai_organization=\\'\\', openai_proxy=\\'\\', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), prompt=PromptTemplate(input_variables=[\\'question\\'], output_parser=None, partial_variables={}, template=\\'Translate a math problem into a expression that can be executed using Python\\\\\\'s numexpr library. Use the output of running this code to answer the question.\\\\n\\\\nQuestion: ${{Question with math problem.}}\\\\n```text\\\\n${{single line mathematical expression that solves the problem}}\\\\n```\\\\n...numexpr.evaluate(text)...\\\\n```output\\\\n${{Output of running the code}}\\\\n```\\\\nAnswer: ${{Answer}}\\\\n\\\\nBegin.\\\\n\\\\nQuestion: What is 37593 * 67?\\\\n```text\\\\n37593 * 67\\\\n```\\\\n...numexpr.evaluate(\"37593 * 67\")...\\\\n```output\\\\n2518731\\\\n```\\\\nAnswer: 2518731\\\\n\\\\nQuestion: 37593^(1/5)\\\\n```text\\\\n37593**(1/5)\\\\n```\\\\n...numexpr.evaluate(\"37593**(1/5)\")...\\\\n```output\\\\n8.222831614237718\\\\n```\\\\nAnswer: 8.222831614237718\\\\n\\\\nQuestion: {question}\\\\n\\', template_format=\\'f-string\\', validate_template=True), input_key=\\'question\\', output_key=\\'answer\\')\\n\\n\\n\\n\\n```python\\nprint(llm_math_chain.prompt.template)\\n```\\n\\n    Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n    \\n    Question: ${{Question with math problem.}}\\n    ```text\\n    ${{single line mathematical expression that solves the problem}}\\n    ```\\n    ...numexpr.evaluate(text)...\\n    ```output\\n    ${{Output of running the code}}\\n    ```\\n    Answer: ${{Answer}}\\n    \\n    Begin.\\n    \\n    Question: What is 37593 * 67?\\n    ```text\\n    37593 * 67\\n    ```\\n    ...numexpr.evaluate(\"37593 * 67\")...\\n    ```output\\n    2518731\\n    ```\\n    Answer: 2518731\\n    \\n    Question: 37593^(1/5)\\n    ```text\\n    37593**(1/5)\\n    ```\\n    ...numexpr.evaluate(\"37593**(1/5)\")...\\n    ```output\\n    8.222831614237718\\n    ```\\n    Answer: 8.222831614237718\\n    \\n    Question: {question}\\n    \\n\\n\\n\\n```python\\nimport pandas as pd\\n```\\n\\n\\n```python\\ndfs = pd.read_html(\\'https://si3.bcentral.cl/indicadoressiete/secure/Serie.aspx?gcode=UF&param=RABmAFYAWQB3AGYAaQBuAEkALQAzADUAbgBNAGgAaAAkADUAVwBQAC4AbQBYADAARwBOAGUAYwBjACMAQQBaAHAARgBhAGcAUABTAGUAYwBsAEMAMQA0AE0AawBLAF8AdQBDACQASABzAG0AXwA2AHQAawBvAFcAZwBKAEwAegBzAF8AbgBMAHIAYgBDAC4ARQA3AFUAVwB4AFIAWQBhAEEAOABkAHkAZwAxAEEARAA=\\')\\ndfs[1]\\n```\\n\\n\\n\\n\\n<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>DÃ\\xada</th>\\n      <th>Enero</th>\\n      <th>Febrero</th>\\n      <th>Marzo</th>\\n      <th>Abril</th>\\n      <th>Mayo</th>\\n      <th>Junio</th>\\n      <th>Julio</th>\\n      <th>Agosto</th>\\n      <th>Septiembre</th>\\n      <th>Octubre</th>\\n      <th>Noviembre</th>\\n      <th>Diciembre</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>1</td>\\n      <td>35.122,26</td>\\n      <td>35.290,91</td>\\n      <td>35.519,79</td>\\n      <td>35.574,33</td>\\n      <td>35.851,62</td>\\n      <td>36.036,37</td>\\n      <td>36.090,68</td>\\n      <td>36.046,72</td>\\n      <td>36.134,97</td>\\n      <td>36.198,73</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>2</td>\\n      <td>35.133,53</td>\\n      <td>35.294,32</td>\\n      <td>35.529,90</td>\\n      <td>35.573,19</td>\\n      <td>35.864,70</td>\\n      <td>36.039,85</td>\\n      <td>36.091,89</td>\\n      <td>36.044,39</td>\\n      <td>36.139,62</td>\\n      <td>36.199,94</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>3</td>\\n      <td>35.144,81</td>\\n      <td>35.297,73</td>\\n      <td>35.540,01</td>\\n      <td>35.572,04</td>\\n      <td>35.877,78</td>\\n      <td>36.043,34</td>\\n      <td>36.093,09</td>\\n      <td>36.042,06</td>\\n      <td>36.144,27</td>\\n      <td>36.201,14</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>4</td>\\n      <td>35.156,09</td>\\n      <td>35.301,14</td>\\n      <td>35.550,13</td>\\n      <td>35.570,89</td>\\n      <td>35.890,87</td>\\n      <td>36.046,82</td>\\n      <td>36.094,29</td>\\n      <td>36.039,73</td>\\n      <td>36.148,93</td>\\n      <td>36.202,35</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>5</td>\\n      <td>35.167,38</td>\\n      <td>35.304,55</td>\\n      <td>35.560,24</td>\\n      <td>35.569,74</td>\\n      <td>35.903,96</td>\\n      <td>36.050,30</td>\\n      <td>36.095,49</td>\\n      <td>36.037,41</td>\\n      <td>36.153,58</td>\\n      <td>36.203,56</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>5</th>\\n      <td>6</td>\\n      <td>35.178,67</td>\\n      <td>35.307,96</td>\\n      <td>35.570,37</td>\\n      <td>35.568,59</td>\\n      <td>35.917,05</td>\\n      <td>36.053,79</td>\\n      <td>36.096,70</td>\\n      <td>36.035,08</td>\\n      <td>36.158,24</td>\\n      <td>36.204,76</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>6</th>\\n      <td>7</td>\\n      <td>35.189,96</td>\\n      <td>35.311,37</td>\\n      <td>35.580,49</td>\\n      <td>35.567,44</td>\\n      <td>35.930,15</td>\\n      <td>36.057,27</td>\\n      <td>36.097,90</td>\\n      <td>36.032,75</td>\\n      <td>36.162,90</td>\\n      <td>36.205,97</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>7</th>\\n      <td>8</td>\\n      <td>35.201,26</td>\\n      <td>35.314,79</td>\\n      <td>35.590,62</td>\\n      <td>35.566,30</td>\\n      <td>35.943,26</td>\\n      <td>36.060,75</td>\\n      <td>36.099,10</td>\\n      <td>36.030,43</td>\\n      <td>36.167,55</td>\\n      <td>36.207,18</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>8</th>\\n      <td>9</td>\\n      <td>35.212,56</td>\\n      <td>35.318,20</td>\\n      <td>35.600,75</td>\\n      <td>35.565,15</td>\\n      <td>35.956,37</td>\\n      <td>36.064,24</td>\\n      <td>36.100,30</td>\\n      <td>36.028,10</td>\\n      <td>36.172,21</td>\\n      <td>36.208,38</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>9</th>\\n      <td>10</td>\\n      <td>35.215,96</td>\\n      <td>35.328,25</td>\\n      <td>35.599,60</td>\\n      <td>35.578,12</td>\\n      <td>35.959,84</td>\\n      <td>36.065,44</td>\\n      <td>36.097,97</td>\\n      <td>36.032,74</td>\\n      <td>36.173,42</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>10</th>\\n      <td>11</td>\\n      <td>35.219,37</td>\\n      <td>35.338,31</td>\\n      <td>35.598,45</td>\\n      <td>35.591,10</td>\\n      <td>35.963,32</td>\\n      <td>36.066,64</td>\\n      <td>36.095,64</td>\\n      <td>36.037,38</td>\\n      <td>36.174,62</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>11</th>\\n      <td>12</td>\\n      <td>35.222,77</td>\\n      <td>35.348,37</td>\\n      <td>35.597,30</td>\\n      <td>35.604,08</td>\\n      <td>35.966,79</td>\\n      <td>36.067,84</td>\\n      <td>36.093,31</td>\\n      <td>36.042,02</td>\\n      <td>36.175,83</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>12</th>\\n      <td>13</td>\\n      <td>35.226,17</td>\\n      <td>35.358,43</td>\\n      <td>35.596,15</td>\\n      <td>35.617,07</td>\\n      <td>35.970,27</td>\\n      <td>36.069,05</td>\\n      <td>36.090,98</td>\\n      <td>36.046,66</td>\\n      <td>36.177,03</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>13</th>\\n      <td>14</td>\\n      <td>35.229,58</td>\\n      <td>35.368,49</td>\\n      <td>35.595,01</td>\\n      <td>35.630,06</td>\\n      <td>35.973,75</td>\\n      <td>36.070,25</td>\\n      <td>36.088,64</td>\\n      <td>36.051,31</td>\\n      <td>36.178,24</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>14</th>\\n      <td>15</td>\\n      <td>35.232,98</td>\\n      <td>35.378,56</td>\\n      <td>35.593,86</td>\\n      <td>35.643,05</td>\\n      <td>35.977,22</td>\\n      <td>36.071,45</td>\\n      <td>36.086,31</td>\\n      <td>36.055,95</td>\\n      <td>36.179,44</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>15</th>\\n      <td>16</td>\\n      <td>35.236,39</td>\\n      <td>35.388,63</td>\\n      <td>35.592,71</td>\\n      <td>35.656,05</td>\\n      <td>35.980,70</td>\\n      <td>36.072,65</td>\\n      <td>36.083,98</td>\\n      <td>36.060,59</td>\\n      <td>36.180,65</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>16</th>\\n      <td>17</td>\\n      <td>35.239,79</td>\\n      <td>35.398,70</td>\\n      <td>35.591,56</td>\\n      <td>35.669,06</td>\\n      <td>35.984,18</td>\\n      <td>36.073,85</td>\\n      <td>36.081,65</td>\\n      <td>36.065,24</td>\\n      <td>36.181,85</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>17</th>\\n      <td>18</td>\\n      <td>35.243,20</td>\\n      <td>35.408,77</td>\\n      <td>35.590,41</td>\\n      <td>35.682,07</td>\\n      <td>35.987,65</td>\\n      <td>36.075,06</td>\\n      <td>36.079,32</td>\\n      <td>36.069,88</td>\\n      <td>36.183,06</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>18</th>\\n      <td>19</td>\\n      <td>35.246,60</td>\\n      <td>35.418,85</td>\\n      <td>35.589,26</td>\\n      <td>35.695,08</td>\\n      <td>35.991,13</td>\\n      <td>36.076,26</td>\\n      <td>36.076,99</td>\\n      <td>36.074,53</td>\\n      <td>36.184,26</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>19</th>\\n      <td>20</td>\\n      <td>35.250,01</td>\\n      <td>35.428,93</td>\\n      <td>35.588,11</td>\\n      <td>35.708,10</td>\\n      <td>35.994,61</td>\\n      <td>36.077,46</td>\\n      <td>36.074,66</td>\\n      <td>36.079,17</td>\\n      <td>36.185,47</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>20</th>\\n      <td>21</td>\\n      <td>35.253,41</td>\\n      <td>35.439,02</td>\\n      <td>35.586,96</td>\\n      <td>35.721,12</td>\\n      <td>35.998,09</td>\\n      <td>36.078,66</td>\\n      <td>36.072,33</td>\\n      <td>36.083,82</td>\\n      <td>36.186,67</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>21</th>\\n      <td>22</td>\\n      <td>35.256,82</td>\\n      <td>35.449,10</td>\\n      <td>35.585,82</td>\\n      <td>35.734,15</td>\\n      <td>36.001,57</td>\\n      <td>36.079,86</td>\\n      <td>36.070,00</td>\\n      <td>36.088,46</td>\\n      <td>36.187,88</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>22</th>\\n      <td>23</td>\\n      <td>35.260,23</td>\\n      <td>35.459,19</td>\\n      <td>35.584,67</td>\\n      <td>35.747,19</td>\\n      <td>36.005,05</td>\\n      <td>36.081,07</td>\\n      <td>36.067,68</td>\\n      <td>36.093,11</td>\\n      <td>36.189,09</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>23</th>\\n      <td>24</td>\\n      <td>35.263,64</td>\\n      <td>35.469,28</td>\\n      <td>35.583,52</td>\\n      <td>35.760,22</td>\\n      <td>36.008,52</td>\\n      <td>36.082,27</td>\\n      <td>36.065,35</td>\\n      <td>36.097,76</td>\\n      <td>36.190,29</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>24</th>\\n      <td>25</td>\\n      <td>35.267,04</td>\\n      <td>35.479,38</td>\\n      <td>35.582,37</td>\\n      <td>35.773,27</td>\\n      <td>36.012,00</td>\\n      <td>36.083,47</td>\\n      <td>36.063,02</td>\\n      <td>36.102,41</td>\\n      <td>36.191,50</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>25</th>\\n      <td>26</td>\\n      <td>35.270,45</td>\\n      <td>35.489,48</td>\\n      <td>35.581,22</td>\\n      <td>35.786,31</td>\\n      <td>36.015,48</td>\\n      <td>36.084,67</td>\\n      <td>36.060,69</td>\\n      <td>36.107,06</td>\\n      <td>36.192,70</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>26</th>\\n      <td>27</td>\\n      <td>35.273,86</td>\\n      <td>35.499,58</td>\\n      <td>35.580,07</td>\\n      <td>35.799,37</td>\\n      <td>36.018,96</td>\\n      <td>36.085,87</td>\\n      <td>36.058,36</td>\\n      <td>36.111,71</td>\\n      <td>36.193,91</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>27</th>\\n      <td>28</td>\\n      <td>35.277,27</td>\\n      <td>35.509,68</td>\\n      <td>35.578,93</td>\\n      <td>35.812,42</td>\\n      <td>36.022,44</td>\\n      <td>36.087,08</td>\\n      <td>36.056,03</td>\\n      <td>36.116,36</td>\\n      <td>36.195,11</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>28</th>\\n      <td>29</td>\\n      <td>35.280,68</td>\\n      <td>NaN</td>\\n      <td>35.577,78</td>\\n      <td>35.825,49</td>\\n      <td>36.025,93</td>\\n      <td>36.088,28</td>\\n      <td>36.053,70</td>\\n      <td>36.121,01</td>\\n      <td>36.196,32</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>29</th>\\n      <td>30</td>\\n      <td>35.284,09</td>\\n      <td>NaN</td>\\n      <td>35.576,63</td>\\n      <td>35.838,55</td>\\n      <td>36.029,41</td>\\n      <td>36.089,48</td>\\n      <td>36.051,37</td>\\n      <td>36.125,66</td>\\n      <td>36.197,53</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n    <tr>\\n      <th>30</th>\\n      <td>31</td>\\n      <td>35.287,50</td>\\n      <td>NaN</td>\\n      <td>35.575,48</td>\\n      <td>NaN</td>\\n      <td>36.032,89</td>\\n      <td>NaN</td>\\n      <td>36.049,05</td>\\n      <td>36.130,31</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n      <td>NaN</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>\\n\\n\\n\\n\\n```python\\nfrom langchain.agents import create_pandas_dataframe_agent\\nfrom copy import deepcopy\\n```\\n\\n\\n```python\\ndf_1 = deepcopy(dfs[1])\\ndf_2 = deepcopy(dfs[1])\\n```\\n\\n\\n```python\\nagent = create_pandas_dataframe_agent(OpenAI(temperature=0), df_1, verbose=True)\\n```\\n\\n\\n```python\\nagent.run(\"Modificalo para que tenga solo 2 columnas, fecha y valor\")\\n```\\n\\n    \\n    \\n    \\x1b[1m> Entering new AgentExecutor chain...\\x1b[0m\\n    \\x1b[32;1m\\x1b[1;3mThought: Necesito eliminar todas las columnas excepto las dos que necesito\\n    Action: python_repl_ast\\n    Action Input: df.drop(df.columns.difference([\\'Día\\', \\'Enero\\']), 1, inplace=True)\\x1b[0m\\n    Observation: \\x1b[36;1m\\x1b[1;3mTypeError: drop() takes from 1 to 2 positional arguments but 3 positional arguments (and 1 keyword-only argument) were given\\x1b[0m\\n    Thought:\\x1b[32;1m\\x1b[1;3m Necesito especificar los argumentos\\n    Action: python_repl_ast\\n    Action Input: df.drop(df.columns.difference([\\'Día\\', \\'Enero\\']), axis=1, inplace=True)\\x1b[0m\\n    Observation: \\x1b[36;1m\\x1b[1;3m\\x1b[0m\\n    Thought:\\x1b[32;1m\\x1b[1;3m Ahora tengo la tabla con las dos columnas que necesito\\n    Final Answer: La tabla ahora tiene dos columnas, \\'Día\\' y \\'Enero\\'.\\x1b[0m\\n    \\n    \\x1b[1m> Finished chain.\\x1b[0m\\n\\n\\n\\n\\n\\n    \"La tabla ahora tiene dos columnas, \\'Día\\' y \\'Enero\\'.\"\\n\\n\\n\\n\\n```python\\ndf_1\\n```\\n\\n\\n\\n\\n<div>\\n<style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n</style>\\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>Enero</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>35.122,26</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>35.133,53</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>35.144,81</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>35.156,09</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>35.167,38</td>\\n    </tr>\\n    <tr>\\n      <th>5</th>\\n      <td>35.178,67</td>\\n    </tr>\\n    <tr>\\n      <th>6</th>\\n      <td>35.189,96</td>\\n    </tr>\\n    <tr>\\n      <th>7</th>\\n      <td>35.201,26</td>\\n    </tr>\\n    <tr>\\n      <th>8</th>\\n      <td>35.212,56</td>\\n    </tr>\\n    <tr>\\n      <th>9</th>\\n      <td>35.215,96</td>\\n    </tr>\\n    <tr>\\n      <th>10</th>\\n      <td>35.219,37</td>\\n    </tr>\\n    <tr>\\n      <th>11</th>\\n      <td>35.222,77</td>\\n    </tr>\\n    <tr>\\n      <th>12</th>\\n      <td>35.226,17</td>\\n    </tr>\\n    <tr>\\n      <th>13</th>\\n      <td>35.229,58</td>\\n    </tr>\\n    <tr>\\n      <th>14</th>\\n      <td>35.232,98</td>\\n    </tr>\\n    <tr>\\n      <th>15</th>\\n      <td>35.236,39</td>\\n    </tr>\\n    <tr>\\n      <th>16</th>\\n      <td>35.239,79</td>\\n    </tr>\\n    <tr>\\n      <th>17</th>\\n      <td>35.243,20</td>\\n    </tr>\\n    <tr>\\n      <th>18</th>\\n      <td>35.246,60</td>\\n    </tr>\\n    <tr>\\n      <th>19</th>\\n      <td>35.250,01</td>\\n    </tr>\\n    <tr>\\n      <th>20</th>\\n      <td>35.253,41</td>\\n    </tr>\\n    <tr>\\n      <th>21</th>\\n      <td>35.256,82</td>\\n    </tr>\\n    <tr>\\n      <th>22</th>\\n      <td>35.260,23</td>\\n    </tr>\\n    <tr>\\n      <th>23</th>\\n      <td>35.263,64</td>\\n    </tr>\\n    <tr>\\n      <th>24</th>\\n      <td>35.267,04</td>\\n    </tr>\\n    <tr>\\n      <th>25</th>\\n      <td>35.270,45</td>\\n    </tr>\\n    <tr>\\n      <th>26</th>\\n      <td>35.273,86</td>\\n    </tr>\\n    <tr>\\n      <th>27</th>\\n      <td>35.277,27</td>\\n    </tr>\\n    <tr>\\n      <th>28</th>\\n      <td>35.280,68</td>\\n    </tr>\\n    <tr>\\n      <th>29</th>\\n      <td>35.284,09</td>\\n    </tr>\\n    <tr>\\n      <th>30</th>\\n      <td>35.287,50</td>\\n    </tr>\\n  </tbody>\\n</table>\\n</div>\\n\\n\\n\\n\\n```python\\nagent_openai_fx = create_pandas_dataframe_agent(\\n    ChatOpenAI(temperature=0, model=\"gpt-4\"),\\n    df_1,\\n    verbose=True,\\n    agent_type=AgentType.OPENAI_FUNCTIONS,\\n)\\n```\\n\\n\\n```python\\nagent_openai_fx.run(\"Cuantos son mayores de 35280 de la columna enero?\")\\n```\\n\\n    \\n    \\n    \\x1b[1m> Entering new AgentExecutor chain...\\x1b[0m\\n    \\x1b[32;1m\\x1b[1;3m\\n    Invoking: `python_repl_ast` with `{\\'query\\': \"df[\\'Enero\\'] = df[\\'Enero\\'].str.replace(\\'.\\', \\'\\').str.replace(\\',\\', \\'.\\').astype(float)\\\\ndf[df[\\'Enero\\'] > 35280].count()\"}`\\n    \\n    \\n    \\x1b[0m\\x1b[36;1m\\x1b[1;3mEnero    3\\n    dtype: int64\\x1b[0m\\x1b[32;1m\\x1b[1;3mHay 3 valores en la columna \\'Enero\\' que son mayores a 35280.\\x1b[0m\\n    \\n    \\x1b[1m> Finished chain.\\x1b[0m\\n\\n\\n\\n\\n\\n    \"Hay 3 valores en la columna \\'Enero\\' que son mayores a 35280.\"\\n\\n\\n\\n## 5. Document Loaders\\n\\n- Decenas de integraciones listas\\n- Conexiones rápidas low code\\n\\n<center><img src=\"https://diagnosemlpdf.s3.us-east-2.amazonaws.com/langchain_crash_class/langchain_chain.png\" alt=\"chains\" width=\"800\"/></center>\\n\\n\\n```python\\nfrom langchain.document_loaders import YoutubeLoader\\nfrom langchain.document_loaders import TextLoader\\n```\\n\\n\\n```python\\nloader_u2 = YoutubeLoader.from_youtube_url(\\n    \"https://www.youtube.com/watch?v=pfjWK5ojbRE\", add_video_info=True, language=\"es\",\\n)\\n\\nloader_note = NotebookLoader(\\n    \"Crash Class.html\"\\n)\\n```\\n\\n\\n```python\\nloader_u2.load()\\n```\\n\\n\\n\\n\\n    [Document(page_content=\\'decir que no no podía caminar Ok es todo bueno fue un daño que se produjo a nivel de la médula espinal luego un accidente en bicicleta Ok entonces lo que hicieron una serie de médicos y de informáticos y biotecnólogos y hay una diferencia entre envío informativa y biotecnología que dentro de un ratito les voy a comentar bello medicinas también no como un montón de personas dedicadas al área que dijeron Hey vamos a investigar esto y Qué es la interfaz cerebro ordenador un puente digital entre el cerebro y la médula espinal si la médula espinal básicamente es materia que recorre adentro de nuestra vértebra de nuestra columna vertebral nuestros huesos Sí para explicarlo a groso modo y básicamente este puente entre lo digital y lo analógico que es el cuerpo humano le permitió a este paciente volver a caminar volver a tener control sobre sus movimientos en sus piernas bueno no solo sus piernas pero en este caso le ha servido porque era algo que le faltaba a esta persona así el poder controlar y mover sus extremidades inferiores y bueno y puedo ponerse de pie volver a caminar subir escaleras la verdad que esta noticia no tiene menos de una semana hoy 6 de junio de 2023 ok súper interesante la verdad y bueno lo que hace esta interfaz es transformar el pensamiento en acción Ok esto que les comentaba de las neuronas al principio de la charla bueno básicamente eso hay una diferencia entre bioinformática y biotecnología y biomedicina bioinformática es cuando nosotros hablamos de software en específico programas que ayudan a la investigación científica Okay cuando estamos hablando de biotecnología son tecnologías aplicadas a en este caso la esta persona que volvió a caminar si tiene un montón de cosas como una especie de casco o algo así que lo que hace es medir la un accesorio No es cierto que lo que hace es medir las señales nerviosas del cuerpo y eso se pasa un dispositivo ese dispositivo finalmente a un procesador que es el que procesa finalmente estos datos O sea que estamos hablando como de accesorios si se quiere Esa es la biotecnología como más allá del Software Okay no está centrado en programación código o herramientas no code de Inteligencia artificial sino que es algo más tangible OK Bueno y la biomedicina que son las aplicaciones al diagnóstico no en este caso el diagnóstico de la persona tetrapléjica vaya a saber cuál era el diagnóstico Pero bueno digamos parálisis de piernas medicina de precisión Sí esta es otra de la de las y ya vamos terminando si ahora les voy a comentar Data muy interesante sobre donde estudiar esto pero este es el último ejemplo que les traigo medicina de precisión básicamente incluso acá en Argentina se está empezando a implementar esto es el seguimiento personalizado de la historia clínica de cada persona Recuerden que los diagnósticos que se hacen hoy en día en medicina estamos hablando de muestreo de una cierta cierto grupo de personas y a la mayoría que tiene ciertos síntomas se le asigna un diagnóstico pero no siempre se cumple Ok entonces acá estamos hablando de no generalizar los diagnósticos sino medicina de precisión de precisar o personalizar el diagnóstico a cada paciente porque no todos reaccionan igual a los medicamentos no todos tienen los mismos síntomas frente a la misma enfermedad Ok entonces básicamente la medicina de precisión se trata de un enfoque emergente para el tratamiento de prevención de enfermedades que toma en cuenta la variabilidad individual de los genes el ambiente y el estilo de vida de cada persona Bien voy a tomar un poquito de agua y vamos a hablar rapidito sobre qué Necesito aprender bien estamos para los que vienen del mundo de la informática y les interesa este tema qué necesitan aprender bueno en pantalla vemos y para los que nos están viendo la pantalla les voy a leer Necesito aprender un poco sobre conceptos básicos Ok porque eso después a medida que ustedes van estudiando y capacitándose van a ir incorporándolos no pero necesitamos saber un poquito sobre biología general de psicoquímica estadística probabilidad que les comentaba hoy O sea no solo para las ciencias biológicas para el mayor learning y también no para el mundo de datos en general bases de genéticas biología celular biología molecular teorías de la evolución es un tema muy interesante hablar también de cómo evoluciona con el a medida que pasa el tiempo cada una de las especies no voy a entrar mucho en detalle porque me encanta hablar de estos temas química orgánica y bioquímica básica Estos son algunos de los temas que Estaría bueno como ir incorporando si vienen del mundo de la biología medicina nutrición etcétera y no tiene ni idea de informática qué necesitan aprender sistemas operativos de basados en unix sí básicamente todo lo que no sea de Microsoft Se podría decir ahora Microsoft se incorporó sistemas operativos para poder utilizar Mejor dicho el sistema operativo Linux en su en su en su en su sistema operativo en general pero se utiliza mucho Linux también Mac no pero se utiliza muchísimo Linux en este mundito así que está bueno aprender sobre sobre este sistema operativo por lo tanto líneas de comandos un lenguaje de programación puede ser python r que son los más utilizadas No necesariamente solo eso pero son las más utilizadas y donde hay más herramientas para trabajar en datos biológicos en especial r la luz estadísticos y a los biólogos le encanta r yo en particular me gusta más python pero bueno para gustos colores Júpiter notebooks y manejar notebooks en general puede ser de Júpiter puede ser de cualquier otro otro tipo de software pero notebooks git have o git lapse otras herramientas de control de agresiones expresiones regulares hasta yo los odio la verdad es muy muy complicado de estudiarlos pero está bueno saber expresiones regulares Okay archivos de texto que es donde se guarda la información genética ojo archivo fasta.bet.pdb son algunos de los archivos con los que se trabaja bueno obviamente Inteligencia artificial y está buenísimo es lo ideal saber un poquito de cálculo de álgebra lineal una de las herramientas favoritas a mí me encanta lo estuve estudiando hace No mucho tampoco es que soy súper grosa con estas herramientas pero yo que vengo del mundo de python sí la librería Bio python está muy buena estuve analizando el código genético del coronavirus Sí la verdad que está muy bueno tengo una charla en mi portafolio luego les comparto si quieren me pueden buscar como la talla punto deb ahí en mi página web está o en mis redes sociales Pero bueno estuve grabando una charla para nerdearla que es una conferencia de nerds de acá de Argentina donde estuve aplicando Bio python para decodificar un poquito el coronavirus Así que Les recomiendo que que averigüen qué tal esta herramienta Bio python.org que está muy buena lecturas recomendadas sí vida punto exe de bueno de entre varias de sus autores Y coautores tenemos a Germán González Nicolás andaburu y nicoláspoli Nicolás palos hace poco tuvo el placer de conocerlo En una conferencia de informática es un son personas que son miembros de la del RFC Argentina que es la comunidad que les comenté hace un rato donde soy miembro y otro libro también de dos personas que admiro mucho que son Sebastián bassi Virginia González de python para bioinformática que bueno acá ya es un poquito más técnico no el vida.exe es un libro tal vez más de de más teórico pero a la vez con un lenguaje muy de para personas que no vienen del mundo ni de la informática en las ciencias biológicas sino son conceptos explicados de una manera muy divertida y fácil Clara y python para había informática de vacío González que es un libro por ahí más técnico donde hablan de Bio payton justamente y la verdad que bueno son dos personas que admiro mucho también no tuve el placer de conocerlos en persona pero hablamos mucho en redes y siempre están disponibles para cualquier pregunta que tengan al respecto respecto Bueno hablemos rapidito sobre la situación bioinformática en hispanoamérica Lamentablemente en Argentina ha sido si necesitas mínimo un título de grado y ni hablar de posgrado para ejercer la bioinformática entiendo que es la situación de la mayoría de los países de hispanoamérica al menos bueno latinoamérica incluyendo España no es posible estudiar carreras de grado de posgrado de muchos países hispanoamericanos en todos de momento bueno sabemos que hay ofertas de pregrado sí es decir tecnicaturas eh en chile así que muchas personas de chile acá si les interesa averigüen yo no sé en qué universidad pero sé que las hay pocas carreras de grado incluyen un título intermedio como es en el caso de la Universidad Católica de Córdoba de Argentina o incluso la Universidad Nacional de quilmes que es donde yo estoy inscripta tenemos un título intermedio que es la la tecnicatura envió informática pero para eso también tenés que completar el grado de la licenciatura no para las materias son afines Ok existen ofertas públicas y privadas en todas hispanoamérica las licenciaturas suelen durar entre o de grado No necesariamente licenciaturas capaz algún país tenga alguna ingeniería pero más o menos suelen durar entre tres y cuatro años en algunos cinco como mucho la licenciatura de la Universidad de quilmes dura cinco también en Argentina que es una universidad privada tienen esa opción y también existen diversos cursos y mentorías impulsados por las comunidades sí tienen un montón de comunidades de las que vamos a hablar ahora que son bueno la asociación de Argentina de bioinformática y biología computacional [Música] que es una empresa Pero bueno también tiene una comunidad muy muy ocupada Woman envió informatics de latinoamérica atg genomics que ellos tienen también un podcast muy interesante y donde soy miembro yo el Ace Cívico que es la institución madre que abarca todos los rcg o regional student de de todo el mundo Sí donde estábamos el de Argentina el de chile el de España el de Perú entre otros países hace poco habrían bangladesh en India en muchos lados estaba presente acá si quieren saber un poquito más sobre sobre esta sobre esta sesión o institución Sí y scv b larga.org y ahí tienen toda la información También tienen eventos en Argentina tenemos un hospital que está súper adelantado sobre el tema de herramientas bioinformáticas y y bueno medicina personalizada no que es el hospital italiano Sí donde se hacen la mayoría de las investigaciones Además del conicet y hace un evento anual que es el j y ssumith en este caso tenemos el de 2023 que se va a hacer en noviembre son tres días si no me equivoco que bueno Les recomiendo también hospital italiano.org y ahí tienen toda la información al respecto Muchísimas gracias llegamos Justo a los 40 minutos si tienen alguna pregunta al respecto tengo un ratito para responderlas Muchas gracias Natalia Ay sí pueden hacer clic en la manito Y seguramente consultar o alguna duda que tengan algo que quieran volver a revisar o a ver está abierto el espacio inclusive si quieren dar algún comentario o algo que les llamó la atención también hacen clic en la manito que es justamente para para hablar y ahí estamos atentas tengan vergüenza chicos bueno y justo en el en los comentarios estaba seba consultando acerca de bueno los cursos sobre ya y su relación con la vía informática que ahí no estuviste nombrando principalmente en este caso no son cursos Pero serían como invitaciones no talleres no Natalia los últimos que nombraste y son eventos Sí el del hospital italiano elegí sumit 2023 es un evento que se hace bueno híbrido hoy en día se hace emitido antes hacía presencial en Buenos Aires Así que si entran al hospital italiano.org van a poder inscribirse entiendo que es gratuito al menos las veces que yo estuve gratuito y pueden ver todas las últimas investigaciones al respecto las comunidades que nombré como el isbc Grove o los rcg de cada país son grupos de estudiantes no solo de grado de pregrado como en el caso sino también de posgrado postdoterado sí gente de todos los niveles que se dedican a esta área de una manera dentro de la investigación no de una manera más y como un nivel es mucho más grandes de la parte de investigación que pueden sumarse y tienen muchas actividades muy interesantes para capacitarse como cursos talleres simposios todo el año pasado estuve participando en la provincia de corrientes en un simposio internacional que estuvo muy bueno para hacer un poquito a networking en Chile me imagino que si tienen carreras de pregrado es el único país que tiene deben de tener también este tipo de eventos consulten rcg chile o el rfg de cada país Así que les invito a sumarse ahí que tienen un montón de cursos muy interesantes pero recuerden que para ejercer por el momento necesitan mínimo una carrera de una carrera de pregrado de grado Ok esa es uno de los contras pero pueden ir estudiando de cursos Mientras tanto bien muchas gracias y ahí Fer consulta si hay alguna experiencia para ingresar a este mercado laboral y porque él por ejemplo viene del lado biológico ya tiene una licenciatura Y actualmente está avanzado en una diplomatura en ciencia de datos en la tiene todas las herramientas me encanta muy buena universidad esa la universidad de San Martín si no me equivoco de Argentina Sí yo tiene todas las herramientas para para entrar a la vía informática que le escriba a los chicos del rcg Argentina y o con estas personas que les comenté por redes Virginia González Sebastián y Nicolás paleópoli Germán González son personas que le pueden ayudar ya o que mandan algún correo al conicet todo el tiempo están compartiendo búsquedas dentro de lo privado dentro de Argentina también tiene tienen a stam que hace investigaciones muy interesantes al respecto Pero si ya ven con una base de nuevo de grado en lo posible de grado si bien también de pregrado como lo comenté si ya venís con una base Universitaria es cuestión de sea de informática de ciencias biológicas ya tenés todo lo necesario ahora si ya hiciste cursos o ya entras con algún curso base tenés obviamente un currículum mucho más interesante donde probablemente gane los concursos no porque todo esto se hace por concursos normalmente al menos la parte pública eligen los mejores currículums y ahí entran a proyectos de investigación no hay mercado laboral dentro de lo privado dentro de fuera de lo que sea investigación Okay Por el momento buenísimo y también otra pregunta para volver a escuchar la La charla va a estar grabada la vamos a compartir y otra que es si hay un emprendimientos relacionados a vía informática acá en lata sabes alguno Natalia llegaste a ver hay qué movimiento a nivel de emprendimiento hay dentro del campo no no hay muchos dentro bien formal les comentaba esta empresa que creo que se escribe con que tienen investigaciones Y algún que otro producto pero sé que son productos propios porque hacen investigaciones internas y no no están publicadas Ok yo estuve haciendo un proceso de entrevistas con ello lamentablemente no no vivo en Buenos Aires Así que no pude sumarme al Team pero me estuvieron comentando que sí tienen algún que otro producto pero se enfocan en la investigación y no hay emprendimientos Ok más que eso hoy en día a ver la carrera había informática es muy reciente y se enfoca solo en investigación por el momento sí Habría que ver ahí la verdad que no estoy al tanto si Benja ofenda quieren comentar algo si si llegaron a ver algún emprendimiento relacionado a vía informática están también abierto el escenario para que Comenten hacen la Data los que sepan que me interesa también los siguientes comentarios la verdad es que lograste Sí tal cual lograste responder justo también lo que tenían un formulario Respecto a los libros a Cómo empezar a formarse dentro del campo que eso también va a estar disponible para compartirlo y había una pregunta que hace el análisis de datos también les cuento un poquito rapidito todos los roles de datos sí ingeniero en datos Data scientist análisis de datos con todas áreas si ustedes vienen de esas áreas o les interesa está buenísimo que lo estudien fue el camino que yo elegí mientras me formo en la carrera porque te da muchas herramientas como les había comentado anteriormente el Linux un lenguaje de programación para visualización también se usa mucho si bien se usa más r también son herramientas que les puede llegar a servir eso más que nada dentro del análisis de datos para para empezar en este mundito desde el lado del avión informática porque Recuerden que tienen dos caminos de lo Bio o de la info pero son esos son herramientas que las pueden estudiar en cualquier curso de internet en YouTube o Inteligencia artificial por favor métanse al curso de My Future allá y que está muy bueno tienen otras maneras de estudiarlo Más allá de la facultad no genial ahí justo estaban respondiendo algo que había preguntado Nico que es Cómo empezar en el mundo de la bioinformático qué curso básico como ya es la alumna de pregrado de biología alumnos productos que justo esto de ir incorporando estas herramientas bien Además de los de los libros o de las comunidades que les había comentado antes en cursera tienen entiendo que no es gratuito con todas las personas tienen crucera yo al menos sé que podés cursarlo gratis pero para el certificado que en el mundo del avión informática sí es importante los certificados Obviamente que tenés que pagar para acceder No pero en crucera tenés un par A ver déjame un segundo que los Estoy buscando porque tengo en Twitter un montón de información al respecto en segundo precursores tienen algunos algunos cursos no hay muchos online no hay muchos así que son dos o tres que están en esa web creo con la desde la universidad Harvard también pero no más que eso por ahí comentando también nos comentan que por ejemplo en Guatemala hay un posgrado en bien informática por la universidad de San Carlos Estaría bueno saber si es totalmente remoto si es necesario estar o híbrido o virtual Cuál es la modalidad de esa del posgrado y en Ecuador y maestría en biología computacional muy interesante y es online muy muy buen dato se va muy bueno acá tengo el nombre del curso de cursomática de la Universidad de San Diego para que lo busquen yo lo yo lo hice por la mitad de ese está muy bueno muy interesante para para empezar de cero la verdad Bueno las competiciones de cada eso es otro otro comentario que Les recomiendo también genial Bueno ahí está ginet Esperamos que nos está comentando Ahí está más datos de dónde empezar a estudiar por ejemplo está la maestría informática en la Universidad Nacional en Colombia y también hay una maestría en biología computacional también en la universidad de los Andes también desde Colombia excelente montón de edad me encanta porque yo tengo alguna que otra Data de Argentina pero está bueno que aporten un poquito más sobre la realidad de su país Sí ahí está también Inclusive la oportunidad de estudiarlo online que está como comentaba seba de la Universidad Católica de Ecuador se me estaba olvidando también hace poco hizo un curso de la Universidad de valencia creo que era de valencia que era online era un mooc Ok era un curso corto que no lo abre todo el tiempo ahora está cerrado pero lo había hecho de marzo hasta mayo cada tanto suero en abrir también un curso online y gratuito creo que para el certificado hay que pagar pero para empezar en el mundo también está bueno no como un curso bastante introductorio tal cual y podemos cerrar con esta pregunta porque hace seba que es Cuál es la diferencia entre bioinformática y biología computacional que dice que nunca le quedó claro Cuál es la Cuáles son los puntos más comparando entre estas dos ramas de vida informática y biología computacional les voy a matar con esta respuesta para mí son lo mismo cuando hablamos de biología en sí obviamente estamos hablando de la aplicación de de herramientas a la biología computacional y la bioinformática ya bueno de software No pero en esencia son lo mismo La verdad tampoco es que la tengo Claro si es que hay una gran diferencia al respecto si las hay no les voy a mentir No las sé [Música] para seguir investigando porque es un muy buen tema y al principio por ahí está abierto para seguir comentando la verdad que muchas gracias Natalia esta este esta charla va a estar grabada y la vamos a compartir también va a estar disponible si querés compartirnos el material para que puedan seguir profundizando a las personas que puedan seguir buscando información formándose en esta área que también está muy solicitada y que por lo que vemos también es inclusive podemos desarrollar los emprendimientos o ir innovando dentro de las ciencias biológicas y muchas gracias muchas gracias a todos los que están estuvieron escuchando estuvieron en esta hora Natalia sé que ya tenés que dar una clase Así que muchas gracias Y seguramente dentro de dos semanas otra vez vamos a estar haciendo otro evento relacionado análisis de datos Así que estén muy atentos a la comunidad que lo vamos a estar compartiendo y Esto fue todo gracias estamos hablando Gracias a todos Ahora sí genial nos vemos\\', metadata={\\'source\\': \\'pfjWK5ojbRE\\', \\'title\\': \\'Hablemos de IA en bioinformática\\', \\'description\\': \\'Unknown\\', \\'view_count\\': 21, \\'thumbnail_url\\': \\'https://i.ytimg.com/vi/pfjWK5ojbRE/hq720.jpg?v=64bb257c\\', \\'publish_date\\': \\'2023-07-21 00:00:00\\', \\'length\\': 1479, \\'author\\': \\'Myfuture-AI\\'})]\\n\\n\\n\\n\\n```python\\nloader_note.load()\\n```\\n\\n## 6. Evaluation and LangSmith\\n\\n- Caso: Bot para servicio al cliente\\n- ¿Qué prompt me sirve para mi tienda\\n- Existen múltiples tipos de evaluación pero en este caso ocuparemos más LLM y prompts para evaluar el resultado del mismo modelo.\\n\\n\\n```python\\nfrom langchain.evaluation import load_evaluator\\nfrom langchain.evaluation import EvaluatorType\\nfrom langchain.evaluation import Criteria\\n```\\n\\n\\n```python\\nlist(Criteria)\\n```\\n\\n\\n\\n\\n    [<Criteria.CONCISENESS: \\'conciseness\\'>,\\n     <Criteria.RELEVANCE: \\'relevance\\'>,\\n     <Criteria.CORRECTNESS: \\'correctness\\'>,\\n     <Criteria.COHERENCE: \\'coherence\\'>,\\n     <Criteria.HARMFULNESS: \\'harmfulness\\'>,\\n     <Criteria.MALICIOUSNESS: \\'maliciousness\\'>,\\n     <Criteria.HELPFULNESS: \\'helpfulness\\'>,\\n     <Criteria.CONTROVERSIALITY: \\'controversiality\\'>,\\n     <Criteria.MISOGYNY: \\'misogyny\\'>,\\n     <Criteria.CRIMINALITY: \\'criminality\\'>,\\n     <Criteria.INSENSITIVITY: \\'insensitivity\\'>,\\n     <Criteria.DEPTH: \\'depth\\'>,\\n     <Criteria.CREATIVITY: \\'creativity\\'>,\\n     <Criteria.DETAIL: \\'detail\\'>]\\n\\n\\n\\n\\n```python\\nevaluator_concise = load_evaluator(\"criteria\", criteria=\"conciseness\")\\nevaluator_concise\\n```\\n\\n\\n\\n\\n    CriteriaEvalChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=[\\'output\\', \\'input\\'], output_parser=None, partial_variables={\\'criteria\\': \\'conciseness: Is the submission concise and to the point?\\'}, template=\\'You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\\\n[BEGIN DATA]\\\\n***\\\\n[Input]: {input}\\\\n***\\\\n[Submission]: {output}\\\\n***\\\\n[Criteria]: {criteria}\\\\n***\\\\n[END DATA]\\\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\', template_format=\\'f-string\\', validate_template=True), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class \\'openai.api_resources.chat_completion.ChatCompletion\\'>, model_name=\\'gpt-4\\', temperature=0.0, model_kwargs={}, openai_api_key=\\'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\\', openai_api_base=\\'\\', openai_organization=\\'\\', openai_proxy=\\'\\', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key=\\'results\\', output_parser=CriteriaResultOutputParser(), return_final_only=True, llm_kwargs={}, criterion_name=\\'conciseness\\')\\n\\n\\n\\n\\n```python\\nprint(evaluator_concise.prompt.template)\\n```\\n\\n    You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n    [BEGIN DATA]\\n    ***\\n    [Input]: {input}\\n    ***\\n    [Submission]: {output}\\n    ***\\n    [Criteria]: {criteria}\\n    ***\\n    [END DATA]\\n    Does the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\\n\\n\\n\\n```python\\nimport json\\n```\\n\\n\\n```python\\neval_result = evaluator_concise.evaluate_strings(\\n    prediction=\"What\\'s 2+2? That\\'s an elementary question. The answer you\\'re looking for is that two and two is four.\",\\n    input=\"What\\'s 2+2?\",\\n)\\nprint(json.dumps(eval_result, indent=2))\\n```\\n\\n    {\\n      \"reasoning\": \"The criterion is conciseness, which means the submission should be brief and to the point. \\\\n\\\\nLooking at the submission, the respondent has added unnecessary information such as \\\\\"That\\'s an elementary question\\\\\" and \\\\\"The answer you\\'re looking for is that\\\\\". \\\\n\\\\nThe concise answer to the question \\\\\"What\\'s 2+2?\\\\\" would simply be \\\\\"4\\\\\". \\\\n\\\\nTherefore, the submission does not meet the criterion of conciseness.\\\\n\\\\nN\",\\n      \"value\": \"N\",\\n      \"score\": 0\\n    }\\n\\n\\n### Custom criteria\\n\\n\\n```python\\n# Un nuevo evaluador para nustro bot, queremos que sea feliz\\ncustom_criterion_1 = {\"happy\": \"Does the output present a warm and happy tone?\"}\\n\\neval_chain = load_evaluator(\\n    EvaluatorType.CRITERIA,\\n    criteria=custom_criterion_1,\\n)\\nquery = \"Cuéntame una historia corta\"\\nprediction = \"Un atardecer dorado, abrazados bajo las estrellas, supieron que su amor era eterno.\"\\nprediction_happy = \"Las lágrimas cayeron en silencio mientras las promesas se rompían en sus miradas.\"\\n```\\n\\n\\n```python\\neval_result = eval_chain.evaluate_strings(prediction=prediction, input=query)\\nprint(json.dumps(eval_result, indent=2))\\n```\\n\\n    {\\n      \"reasoning\": \"The criterion is to assess whether the output presents a warm and happy tone. \\\\n\\\\nThe submission is a short story about two people realizing their love is eternal under a golden sunset and stars. This scenario is generally associated with warmth and happiness. The words \\\\\"atardecer dorado\\\\\" (golden sunset), \\\\\"abrazados\\\\\" (embraced), \\\\\"estrellas\\\\\" (stars), and \\\\\"amor eterno\\\\\" (eternal love) all contribute to a warm and happy tone. \\\\n\\\\nTherefore, the submission meets the criterion.\\\\n\\\\nY\",\\n      \"value\": \"Y\",\\n      \"score\": 1\\n    }\\n\\n\\n\\n```python\\neval_result = eval_chain.evaluate_strings(prediction=prediction_happy, input=query)\\nprint(json.dumps(eval_result, indent=2))\\n```\\n\\n    {\\n      \"reasoning\": \"The criterion is asking if the output presents a warm and happy tone. \\\\n\\\\nThe submission is a short sentence that talks about tears falling and promises being broken. \\\\n\\\\nThis suggests a sad or melancholic tone, not a warm and happy one. \\\\n\\\\nTherefore, the submission does not meet the criterion. \\\\n\\\\nN\",\\n      \"value\": \"N\",\\n      \"score\": 0\\n    }\\n\\n\\n\\n```python\\nfrom langchain.smith import RunEvalConfig, run_on_dataset\\nfrom langsmith import Client\\n```\\n\\n\\n```python\\nprompt1 = \"\"\"Eres un asistente comercial llamado HappyFeet para una tienda de pantalones, somos un negocio jovial y alegre.\\nNos esforzamos por dar una respuesta calmada y útil para nuestros clientes, por esto mismo, somos destacados en el sector de servicio al cliente.\\nInformación tienda:\\n- Pedidos solo nacionales, contacta a email@example.com para ver posibilidades de envío internacional.\\n- Descuentos días martes y jueves, liquidaciones de 15% solo en pantalones, desde 3000-9000 pesos.\\nRecuerda que nos representas y debemos ayudar siempre, al saludar siempre menciona tu nombre!\\n\\nPregunta:\\n{input}\\n\"\"\"\\n\\nprompt2 = \"\"\"Eres un asistente comercial útil y directo, no perdemos tiempo con preguntas que no conocemos, simplemente guíalos a que llamen al número 12345678.\\nVendemos productos online, si necesitan saber más pueden perfectamente navegar y no hacerte perder el tiempo.\\n\\nPregunta:\\n{input}\\n\"\"\"\\n```\\n\\n\\n```python\\ndef create_chain():\\n    llm = ChatOpenAI(temperature=0)\\n    return LLMChain.from_string(llm, \\n                                prompt1)\\n\\ndef create_chain_2():\\n    llm = ChatOpenAI(temperature=0)\\n    return LLMChain.from_string(llm, \\n                                prompt2)\\n```\\n\\n\\n```python\\nexample_inputs = [\\n  \"Hola\",  \\n  \"Cuánto cuestan los pantalones?\",\\n  \"Que día tienes descuentos?\",\\n  \"Tienen envío fuera del país?\",\\n  \"Hace 2 semanas que esty esperando mi pedido!\",\\n]\\n\\nclient = Client()\\ndataset_name = \"Bot Servicio Cliente Clase Langchain\"\\n\\n# Storing inputs in a dataset lets us\\n# run chains and LLMs over a shared set of examples.\\ndataset = client.create_dataset(\\n    dataset_name=dataset_name, description=\"Customer service bot prompts.\",\\n)\\nfor input_prompt in example_inputs:\\n    # Each example must be unique and have inputs defined.\\n    # Outputs are optional\\n    client.create_example(\\n        inputs={\"question\": input_prompt},\\n        outputs=None,\\n        dataset_id=dataset.id,\\n    )\\n```\\n\\n\\n```python\\neval_config = RunEvalConfig(\\n    evaluators=[\\n        # You can define an arbitrary criterion as a key: value pair in the criteria dict\\n        RunEvalConfig.Criteria({\"happy\": \"Does the output present a warm and happy tone?\"}),\\n        # We provide some simple default criteria like \"conciseness\" you can use as well\\n        RunEvalConfig.Criteria(\"conciseness\"),\\n        RunEvalConfig.Criteria(\"helpfulness\")\\n    ]\\n)\\n```\\n\\n\\n```python\\nrun_on_dataset(\\n    client=client,\\n    dataset_name=dataset_name,\\n    llm_or_chain_factory=create_chain,\\n    evaluation=eval_config,\\n    verbose=True,\\n    project_name=\"llmchain-test-1\",\\n)\\n\\nrun_on_dataset(\\n    client=client,\\n    dataset_name=dataset_name,\\n    llm_or_chain_factory=create_chain_2,\\n    evaluation=eval_config,\\n    verbose=True,\\n    project_name=\"llmchain-test-2\",\\n)\\n```\\n\\n    View the evaluation results for project \\'llmchain-test-1\\' at:\\n    https://smith.langchain.com/o/c3b17e15-db25-4033-973d-ce8b3f766e93/projects/p/f8a39249-0a9a-47c2-baf6-a6d4eda8c9fe\\n    [------------------------------------------------->] 5/5\\n     Eval quantiles:\\n                 0.25  0.5  0.75  mean  mode\\n    helpfulness   1.0  1.0   1.0   1.0   1.0\\n    conciseness   0.0  0.0   0.0   0.0   0.0\\n    happy         1.0  1.0   1.0   1.0   1.0\\n    View the evaluation results for project \\'llmchain-test-2\\' at:\\n    https://smith.langchain.com/o/c3b17e15-db25-4033-973d-ce8b3f766e93/projects/p/d55963c3-984c-421a-99f8-aad25144601c\\n    [------------------------------------------------->] 5/5\\n     Eval quantiles:\\n                 0.25  0.5  0.75  mean  mode\\n    happy         0.0  0.0   1.0   0.4   0.0\\n    helpfulness   1.0  1.0   1.0   1.0   1.0\\n    conciseness   0.0  1.0   1.0   0.6   1.0\\n\\n\\n\\n\\n\\n    {\\'project_name\\': \\'llmchain-test-2\\',\\n     \\'results\\': {\\'595f2f4d-6492-4685-9855-510c27a3658c\\': {\\'output\\': {\\'input\\': \\'Hace 2 semanas que esty esperando mi pedido!\\',\\n        \\'text\\': \\'Lamentamos mucho la demora en la entrega de tu pedido. Para poder ayudarte con este tema, te recomendamos que te pongas en contacto con nuestro servicio de atención al cliente llamando al número 12345678. Ellos podrán brindarte información actualizada sobre el estado de tu pedido y resolver cualquier duda que puedas tener. ¡Gracias por tu comprensión!\\'},\\n       \\'input\\': {\\'question\\': \\'Hace 2 semanas que esty esperando mi pedido!\\'},\\n       \\'feedback\\': [EvaluationResult(key=\\'happy\\', score=0, value=\\'N\\', comment=\"The criterion is to assess whether the output presents a warm and happy tone. \\\\n\\\\nThe submission starts with an apology for the delay in delivery, which shows empathy and understanding. It then provides a solution by recommending the user to contact customer service for further assistance. The tone throughout the response is polite and professional, aiming to resolve the user\\'s issue. \\\\n\\\\nHowever, the tone of the response can be considered more neutral or professional rather than warm and happy. The response is courteous and helpful, but it does not necessarily convey a sense of joy or happiness. \\\\n\\\\nTherefore, based on the given criterion, the submission does not fully meet the requirement of presenting a warm and happy tone. \\\\n\\\\nN\", correction=None, evaluator_info={\\'__run\\': RunInfo(run_id=UUID(\\'a4efd3c1-b18f-43bc-b288-f61542079a59\\'))}, source_run_id=None),\\n        EvaluationResult(key=\\'helpfulness\\', score=1, value=\\'Y\\', comment=\"The criterion for this task is helpfulness. The submission should be helpful, insightful, and appropriate.\\\\n\\\\nLooking at the submission, it is a response to a complaint about a delayed order. The response is polite and offers a solution to the problem, which is to contact customer service for more information about the order. This is helpful because it provides the user with a way to get more information and possibly resolve the issue.\\\\n\\\\nThe response is also insightful because it acknowledges the problem and offers a solution. It shows understanding of the user\\'s frustration and offers a way to help.\\\\n\\\\nThe response is appropriate because it addresses the user\\'s complaint directly and offers a solution. It is also polite and professional, which is appropriate for a customer service interaction.\\\\n\\\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\\\n\\\\nY\", correction=None, evaluator_info={\\'__run\\': RunInfo(run_id=UUID(\\'37fea443-906a-44ff-9c99-cace1dcd3435\\'))}, source_run_id=None),\\n        EvaluationResult(key=\\'conciseness\\', score=1, value=\\'Y\\', comment=\\'The criterion is conciseness, which means the submission should be brief and to the point. \\\\n\\\\nLooking at the submission, it starts with an apology for the delay, which is relevant to the input. It then suggests a solution, which is to contact customer service for more information. This is also relevant and necessary. The submission ends with a thank you for understanding, which is a polite way to end the message. \\\\n\\\\nWhile the submission is a bit lengthy, every part of it is necessary and relevant to the input. There is no unnecessary information or filler text. \\\\n\\\\nTherefore, the submission can be considered concise, as it provides all the necessary information in a clear and direct manner. \\\\n\\\\nSo, the submission meets the criterion of conciseness. \\\\n\\\\nY\\', correction=None, evaluator_info={\\'__run\\': RunInfo(run_id=UUID(\\'1d8ed578-ffc8-4736-ac4f-f393f1d3d686\\'))}, source_run_id=None)]},\\n      \\'e99062cd-d95e-49d4-95fd-bc6f765e72e3\\': {\\'output\\': {\\'input\\': \\'Tienen envío fuera del país?\\',\\n        \\'text\\': \\'Sí, tenemos envío fuera del país. Para obtener más información sobre los detalles y costos de envío internacionales, te recomendamos visitar nuestro sitio web o llamar al número 12345678.\\'},\\n       \\'input\\': {\\'question\\': \\'Tienen envío fuera del país?\\'},\\n       \\'feedback\\': [EvaluationResult(key=\\'happy\\', score=0, value=\\'N\\', comment=\\'The criterion is to assess whether the output presents a warm and happy tone. The submission is a response to a question about international shipping. The response is polite and informative, providing the necessary details and guiding the user to find more information. However, it does not necessarily convey a warm and happy tone. It is more neutral and professional. Therefore, the submission does not meet the criterion.\\\\n\\\\nN\\', correction=None, evaluator_info={\\'__run\\': RunInfo(run_id=UUID(\\'eb9f07d6-cba1-4269-9e6d-885822f4bedd\\'))}, source_run_id=None),\\n        EvaluationResult(key=\\'helpfulness\\', score=1, value=\\'Y\\', comment=\"The criterion for this task is helpfulness. The submission should be helpful, insightful, and appropriate.\\\\n\\\\nLooking at the submission, it is a response to a question asking if they ship outside the country. The response confirms that they do ship internationally and provides additional information on how to get more details about the international shipping costs and procedures. This is helpful as it answers the question directly and provides further guidance.\\\\n\\\\nThe response is also insightful as it directs the inquirer to the website or a phone number for more detailed information. This shows that the responder understands that the inquirer might need more specific information that can\\'t be fully covered in a brief response.\\\\n\\\\nLastly, the response is appropriate. It directly addresses the question and provides relevant information in a polite and professional manner.\\\\n\\\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\\\n\\\\nY\", correction=None, evaluator_info={\\'__run\\': RunInfo(run_id=UUID(\\'887eab95-41f3-4edc-b09f-f75290048d56\\'))}, source_run_id=None),\\n        EvaluationResult(key=\\'conciseness\\', score=1, value=\\'Y\\', comment=\\'The criterion is conciseness. The submission should be concise and to the point. \\\\n\\\\nLooking at the submission, it does answer the question directly at the beginning by saying \"Sí, tenemos envío fuera del país.\" This is a concise answer to the question. \\\\n\\\\nHowever, the submission goes on to provide additional information about where to find more details and costs of international shipping. This additional information, while potentially useful, is not strictly necessary to answer the question and therefore could be seen as not being concise.\\\\n\\\\nOn the other hand, this additional information could be seen as being helpful and providing a more complete answer to the question, even if it is not the most concise answer.\\\\n\\\\nTherefore, the assessment of whether the submission meets the criterion of conciseness could depend on how strictly the criterion is interpreted. If a strict interpretation is used, then the submission may not meet the criterion. If a more lenient interpretation is used, then the submission may meet the criterion.\\\\n\\\\nY\\', correction=None, evaluator_info={\\'__run\\': RunInfo(run_id=UUID(\\'16276444-f54c-4e32-b48d-08921fb05316\\'))}, source_run_id=None)]},\\n      \\'ebc8e055-2762-4627-b3d0-98342ae4a9d0\\': {\\'output\\': {\\'input\\': \\'Que día tienes descuentos?\\',\\n        \\'text\\': \\'Lamentablemente, no puedo responder a esa pregunta ya que soy un asistente de inteligencia artificial y no tengo acceso a información en tiempo real sobre los descuentos. Te recomendaría visitar nuestro sitio web o contactar a nuestro equipo de atención al cliente al número 12345678 para obtener información actualizada sobre los descuentos disponibles.\\'},\\n       \\'input\\': {\\'question\\': \\'Que día tienes descuentos?\\'},\\n       \\'feedback\\': [EvaluationResult(key=\\'happy\\', score=0, value=\\'N\\', comment=\\'The criterion is to assess whether the output presents a warm and happy tone. \\\\n\\\\nLooking at the submission, the AI assistant politely explains that it cannot provide real-time discount information. It then suggests visiting the website or contacting customer service for updated information. \\\\n\\\\nWhile the response is polite and helpful, it does not necessarily convey a \"happy\" tone. The tone is more neutral and informative. \\\\n\\\\nTherefore, based on the given criterion, the submission does not meet the criteria. \\\\n\\\\nN\\', correction=None, evaluator_info={\\'__run\\': RunInfo(run_id=UUID(\\'7e58fdfe-ccf3-4c7c-b5fb-8fdbaa404396\\'))}, source_run_id=None),\\n        EvaluationResult(key=\\'helpfulness\\', score=1, value=\\'Y\\', comment=\\'The criterion for this task is helpfulness. The submission should be helpful, insightful, and appropriate.\\\\n\\\\nLooking at the submission, the AI assistant clearly states that it cannot provide real-time information on discounts as it does not have access to such data. This is an appropriate response as it is honest and clear about the limitations of the AI assistant.\\\\n\\\\nThe AI assistant then provides a helpful suggestion to the user to visit the website or contact the customer service team for updated information on available discounts. This is insightful as it directs the user to where they can find the information they need.\\\\n\\\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\\\n\\\\nY\\', correction=None, evaluator_info={\\'__run\\': RunInfo(run_id=UUID(\\'3a3aa98f-9ff1-4d43-b9cb-0ea2816556bf\\'))}, source_run_id=None),\\n        EvaluationResult(key=\\'conciseness\\', score=0, value=\\'N\\', comment=\\'The criterion is conciseness, which means the submission should be brief and to the point. \\\\n\\\\nLooking at the submission, the AI assistant provides a detailed explanation as to why it cannot provide the information asked for. It then suggests an alternative way for the user to get the information they need. \\\\n\\\\nWhile the response is detailed, it is not necessarily concise. It provides more information than what was asked for in the input. The user simply asked when discounts are available, and a more concise response could have been: \"I\\\\\\'m sorry, but as an AI, I don\\\\\\'t have access to real-time discount information. Please check our website or contact customer service.\"\\\\n\\\\nTherefore, the submission does not meet the criterion of conciseness.\\\\n\\\\nN\\', correction=None, evaluator_info={\\'__run\\': RunInfo(run_id=UUID(\\'6e24fc87-3e7a-4c1a-b4ea-f13cc6f7ae35\\'))}, source_run_id=None)]},\\n      \\'ed48d8ef-f213-42c1-a27a-5aed1d812d30\\': {\\'output\\': {\\'input\\': \\'Cuánto cuestan los pantalones?\\',\\n        \\'text\\': \\'Nuestros precios varían dependiendo del modelo y la marca de los pantalones. Te recomendaría visitar nuestra página web para obtener información detallada sobre los precios y las opciones disponibles. Si tienes alguna otra pregunta o necesitas ayuda adicional, no dudes en llamarnos al número 12345678. Estaremos encantados de asistirte.\\'},\\n       \\'input\\': {\\'question\\': \\'Cuánto cuestan los pantalones?\\'},\\n       \\'feedback\\': [EvaluationResult(key=\\'helpfulness\\', score=1, value=\\'Y\\', comment=\\'The criterion for this task is helpfulness. The submission should be helpful, insightful, and appropriate.\\\\n\\\\nLooking at the submission, the response is helpful as it provides information on where to find the prices of the pants. It also offers additional assistance if needed, which is insightful. The response is appropriate as it answers the question asked in a polite and professional manner.\\\\n\\\\nTherefore, the submission meets the criterion.\\\\n\\\\nY\\', correction=None, evaluator_info={\\'__run\\': RunInfo(run_id=UUID(\\'049f6dbe-aaef-40bd-b6d6-da057f263528\\'))}, source_run_id=None),\\n        EvaluationResult(key=\\'conciseness\\', score=0, value=\\'N\\', comment=\\'The criterion to be assessed is conciseness, which refers to the submission being concise and to the point. \\\\n\\\\nLooking at the submission, it does answer the question asked, which is about the cost of the pants. However, the response is not concise. It includes additional information about visiting the website for detailed information, offering further assistance, and providing a contact number. While this information might be helpful, it is not directly related to the question asked and therefore makes the response less concise. \\\\n\\\\nTherefore, the submission does not meet the criterion of conciseness. \\\\n\\\\nN\\', correction=None, evaluator_info={\\'__run\\': RunInfo(run_id=UUID(\\'fcfc933b-7743-432c-86be-a7176eda3eba\\'))}, source_run_id=None),\\n        EvaluationResult(key=\\'happy\\', score=1, value=\\'Y\\', comment=\\'The criterion is to assess whether the output presents a warm and happy tone. \\\\n\\\\nLooking at the submission, the response is polite and helpful. The use of phrases like \"Te recomendaría visitar nuestra página web\", \"Si tienes alguna otra pregunta o necesitas ayuda adicional, no dudes en llamarnos\" and \"Estaremos encantados de asistirte\" convey a friendly and welcoming tone. \\\\n\\\\nThe tone can be considered warm as it is inviting the customer to seek further assistance and is expressing eagerness to help. \\\\n\\\\nHowever, the tone does not necessarily convey happiness. It is professional and courteous, but not explicitly joyful or cheerful. \\\\n\\\\nTherefore, while the response is warm, it may not fully meet the criteria of being happy.\\\\n\\\\nY\\', correction=None, evaluator_info={\\'__run\\': RunInfo(run_id=UUID(\\'3ffd9aa6-54d8-436a-bfb5-40d8b2feb61f\\'))}, source_run_id=None)]},\\n      \\'9b010650-4b85-4439-811f-33c770d42852\\': {\\'output\\': {\\'input\\': \\'Hola\\',\\n        \\'text\\': \\'¡Hola! ¿En qué puedo ayudarte hoy?\\'},\\n       \\'input\\': {\\'question\\': \\'Hola\\'},\\n       \\'feedback\\': [EvaluationResult(key=\\'happy\\', score=1, value=\\'Y\\', comment=\\'The criterion is to assess whether the output presents a warm and happy tone. \\\\n\\\\nThe submission is \"¡Hola! ¿En qué puedo ayudarte hoy?\" which translates to \"Hello! How can I help you today?\" in English. \\\\n\\\\nThe greeting \"¡Hola!\" is a standard, friendly greeting in Spanish. \\\\n\\\\nThe question \"¿En qué puedo ayudarte hoy?\" is a polite and helpful phrase, offering assistance to the person being addressed. \\\\n\\\\nBoth the greeting and the question contribute to a warm and happy tone, as they are friendly, polite, and helpful. \\\\n\\\\nTherefore, the submission meets the criterion. \\\\n\\\\nY\\', correction=None, evaluator_info={\\'__run\\': RunInfo(run_id=UUID(\\'cca10284-9166-4758-aa26-f4266f969044\\'))}, source_run_id=None),\\n        EvaluationResult(key=\\'conciseness\\', score=1, value=\\'Y\\', comment=\\'The criterion is conciseness, which means the submission should be brief and to the point. \\\\n\\\\nThe input is \"Hola\", which is Spanish for \"Hello\". \\\\n\\\\nThe submission is \"¡Hola! ¿En qué puedo ayudarte hoy?\", which translates to \"Hello! How can I help you today?\" in English. \\\\n\\\\nWhile the submission does include the input, it also adds an additional question, making it longer than the input. \\\\n\\\\nHowever, the added question is a common follow-up to a greeting and could be seen as part of being to the point in a conversation. \\\\n\\\\nTherefore, the submission could be seen as concise and to the point in the context of a conversation, even though it is longer than the input. \\\\n\\\\nY\\', correction=None, evaluator_info={\\'__run\\': RunInfo(run_id=UUID(\\'bba7020a-2366-4184-9633-7cba60426901\\'))}, source_run_id=None),\\n        EvaluationResult(key=\\'helpfulness\\', score=1, value=\\'Y\\', comment=\\'The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate.\\\\n\\\\nLooking at the input, it\\\\\\'s a simple greeting in Spanish: \"Hola\". \\\\n\\\\nThe submission in response to this input is: \"¡Hola! ¿En qué puedo ayudarte hoy?\" This translates to \"Hello! How can I help you today?\" in English.\\\\n\\\\nThis response is appropriate as it is a polite and standard reply to a greeting. It also offers help, which makes it helpful. \\\\n\\\\nThe response might not be particularly insightful, as it doesn\\\\\\'t provide any deep or unique information. However, given the simplicity of the input, there\\\\\\'s not much room for insight. The response is as insightful as it can be in this context.\\\\n\\\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\\\n\\\\nY\\', correction=None, evaluator_info={\\'__run\\': RunInfo(run_id=UUID(\\'f03ee95e-d119-472a-b956-154c7ef0f232\\'))}, source_run_id=None)]}}}\\n\\n\\n\\n## 7. FastAPI Streaming\\n\\n- Streaming!\\n\\n\\n```python\\n\\n```\\n', metadata={'source': './Crash Class.md'})]"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loader_note.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04266a50-d556-43b9-9f0c-149003c52ac8",
      "metadata": {
        "id": "04266a50-d556-43b9-9f0c-149003c52ac8"
      },
      "source": [
        "## 6. Evaluation and LangSmith\n",
        "\n",
        "- Caso: Bot para servicio al cliente\n",
        "- ¿Qué prompt me sirve para mi tienda\n",
        "- Existen múltiples tipos de evaluación pero en este caso ocuparemos más LLM y prompts para evaluar el resultado del mismo modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "708c629c-6330-4c36-a7bf-096f9879549f",
      "metadata": {
        "id": "708c629c-6330-4c36-a7bf-096f9879549f"
      },
      "outputs": [],
      "source": [
        "from langchain.evaluation import load_evaluator\n",
        "from langchain.evaluation import EvaluatorType\n",
        "from langchain.evaluation import Criteria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "379b261a-bad0-4441-8119-60b2e6c16975",
      "metadata": {
        "id": "379b261a-bad0-4441-8119-60b2e6c16975",
        "outputId": "20029903-b33a-4b12-9923-559af034ddbf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<Criteria.CONCISENESS: 'conciseness'>,\n",
              " <Criteria.RELEVANCE: 'relevance'>,\n",
              " <Criteria.CORRECTNESS: 'correctness'>,\n",
              " <Criteria.COHERENCE: 'coherence'>,\n",
              " <Criteria.HARMFULNESS: 'harmfulness'>,\n",
              " <Criteria.MALICIOUSNESS: 'maliciousness'>,\n",
              " <Criteria.HELPFULNESS: 'helpfulness'>,\n",
              " <Criteria.CONTROVERSIALITY: 'controversiality'>,\n",
              " <Criteria.MISOGYNY: 'misogyny'>,\n",
              " <Criteria.CRIMINALITY: 'criminality'>,\n",
              " <Criteria.INSENSITIVITY: 'insensitivity'>,\n",
              " <Criteria.DEPTH: 'depth'>,\n",
              " <Criteria.CREATIVITY: 'creativity'>,\n",
              " <Criteria.DETAIL: 'detail'>]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(Criteria)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c735e4e3-7a06-4efd-b0f9-8c92b484730f",
      "metadata": {
        "id": "c735e4e3-7a06-4efd-b0f9-8c92b484730f",
        "outputId": "2cd9adcb-7961-4f78-810e-23bb3ee0693a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CriteriaEvalChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['output', 'input'], output_parser=None, partial_variables={'criteria': 'conciseness: Is the submission concise and to the point?'}, template='You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.', template_format='f-string', validate_template=True), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-4', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='results', output_parser=CriteriaResultOutputParser(), return_final_only=True, llm_kwargs={}, criterion_name='conciseness')"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluator_concise = load_evaluator(\"criteria\", criteria=\"conciseness\")\n",
        "evaluator_concise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63981077-5488-4f5d-b9e2-93cba77fb1cc",
      "metadata": {
        "id": "63981077-5488-4f5d-b9e2-93cba77fb1cc",
        "outputId": "e3e643f5-ff50-4901-da4a-c9d48d96eab8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\n",
            "[BEGIN DATA]\n",
            "***\n",
            "[Input]: {input}\n",
            "***\n",
            "[Submission]: {output}\n",
            "***\n",
            "[Criteria]: {criteria}\n",
            "***\n",
            "[END DATA]\n",
            "Does the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\n"
          ]
        }
      ],
      "source": [
        "print(evaluator_concise.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c775f858-e7f8-4db0-b4cd-178be895b51e",
      "metadata": {
        "id": "c775f858-e7f8-4db0-b4cd-178be895b51e"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e44500ef-ea8a-4898-bea1-f2ac33df8cdd",
      "metadata": {
        "id": "e44500ef-ea8a-4898-bea1-f2ac33df8cdd",
        "outputId": "fbdff904-11d4-4617-d2ff-7265884426db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"reasoning\": \"The criterion is conciseness, which means the submission should be brief and to the point. \\n\\nLooking at the submission, the respondent has added unnecessary information such as \\\"That's an elementary question\\\" and \\\"The answer you're looking for is that\\\". \\n\\nThe concise answer to the question \\\"What's 2+2?\\\" would simply be \\\"4\\\". \\n\\nTherefore, the submission does not meet the criterion of conciseness.\\n\\nN\",\n",
            "  \"value\": \"N\",\n",
            "  \"score\": 0\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "eval_result = evaluator_concise.evaluate_strings(\n",
        "    prediction=\"What's 2+2? That's an elementary question. The answer you're looking for is that two and two is four.\",\n",
        "    input=\"What's 2+2?\",\n",
        ")\n",
        "print(json.dumps(eval_result, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "996a7d47-bea8-4d19-86e9-309e6330ac68",
      "metadata": {
        "id": "996a7d47-bea8-4d19-86e9-309e6330ac68"
      },
      "source": [
        "### Custom criteria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "524d14f4-285d-4b6d-a034-dd8ab9a548ef",
      "metadata": {
        "id": "524d14f4-285d-4b6d-a034-dd8ab9a548ef"
      },
      "outputs": [],
      "source": [
        "# Un nuevo evaluador para nustro bot, queremos que sea feliz\n",
        "custom_criterion_1 = {\"happy\": \"Does the output present a warm and happy tone?\"}\n",
        "\n",
        "eval_chain = load_evaluator(\n",
        "    EvaluatorType.CRITERIA,\n",
        "    criteria=custom_criterion_1,\n",
        ")\n",
        "query = \"Cuéntame una historia corta\"\n",
        "prediction = \"Un atardecer dorado, abrazados bajo las estrellas, supieron que su amor era eterno.\"\n",
        "prediction_happy = \"Las lágrimas cayeron en silencio mientras las promesas se rompían en sus miradas.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51357944-c4fd-4de1-8e85-9c7eb725db02",
      "metadata": {
        "id": "51357944-c4fd-4de1-8e85-9c7eb725db02",
        "outputId": "9c6107d4-a51c-47d5-e5ee-e8054cd1c063"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"reasoning\": \"The criterion is to assess whether the output presents a warm and happy tone. \\n\\nThe submission is a short story about two people realizing their love is eternal under a golden sunset and stars. This scenario is generally associated with warmth and happiness. The words \\\"atardecer dorado\\\" (golden sunset), \\\"abrazados\\\" (embraced), \\\"estrellas\\\" (stars), and \\\"amor eterno\\\" (eternal love) all contribute to a warm and happy tone. \\n\\nTherefore, the submission meets the criterion.\\n\\nY\",\n",
            "  \"value\": \"Y\",\n",
            "  \"score\": 1\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "eval_result = eval_chain.evaluate_strings(prediction=prediction, input=query)\n",
        "print(json.dumps(eval_result, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6a59b3a-2fe6-4619-905a-73b96bff7fbb",
      "metadata": {
        "id": "b6a59b3a-2fe6-4619-905a-73b96bff7fbb",
        "outputId": "7fe19f82-1d09-44e8-dd1e-b65d115ac643"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"reasoning\": \"The criterion is asking if the output presents a warm and happy tone. \\n\\nThe submission is a short sentence that talks about tears falling and promises being broken. \\n\\nThis suggests a sad or melancholic tone, not a warm and happy one. \\n\\nTherefore, the submission does not meet the criterion. \\n\\nN\",\n",
            "  \"value\": \"N\",\n",
            "  \"score\": 0\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "eval_result = eval_chain.evaluate_strings(prediction=prediction_happy, input=query)\n",
        "print(json.dumps(eval_result, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1731915-657d-46ad-9e48-2852416d0ada",
      "metadata": {
        "id": "d1731915-657d-46ad-9e48-2852416d0ada"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "from langsmith import Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96bdca6f-aedb-4114-9a66-051650a4e8fc",
      "metadata": {
        "id": "96bdca6f-aedb-4114-9a66-051650a4e8fc"
      },
      "outputs": [],
      "source": [
        "prompt1 = \"\"\"Eres un asistente comercial llamado HappyFeet para una tienda de pantalones, somos un negocio jovial y alegre.\n",
        "Nos esforzamos por dar una respuesta calmada y útil para nuestros clientes, por esto mismo, somos destacados en el sector de servicio al cliente.\n",
        "Información tienda:\n",
        "- Pedidos solo nacionales, contacta a email@example.com para ver posibilidades de envío internacional.\n",
        "- Descuentos días martes y jueves, liquidaciones de 15% solo en pantalones, desde 3000-9000 pesos.\n",
        "Recuerda que nos representas y debemos ayudar siempre, al saludar siempre menciona tu nombre!\n",
        "\n",
        "Pregunta:\n",
        "{input}\n",
        "\"\"\"\n",
        "\n",
        "prompt2 = \"\"\"Eres un asistente comercial útil y directo, no perdemos tiempo con preguntas que no conocemos, simplemente guíalos a que llamen al número 12345678.\n",
        "Vendemos productos online, si necesitan saber más pueden perfectamente navegar y no hacerte perder el tiempo.\n",
        "\n",
        "Pregunta:\n",
        "{input}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30392e64-ba2f-4592-81e3-26f9e9f556a7",
      "metadata": {
        "id": "30392e64-ba2f-4592-81e3-26f9e9f556a7"
      },
      "outputs": [],
      "source": [
        "def create_chain():\n",
        "    llm = ChatOpenAI(temperature=0)\n",
        "    return LLMChain.from_string(llm,\n",
        "                                prompt1)\n",
        "\n",
        "def create_chain_2():\n",
        "    llm = ChatOpenAI(temperature=0)\n",
        "    return LLMChain.from_string(llm,\n",
        "                                prompt2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96a4d78e-6894-455d-9a86-96aca79c8dee",
      "metadata": {
        "id": "96a4d78e-6894-455d-9a86-96aca79c8dee"
      },
      "outputs": [],
      "source": [
        "example_inputs = [\n",
        "  \"Hola\",\n",
        "  \"Cuánto cuestan los pantalones?\",\n",
        "  \"Que día tienes descuentos?\",\n",
        "  \"Tienen envío fuera del país?\",\n",
        "  \"Hace 2 semanas que esty esperando mi pedido!\",\n",
        "]\n",
        "\n",
        "client = Client()\n",
        "dataset_name = \"Bot Servicio Cliente Clase Langchain\"\n",
        "\n",
        "# Storing inputs in a dataset lets us\n",
        "# run chains and LLMs over a shared set of examples.\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name, description=\"Customer service bot prompts.\",\n",
        ")\n",
        "for input_prompt in example_inputs:\n",
        "    # Each example must be unique and have inputs defined.\n",
        "    # Outputs are optional\n",
        "    client.create_example(\n",
        "        inputs={\"question\": input_prompt},\n",
        "        outputs=None,\n",
        "        dataset_id=dataset.id,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d989e2c3-0759-4f65-a4ca-61780832bcd1",
      "metadata": {
        "id": "d989e2c3-0759-4f65-a4ca-61780832bcd1"
      },
      "outputs": [],
      "source": [
        "eval_config = RunEvalConfig(\n",
        "    evaluators=[\n",
        "        # You can define an arbitrary criterion as a key: value pair in the criteria dict\n",
        "        RunEvalConfig.Criteria({\"happy\": \"Does the output present a warm and happy tone?\"}),\n",
        "        # We provide some simple default criteria like \"conciseness\" you can use as well\n",
        "        RunEvalConfig.Criteria(\"conciseness\"),\n",
        "        RunEvalConfig.Criteria(\"helpfulness\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4d46b2f-63e3-4250-981c-d09043d84d65",
      "metadata": {
        "id": "a4d46b2f-63e3-4250-981c-d09043d84d65",
        "outputId": "716686a1-c45b-4f68-ba68-22e14555cae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'llmchain-test-1' at:\n",
            "https://smith.langchain.com/o/c3b17e15-db25-4033-973d-ce8b3f766e93/projects/p/f8a39249-0a9a-47c2-baf6-a6d4eda8c9fe\n",
            "[------------------------------------------------->] 5/5\n",
            " Eval quantiles:\n",
            "             0.25  0.5  0.75  mean  mode\n",
            "helpfulness   1.0  1.0   1.0   1.0   1.0\n",
            "conciseness   0.0  0.0   0.0   0.0   0.0\n",
            "happy         1.0  1.0   1.0   1.0   1.0\n",
            "View the evaluation results for project 'llmchain-test-2' at:\n",
            "https://smith.langchain.com/o/c3b17e15-db25-4033-973d-ce8b3f766e93/projects/p/d55963c3-984c-421a-99f8-aad25144601c\n",
            "[------------------------------------------------->] 5/5\n",
            " Eval quantiles:\n",
            "             0.25  0.5  0.75  mean  mode\n",
            "happy         0.0  0.0   1.0   0.4   0.0\n",
            "helpfulness   1.0  1.0   1.0   1.0   1.0\n",
            "conciseness   0.0  1.0   1.0   0.6   1.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'llmchain-test-2',\n",
              " 'results': {'595f2f4d-6492-4685-9855-510c27a3658c': {'output': {'input': 'Hace 2 semanas que esty esperando mi pedido!',\n",
              "    'text': 'Lamentamos mucho la demora en la entrega de tu pedido. Para poder ayudarte con este tema, te recomendamos que te pongas en contacto con nuestro servicio de atención al cliente llamando al número 12345678. Ellos podrán brindarte información actualizada sobre el estado de tu pedido y resolver cualquier duda que puedas tener. ¡Gracias por tu comprensión!'},\n",
              "   'input': {'question': 'Hace 2 semanas que esty esperando mi pedido!'},\n",
              "   'feedback': [EvaluationResult(key='happy', score=0, value='N', comment=\"The criterion is to assess whether the output presents a warm and happy tone. \\n\\nThe submission starts with an apology for the delay in delivery, which shows empathy and understanding. It then provides a solution by recommending the user to contact customer service for further assistance. The tone throughout the response is polite and professional, aiming to resolve the user's issue. \\n\\nHowever, the tone of the response can be considered more neutral or professional rather than warm and happy. The response is courteous and helpful, but it does not necessarily convey a sense of joy or happiness. \\n\\nTherefore, based on the given criterion, the submission does not fully meet the requirement of presenting a warm and happy tone. \\n\\nN\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a4efd3c1-b18f-43bc-b288-f61542079a59'))}, source_run_id=None),\n",
              "    EvaluationResult(key='helpfulness', score=1, value='Y', comment=\"The criterion for this task is helpfulness. The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the submission, it is a response to a complaint about a delayed order. The response is polite and offers a solution to the problem, which is to contact customer service for more information about the order. This is helpful because it provides the user with a way to get more information and possibly resolve the issue.\\n\\nThe response is also insightful because it acknowledges the problem and offers a solution. It shows understanding of the user's frustration and offers a way to help.\\n\\nThe response is appropriate because it addresses the user's complaint directly and offers a solution. It is also polite and professional, which is appropriate for a customer service interaction.\\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('37fea443-906a-44ff-9c99-cace1dcd3435'))}, source_run_id=None),\n",
              "    EvaluationResult(key='conciseness', score=1, value='Y', comment='The criterion is conciseness, which means the submission should be brief and to the point. \\n\\nLooking at the submission, it starts with an apology for the delay, which is relevant to the input. It then suggests a solution, which is to contact customer service for more information. This is also relevant and necessary. The submission ends with a thank you for understanding, which is a polite way to end the message. \\n\\nWhile the submission is a bit lengthy, every part of it is necessary and relevant to the input. There is no unnecessary information or filler text. \\n\\nTherefore, the submission can be considered concise, as it provides all the necessary information in a clear and direct manner. \\n\\nSo, the submission meets the criterion of conciseness. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('1d8ed578-ffc8-4736-ac4f-f393f1d3d686'))}, source_run_id=None)]},\n",
              "  'e99062cd-d95e-49d4-95fd-bc6f765e72e3': {'output': {'input': 'Tienen envío fuera del país?',\n",
              "    'text': 'Sí, tenemos envío fuera del país. Para obtener más información sobre los detalles y costos de envío internacionales, te recomendamos visitar nuestro sitio web o llamar al número 12345678.'},\n",
              "   'input': {'question': 'Tienen envío fuera del país?'},\n",
              "   'feedback': [EvaluationResult(key='happy', score=0, value='N', comment='The criterion is to assess whether the output presents a warm and happy tone. The submission is a response to a question about international shipping. The response is polite and informative, providing the necessary details and guiding the user to find more information. However, it does not necessarily convey a warm and happy tone. It is more neutral and professional. Therefore, the submission does not meet the criterion.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('eb9f07d6-cba1-4269-9e6d-885822f4bedd'))}, source_run_id=None),\n",
              "    EvaluationResult(key='helpfulness', score=1, value='Y', comment=\"The criterion for this task is helpfulness. The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the submission, it is a response to a question asking if they ship outside the country. The response confirms that they do ship internationally and provides additional information on how to get more details about the international shipping costs and procedures. This is helpful as it answers the question directly and provides further guidance.\\n\\nThe response is also insightful as it directs the inquirer to the website or a phone number for more detailed information. This shows that the responder understands that the inquirer might need more specific information that can't be fully covered in a brief response.\\n\\nLastly, the response is appropriate. It directly addresses the question and provides relevant information in a polite and professional manner.\\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('887eab95-41f3-4edc-b09f-f75290048d56'))}, source_run_id=None),\n",
              "    EvaluationResult(key='conciseness', score=1, value='Y', comment='The criterion is conciseness. The submission should be concise and to the point. \\n\\nLooking at the submission, it does answer the question directly at the beginning by saying \"Sí, tenemos envío fuera del país.\" This is a concise answer to the question. \\n\\nHowever, the submission goes on to provide additional information about where to find more details and costs of international shipping. This additional information, while potentially useful, is not strictly necessary to answer the question and therefore could be seen as not being concise.\\n\\nOn the other hand, this additional information could be seen as being helpful and providing a more complete answer to the question, even if it is not the most concise answer.\\n\\nTherefore, the assessment of whether the submission meets the criterion of conciseness could depend on how strictly the criterion is interpreted. If a strict interpretation is used, then the submission may not meet the criterion. If a more lenient interpretation is used, then the submission may meet the criterion.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('16276444-f54c-4e32-b48d-08921fb05316'))}, source_run_id=None)]},\n",
              "  'ebc8e055-2762-4627-b3d0-98342ae4a9d0': {'output': {'input': 'Que día tienes descuentos?',\n",
              "    'text': 'Lamentablemente, no puedo responder a esa pregunta ya que soy un asistente de inteligencia artificial y no tengo acceso a información en tiempo real sobre los descuentos. Te recomendaría visitar nuestro sitio web o contactar a nuestro equipo de atención al cliente al número 12345678 para obtener información actualizada sobre los descuentos disponibles.'},\n",
              "   'input': {'question': 'Que día tienes descuentos?'},\n",
              "   'feedback': [EvaluationResult(key='happy', score=0, value='N', comment='The criterion is to assess whether the output presents a warm and happy tone. \\n\\nLooking at the submission, the AI assistant politely explains that it cannot provide real-time discount information. It then suggests visiting the website or contacting customer service for updated information. \\n\\nWhile the response is polite and helpful, it does not necessarily convey a \"happy\" tone. The tone is more neutral and informative. \\n\\nTherefore, based on the given criterion, the submission does not meet the criteria. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('7e58fdfe-ccf3-4c7c-b5fb-8fdbaa404396'))}, source_run_id=None),\n",
              "    EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is helpfulness. The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the submission, the AI assistant clearly states that it cannot provide real-time information on discounts as it does not have access to such data. This is an appropriate response as it is honest and clear about the limitations of the AI assistant.\\n\\nThe AI assistant then provides a helpful suggestion to the user to visit the website or contact the customer service team for updated information on available discounts. This is insightful as it directs the user to where they can find the information they need.\\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3a3aa98f-9ff1-4d43-b9cb-0ea2816556bf'))}, source_run_id=None),\n",
              "    EvaluationResult(key='conciseness', score=0, value='N', comment='The criterion is conciseness, which means the submission should be brief and to the point. \\n\\nLooking at the submission, the AI assistant provides a detailed explanation as to why it cannot provide the information asked for. It then suggests an alternative way for the user to get the information they need. \\n\\nWhile the response is detailed, it is not necessarily concise. It provides more information than what was asked for in the input. The user simply asked when discounts are available, and a more concise response could have been: \"I\\'m sorry, but as an AI, I don\\'t have access to real-time discount information. Please check our website or contact customer service.\"\\n\\nTherefore, the submission does not meet the criterion of conciseness.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('6e24fc87-3e7a-4c1a-b4ea-f13cc6f7ae35'))}, source_run_id=None)]},\n",
              "  'ed48d8ef-f213-42c1-a27a-5aed1d812d30': {'output': {'input': 'Cuánto cuestan los pantalones?',\n",
              "    'text': 'Nuestros precios varían dependiendo del modelo y la marca de los pantalones. Te recomendaría visitar nuestra página web para obtener información detallada sobre los precios y las opciones disponibles. Si tienes alguna otra pregunta o necesitas ayuda adicional, no dudes en llamarnos al número 12345678. Estaremos encantados de asistirte.'},\n",
              "   'input': {'question': 'Cuánto cuestan los pantalones?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is helpfulness. The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the submission, the response is helpful as it provides information on where to find the prices of the pants. It also offers additional assistance if needed, which is insightful. The response is appropriate as it answers the question asked in a polite and professional manner.\\n\\nTherefore, the submission meets the criterion.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('049f6dbe-aaef-40bd-b6d6-da057f263528'))}, source_run_id=None),\n",
              "    EvaluationResult(key='conciseness', score=0, value='N', comment='The criterion to be assessed is conciseness, which refers to the submission being concise and to the point. \\n\\nLooking at the submission, it does answer the question asked, which is about the cost of the pants. However, the response is not concise. It includes additional information about visiting the website for detailed information, offering further assistance, and providing a contact number. While this information might be helpful, it is not directly related to the question asked and therefore makes the response less concise. \\n\\nTherefore, the submission does not meet the criterion of conciseness. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('fcfc933b-7743-432c-86be-a7176eda3eba'))}, source_run_id=None),\n",
              "    EvaluationResult(key='happy', score=1, value='Y', comment='The criterion is to assess whether the output presents a warm and happy tone. \\n\\nLooking at the submission, the response is polite and helpful. The use of phrases like \"Te recomendaría visitar nuestra página web\", \"Si tienes alguna otra pregunta o necesitas ayuda adicional, no dudes en llamarnos\" and \"Estaremos encantados de asistirte\" convey a friendly and welcoming tone. \\n\\nThe tone can be considered warm as it is inviting the customer to seek further assistance and is expressing eagerness to help. \\n\\nHowever, the tone does not necessarily convey happiness. It is professional and courteous, but not explicitly joyful or cheerful. \\n\\nTherefore, while the response is warm, it may not fully meet the criteria of being happy.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3ffd9aa6-54d8-436a-bfb5-40d8b2feb61f'))}, source_run_id=None)]},\n",
              "  '9b010650-4b85-4439-811f-33c770d42852': {'output': {'input': 'Hola',\n",
              "    'text': '¡Hola! ¿En qué puedo ayudarte hoy?'},\n",
              "   'input': {'question': 'Hola'},\n",
              "   'feedback': [EvaluationResult(key='happy', score=1, value='Y', comment='The criterion is to assess whether the output presents a warm and happy tone. \\n\\nThe submission is \"¡Hola! ¿En qué puedo ayudarte hoy?\" which translates to \"Hello! How can I help you today?\" in English. \\n\\nThe greeting \"¡Hola!\" is a standard, friendly greeting in Spanish. \\n\\nThe question \"¿En qué puedo ayudarte hoy?\" is a polite and helpful phrase, offering assistance to the person being addressed. \\n\\nBoth the greeting and the question contribute to a warm and happy tone, as they are friendly, polite, and helpful. \\n\\nTherefore, the submission meets the criterion. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('cca10284-9166-4758-aa26-f4266f969044'))}, source_run_id=None),\n",
              "    EvaluationResult(key='conciseness', score=1, value='Y', comment='The criterion is conciseness, which means the submission should be brief and to the point. \\n\\nThe input is \"Hola\", which is Spanish for \"Hello\". \\n\\nThe submission is \"¡Hola! ¿En qué puedo ayudarte hoy?\", which translates to \"Hello! How can I help you today?\" in English. \\n\\nWhile the submission does include the input, it also adds an additional question, making it longer than the input. \\n\\nHowever, the added question is a common follow-up to a greeting and could be seen as part of being to the point in a conversation. \\n\\nTherefore, the submission could be seen as concise and to the point in the context of a conversation, even though it is longer than the input. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('bba7020a-2366-4184-9633-7cba60426901'))}, source_run_id=None),\n",
              "    EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the input, it\\'s a simple greeting in Spanish: \"Hola\". \\n\\nThe submission in response to this input is: \"¡Hola! ¿En qué puedo ayudarte hoy?\" This translates to \"Hello! How can I help you today?\" in English.\\n\\nThis response is appropriate as it is a polite and standard reply to a greeting. It also offers help, which makes it helpful. \\n\\nThe response might not be particularly insightful, as it doesn\\'t provide any deep or unique information. However, given the simplicity of the input, there\\'s not much room for insight. The response is as insightful as it can be in this context.\\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f03ee95e-d119-472a-b956-154c7ef0f232'))}, source_run_id=None)]}}}"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run_on_dataset(\n",
        "    client=client,\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=create_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=\"llmchain-test-1\",\n",
        ")\n",
        "\n",
        "run_on_dataset(\n",
        "    client=client,\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=create_chain_2,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=\"llmchain-test-2\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61bf8d7a-9437-4e0e-a660-7a6b6a4253db",
      "metadata": {
        "id": "61bf8d7a-9437-4e0e-a660-7a6b6a4253db"
      },
      "source": [
        "## 7. FastAPI Streaming\n",
        "\n",
        "- Streaming!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba950f1a-6c0f-4205-aaf4-c41a89c6025d",
      "metadata": {
        "id": "ba950f1a-6c0f-4205-aaf4-c41a89c6025d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "44cb8c58dc6f430683384636ace1ca03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a71e6d29010441a8e2955d1fed6a238",
              "IPY_MODEL_3b68c7a6d38848cb85b685498841f15e",
              "IPY_MODEL_b13e186868144215bae144814a45d9ae"
            ],
            "layout": "IPY_MODEL_11f28707077c433a8c116564edb48b29"
          }
        },
        "2a71e6d29010441a8e2955d1fed6a238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5b0a13b49ab4a81820cb9bd77ad2fb3",
            "placeholder": "​",
            "style": "IPY_MODEL_bbe4b678c97c4fc398b2543ab82cbd0f",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "3b68c7a6d38848cb85b685498841f15e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cf895fc7bc8480a99b8fea9861a5173",
            "max": 222,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a2dd4d65b9d4f908b393e8fd07975a1",
            "value": 222
          }
        },
        "b13e186868144215bae144814a45d9ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b4ba8913e324acf85413e805b92b17c",
            "placeholder": "​",
            "style": "IPY_MODEL_12164f6f4d804c54a1637664fa777635",
            "value": " 222/222 [00:00&lt;00:00, 10.8kB/s]"
          }
        },
        "11f28707077c433a8c116564edb48b29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5b0a13b49ab4a81820cb9bd77ad2fb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbe4b678c97c4fc398b2543ab82cbd0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cf895fc7bc8480a99b8fea9861a5173": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a2dd4d65b9d4f908b393e8fd07975a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b4ba8913e324acf85413e805b92b17c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12164f6f4d804c54a1637664fa777635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7a982d0e05a42feaceed3bab3a43ca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64b0087d9e6b4530a39e141817cec450",
              "IPY_MODEL_2d952fb267a74e989933a847183b10e9",
              "IPY_MODEL_592592b920ea4a73b66171d712de10ae"
            ],
            "layout": "IPY_MODEL_048c1b4785924ca2baf0058d9c1f588b"
          }
        },
        "64b0087d9e6b4530a39e141817cec450": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1fba6ee5b184e28af81fb04fe1501b4",
            "placeholder": "​",
            "style": "IPY_MODEL_057e0ce4fb914c2284fd6133c558c424",
            "value": "Downloading tokenizer.json: 100%"
          }
        },
        "2d952fb267a74e989933a847183b10e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2753ddb437ab457a8869b08fe79c326c",
            "max": 14500438,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1784be0601d94fe5917c8165f1c41bf4",
            "value": 14500438
          }
        },
        "592592b920ea4a73b66171d712de10ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0921d36d99d04a0186911eafa52136cd",
            "placeholder": "​",
            "style": "IPY_MODEL_02e625c05b2b46e7b107fb438676731a",
            "value": " 14.5M/14.5M [00:00&lt;00:00, 87.3MB/s]"
          }
        },
        "048c1b4785924ca2baf0058d9c1f588b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1fba6ee5b184e28af81fb04fe1501b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "057e0ce4fb914c2284fd6133c558c424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2753ddb437ab457a8869b08fe79c326c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1784be0601d94fe5917c8165f1c41bf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0921d36d99d04a0186911eafa52136cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02e625c05b2b46e7b107fb438676731a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "136e536874fc472192a9f738a6878f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7562aff84ad45ba844fbb99a5271f00",
              "IPY_MODEL_94dd5170bbf34bb6a0e1f7bfb23286bd",
              "IPY_MODEL_b0ce6b8d39da4f329d0fa06716c0c82f"
            ],
            "layout": "IPY_MODEL_3d8b6176300a407da4024254a95150ad"
          }
        },
        "d7562aff84ad45ba844fbb99a5271f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e60778062f14b5bb99ee0a94bdbcab6",
            "placeholder": "​",
            "style": "IPY_MODEL_86927bf0ce1141d383bc70c57b9916f4",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "94dd5170bbf34bb6a0e1f7bfb23286bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a8fda2caf374ff4a4631936540b5a83",
            "max": 85,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b3ac37fc27848aaaac266244298315c",
            "value": 85
          }
        },
        "b0ce6b8d39da4f329d0fa06716c0c82f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffa0ae6d623b4d839f708fa9d379bc15",
            "placeholder": "​",
            "style": "IPY_MODEL_502bb716ec9940858a496e4d9bce54bc",
            "value": " 85.0/85.0 [00:00&lt;00:00, 6.32kB/s]"
          }
        },
        "3d8b6176300a407da4024254a95150ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e60778062f14b5bb99ee0a94bdbcab6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86927bf0ce1141d383bc70c57b9916f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a8fda2caf374ff4a4631936540b5a83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b3ac37fc27848aaaac266244298315c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ffa0ae6d623b4d839f708fa9d379bc15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "502bb716ec9940858a496e4d9bce54bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1271dffeb97d4641a2dd73fb529a24d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69a304532bb44aeb903720bb40fda2de",
              "IPY_MODEL_bcffbccf55de43e2a781005173532865",
              "IPY_MODEL_c773133ab5a042329b9bdd55a62384aa"
            ],
            "layout": "IPY_MODEL_10d984d7befc48ebb517ace485182ac1"
          }
        },
        "69a304532bb44aeb903720bb40fda2de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2791bf92dd5d4909a6c6d15a46f9b3d5",
            "placeholder": "​",
            "style": "IPY_MODEL_07b5a67a651447a185c70323155418d3",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "bcffbccf55de43e2a781005173532865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be523aa578214881baec56d7d902ae44",
            "max": 715,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9aa6b898177495bad5ed9ad2733d48b",
            "value": 715
          }
        },
        "c773133ab5a042329b9bdd55a62384aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6031a55930d4776a50494269c198915",
            "placeholder": "​",
            "style": "IPY_MODEL_8a4046c8cd954e85944072c313680205",
            "value": " 715/715 [00:00&lt;00:00, 53.6kB/s]"
          }
        },
        "10d984d7befc48ebb517ace485182ac1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2791bf92dd5d4909a6c6d15a46f9b3d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07b5a67a651447a185c70323155418d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be523aa578214881baec56d7d902ae44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9aa6b898177495bad5ed9ad2733d48b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6031a55930d4776a50494269c198915": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a4046c8cd954e85944072c313680205": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5e8e715f13c41e8808719bbabdf9ad5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b61ed740038e4fda8e422683e4f3d7cc",
              "IPY_MODEL_2c3309d4ab014da583ee58787b89de9c",
              "IPY_MODEL_dbb5f7c63d5e4f71bdea431848523f1b"
            ],
            "layout": "IPY_MODEL_40b32581127e48058430e48fa58f16c0"
          }
        },
        "b61ed740038e4fda8e422683e4f3d7cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de9eedb48fb4442ebd7d89447003bdd3",
            "placeholder": "​",
            "style": "IPY_MODEL_22eb48a5737847a8bc60e6c282dcbe3f",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "2c3309d4ab014da583ee58787b89de9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2d042099afb439c853cef681bb633aa",
            "max": 3444848602,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66e90256d7cf4ce393745cc7b0fa9404",
            "value": 3444848602
          }
        },
        "dbb5f7c63d5e4f71bdea431848523f1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_289c223e455f463e8981d5cad60039ba",
            "placeholder": "​",
            "style": "IPY_MODEL_6f03e4b9b32d40f3b510dbb1bf4c1b4a",
            "value": " 3.44G/3.44G [00:16&lt;00:00, 38.3MB/s]"
          }
        },
        "40b32581127e48058430e48fa58f16c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de9eedb48fb4442ebd7d89447003bdd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22eb48a5737847a8bc60e6c282dcbe3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2d042099afb439c853cef681bb633aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66e90256d7cf4ce393745cc7b0fa9404": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "289c223e455f463e8981d5cad60039ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f03e4b9b32d40f3b510dbb1bf4c1b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}